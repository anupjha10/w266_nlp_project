{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "  \"\"\"\n",
    "  Load the data to a pandas dataframe.\n",
    "  Dataframe Columns:\n",
    "    title\n",
    "    para_index\n",
    "    context\n",
    "    q_index\n",
    "    q_id\n",
    "    q_isimpossible\n",
    "    q_question\n",
    "    q_anscount - number of answers\n",
    "    q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for pdata in data['data']:\n",
    "    for para in pdata['paragraphs']:\n",
    "      for q in para['qas']:\n",
    "        result.append({\n",
    "            'title' : pdata['title'],\n",
    "            'context' : para['context'],\n",
    "            'q_id' : q['id'],\n",
    "            'q_isimpossible' : q['is_impossible'],\n",
    "            'q_question' : q['question'],\n",
    "            'q_anscount' : len(q['answers']),\n",
    "            'q_answers' : [a for a in q['answers']],\n",
    "            'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "            'context_lowercase': para['context'].lower(),\n",
    "            'q_question_lowercase' : q['question'].lower(),\n",
    "            'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']],\n",
    "            \n",
    "        })\n",
    "\n",
    "  return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "  \"\"\"\n",
    "  Download a file from google drive with the shared link\n",
    "  \"\"\" \n",
    "  data = load_json_file(filename)\n",
    "  return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 212\n",
    "question_maxlen = 18\n",
    "answer_maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "          if i <= num_words:\n",
    "            vocab[word] = i\n",
    "        else:\n",
    "          vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible_shuffled = answer_impossible[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible_shuffled[:train_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible_shuffled[train_samples:end_samples]\n",
    "    return (train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible)\n",
    "\n",
    "train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,\\\n",
    "train_answer_impossible,\\\n",
    "val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,\\\n",
    "val_answer_impossible\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "ndim = 100\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64):\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_minus_q = layers.subtract([context_outputs,q_increased])\n",
    "            c_minus_m = layers.subtract([context_outputs,m_increased])\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(query,1), context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(m,1), context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "    #print(m.shape)\n",
    "    #print(context_outputs.shape)\n",
    "    #print(question_outputs.shape)\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    #answer_module\n",
    "\n",
    "    answer_input = Input(shape=(None,),dtype='int32',name='Answer_Input')\n",
    "    answer_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                         ndim,\n",
    "                                         mask_zero=True,\n",
    "                                         name='Answer_Embedding')(answer_input)\n",
    "    answer_embedding_plus_question = layers.concatenate(\n",
    "                                       inputs = [tf.tile(tf.keras.backend.expand_dims(question_outputs, 1),\n",
    "                                                         tf.constant([1,max_answer_length+2,1],\n",
    "                                                                    tf.int32)),\n",
    "                                                 answer_embeddings],axis=-1)\n",
    "    for i in range(num_layers_gru):\n",
    "        answer_decoder_layers = layers.GRU(num_unit_gru,\n",
    "                                           dropout=dropout_rate,\n",
    "                                           recurrent_dropout= dropout_rate,\n",
    "                                           recurrent_initializer='glorot_uniform',\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           name='Answer_GRU_Layer'+str(i)\n",
    "                                           )\n",
    "        if i==0:\n",
    "            answer_outputs,_ = answer_decoder_layers(answer_embedding_plus_question,initial_state=m)\n",
    "        else:\n",
    "            answer_outputs,_ = answer_decoder_layers(answer_outputs)\n",
    "        answer_outputs = layers.BatchNormalization()(answer_outputs)\n",
    "    answer_decoder_dense = layers.TimeDistributed(layers.Dense(vocab_size+1, activation='softmax')\n",
    "                                                  ,name='Answer_output')\n",
    "    answer_decoder_outputs = answer_decoder_dense(answer_outputs)\n",
    "\n",
    "    answer_model = Model([context_input,question_input,answer_input],answer_decoder_outputs)\n",
    "    answer_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    answer_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    answer_model.get_layer(\"Answer_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Answer_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    feasibility_input = Input(shape=(concatenated_tensor.shape[1],), name=\"FeasibilityInput\")\n",
    "    for i in range(num_dense_layers_feasibility):\n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                            activation='relu',name='feasibility_layer_'+str(i))(feasibility_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                            activation='relu',name='feasibility_layer_'+str(i))(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',name='feasibility_output')(dropout_layer)\n",
    "    feasibility_model = Model(feasibility_input,feasibility_output)\n",
    "\n",
    "    encoder_model = Model([context_input,question_input], [m,concatenated_tensor,question_outputs])\n",
    "    decoder_inputs = answer_input\n",
    "    decoder_state_input_h = Input(shape=(None,), name=\"DecoderStateInput\")\n",
    "    decoder_question_input = Input(shape=(None,),name=\"DecoderQuestionInput\")\n",
    "    decoder_answer_embeddings = answer_model.get_layer('Answer_Embedding')(decoder_inputs)\n",
    "    decoder_answer_embedding_plus_question = layers.concatenate(\n",
    "                                                 inputs = [tf.keras.backend.expand_dims(decoder_question_input, 1),\n",
    "                                                 decoder_answer_embeddings],axis=-1)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        decoder_layers = answer_model.get_layer('Answer_GRU_Layer'+str(i))\n",
    "        if i==0:\n",
    "            decoder_outputs, decoder_state_h = decoder_layers(decoder_answer_embedding_plus_question,\n",
    "                                                              initial_state=decoder_state_input_h)\n",
    "        else:\n",
    "            decoder_outputs, decoder_state_h = decoder_layers(decoder_outputs)\n",
    "\n",
    "    decoder_dense =  answer_model.get_layer('Answer_output')(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "                        [decoder_inputs] + [decoder_state_input_h]+[decoder_question_input],\n",
    "                        [decoder_dense] + [decoder_state_h])\n",
    "    return (answer_model,encoder_model,decoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sequence(context_input_seq,\n",
    "                    question_input_seq,\n",
    "                    encoder_model,\n",
    "                    decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value,_,question_outputs = encoder_model.predict([context_input_seq,question_input_seq])\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    current_step = 0\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab[\"<s>\"]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value] + [question_outputs])\n",
    "        current_step += 1\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        #print(output_tokens[0,0,0])\n",
    "        #print(output_tokens[0,0,32984])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        #print(sampled_token_index)\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \" \"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == '</s>' or len(decoded_sentence) > answer_maxlen:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 32,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 1,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 16,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 32,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment2': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 1,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.4,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 32,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 2,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment4': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.001},\n",
       " 'Experiment5': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 1,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 32,\n",
       "  'learning_rate': 0.001}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 48, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 32, 'learning_rate': 0.005}, 'Experiment1': {'num_unit_gru': 80, 'num_layers_gru': 1, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 16, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 3, 'num_episodic_network_unit': 32, 'learning_rate': 0.005}, 'Experiment2': {'num_unit_gru': 80, 'num_layers_gru': 1, 'num_episodes': 1, 'num_dense_layer_feasibility_units': 48, 'dropout_rate': 0.4, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 32, 'learning_rate': 0.005}, 'Experiment3': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 48, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 2, 'num_episodic_network_unit': 64, 'learning_rate': 0.005}, 'Experiment4': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 48, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 64, 'learning_rate': 0.001}, 'Experiment5': {'num_unit_gru': 100, 'num_layers_gru': 1, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 32, 'learning_rate': 0.001}}\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    \n",
    "    \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        decoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        answer_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "                                )\n",
    "\n",
    "        answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            decoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            answer_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "                                )\n",
    "\n",
    "            answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            feasibility_model.summary()\n",
    "        \n",
    "        \n",
    "        \n",
    "    history_answer_model = answer_model.fit(\n",
    "                                         {'Context_Input':train_context_padded_seq[:num_training_samples],\n",
    "                                         'Question_Input':train_question_seq_padded[:num_training_samples],\n",
    "                                         'Answer_Input':train_answer_input_seq_padded[:num_training_samples] },\n",
    "                                         {'Answer_output':train_answer_target_seq_padded[:num_training_samples] },\n",
    "                                     epochs=num_epochs,\n",
    "                                     batch_size=batch_size,\n",
    "                                     validation_data=([val_context_padded_seq[:num_validation_samples],\n",
    "                                                       val_question_seq_padded[:num_validation_samples],\n",
    "                                                       val_answer_input_seq_padded[:num_validation_samples]],\n",
    "                                                       val_answer_target_seq_padded[:num_validation_samples]))\n",
    "    _,encoder_prediction,_ = encoder_model.predict([train_context_padded_seq[:num_training_samples],\n",
    "                                                    train_question_seq_padded[:num_training_samples]])\n",
    "    _,encoder_validation_prediction,_ = encoder_model.predict([val_context_padded_seq[:num_validation_samples],\n",
    "                                                               val_question_seq_padded[:num_validation_samples]])\n",
    "    history_feasibility_model = feasibility_model.fit(encoder_prediction,\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            (encoder_validation_prediction,\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "    answer_model.save(ExperimentNo+'original_model_answer_model.h5')\n",
    "    encoder_model.save(ExperimentNo+'original_model_encoder_model.h5')\n",
    "    decoder_model.save(ExperimentNo+'original_model_decoder_model.h5')\n",
    "    feasibility_model.save(ExperimentNo+'original_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'original_model_'+'history_answer_model', 'wb') as file_history:\n",
    "            pickle.dump(history_answer_model.history, file_history)\n",
    "    with open(ExperimentNo+'original_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "            pickle.dump(history_feasibility_model.history, file_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    \n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_decoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "\n",
    "    inference_answer_model.load_weights(ExperimentNo+'original_model_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'original_model_encoder_model.h5')\n",
    "    inference_decoder_model.load_weights(ExperimentNo+'original_model_decoder_model.h5')\n",
    "    inference_feasibility_model.load_weights(ExperimentNo+'original_model_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_decoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context output shape (None, None, 80)\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Question_Input (InputLayer)     [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Question_Embedding (Embedding)  (None, None, 100)    8870200     Question_Input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "Question_Bid_Layer0 (Bidirectio (None, None, 80)     87360       Question_Embedding[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Context_Input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, None, 80)     320         Question_Bid_Layer0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Context_Embedding (Embedding)   (None, None, 100)    8870200     Context_Input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "Question_Bid_Layer1 (Bidirectio (None, 80)           77760       batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "Context_Bid_Layer0 (Bidirection (None, None, 80)     87360       Context_Embedding[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 80)           320         Question_Bid_Layer1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, None, 80)     320         Context_Bid_Layer0[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "Context_Bid_Layer1 (Bidirection (None, None, 80)     77760       batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 80)           0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, None, 80)     320         Context_Bid_Layer1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_14 (Tens [(None, 1, 80)]      0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_15 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_16 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_17 (Tens [(None, 1, 80)]      0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_7 (TensorFlowO [(None, 212, 80)]    0           tf_op_layer_ExpandDims_14[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_8 (TensorFlowO [(None, 212, 80)]    0           tf_op_layer_ExpandDims_15[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_6 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_16[0][0]  \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_7 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_17[0][0]  \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_6 (Subtract)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_7 (Subtract)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "permute_6 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_6[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "permute_7 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_7[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 212, 482)     0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_7[0][0]         \n",
      "                                                                 tf_op_layer_Tile_8[0][0]         \n",
      "                                                                 multiply_3[0][0]                 \n",
      "                                                                 subtract_6[0][0]                 \n",
      "                                                                 subtract_7[0][0]                 \n",
      "                                                                 permute_6[0][0]                  \n",
      "                                                                 permute_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 212, 32)      15456       concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 212, 1)       33          dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_6 (Tensor [(None, 1, 212)]     0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_3 (TensorFl [(None, 1, 212)]     0           tf_op_layer_transpose_6[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_7 (Tensor [(None, 212, 1)]     0           tf_op_layer_Softmax_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_3 (TensorFlowOp [(None, 212, 80)]    0           tf_op_layer_transpose_7[0][0]    \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_3 (TensorFlowOp [(None, 80)]         0           tf_op_layer_mul_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_18 (Tens [(None, 1, 80)]      0           tf_op_layer_Sum_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_19 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_20 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_21 (Tens [(None, 1, 80)]      0           tf_op_layer_Sum_3[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_9 (TensorFlowO [(None, 212, 80)]    0           tf_op_layer_ExpandDims_18[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_10 (TensorFlow [(None, 212, 80)]    0           tf_op_layer_ExpandDims_19[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_8 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_20[0][0]  \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_9 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_21[0][0]  \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_8 (Subtract)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_10[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_9 (Subtract)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "permute_8 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_8[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "permute_9 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_9[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 212, 482)     0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_9[0][0]         \n",
      "                                                                 tf_op_layer_Tile_10[0][0]        \n",
      "                                                                 multiply_4[0][0]                 \n",
      "                                                                 subtract_8[0][0]                 \n",
      "                                                                 subtract_9[0][0]                 \n",
      "                                                                 permute_8[0][0]                  \n",
      "                                                                 permute_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 212, 32)      15456       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 212, 1)       33          dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_8 (Tensor [(None, 1, 212)]     0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_4 (TensorFl [(None, 1, 212)]     0           tf_op_layer_transpose_8[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_9 (Tensor [(None, 212, 1)]     0           tf_op_layer_Softmax_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_4 (TensorFlowOp [(None, 212, 80)]    0           tf_op_layer_transpose_9[0][0]    \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_4 (TensorFlowOp [(None, 80)]         0           tf_op_layer_mul_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_22 (Tens [(None, 1, 80)]      0           tf_op_layer_Sum_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_23 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_24 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_25 (Tens [(None, 1, 80)]      0           tf_op_layer_Sum_4[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_11 (TensorFlow [(None, 212, 80)]    0           tf_op_layer_ExpandDims_22[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_12 (TensorFlow [(None, 212, 80)]    0           tf_op_layer_ExpandDims_23[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_10 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_24[0][0]  \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_11 (TensorFl [(None, 1, None)]    0           tf_op_layer_ExpandDims_25[0][0]  \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_10 (Subtract)          (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_12[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "subtract_11 (Subtract)          (None, 212, 80)      0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_11[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "permute_10 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_10[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, None, 1)      0           tf_op_layer_MatMul_11[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 212, 482)     0           batch_normalization_8[0][0]      \n",
      "                                                                 tf_op_layer_Tile_11[0][0]        \n",
      "                                                                 tf_op_layer_Tile_12[0][0]        \n",
      "                                                                 multiply_5[0][0]                 \n",
      "                                                                 subtract_10[0][0]                \n",
      "                                                                 subtract_11[0][0]                \n",
      "                                                                 permute_10[0][0]                 \n",
      "                                                                 permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 212, 32)      15456       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 212, 1)       33          dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_10 (Tenso [(None, 1, 212)]     0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_5 (TensorFl [(None, 1, 212)]     0           tf_op_layer_transpose_10[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_26 (Tens [(None, 1, 80)]      0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "Answer_Input (InputLayer)       [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_11 (Tenso [(None, 212, 1)]     0           tf_op_layer_Softmax_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_13 (TensorFlow [(None, 12, 80)]     0           tf_op_layer_ExpandDims_26[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "Answer_Embedding (Embedding)    (None, None, 100)    8870200     Answer_Input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_5 (TensorFlowOp [(None, 212, 80)]    0           tf_op_layer_transpose_11[0][0]   \n",
      "                                                                 batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 12, 180)      0           tf_op_layer_Tile_13[0][0]        \n",
      "                                                                 Answer_Embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_5 (TensorFlowOp [(None, 80)]         0           tf_op_layer_mul_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Answer_GRU_Layer0 (GRU)         multiple             62880       concatenate_8[0][0]              \n",
      "                                                                 tf_op_layer_Sum_5[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 12, 80)       320         Answer_GRU_Layer0[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Answer_GRU_Layer1 (GRU)         multiple             38880       batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 12, 80)       320         Answer_GRU_Layer1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "Answer_output (TimeDistributed) multiple             7184862     batch_normalization_12[0][0]     \n",
      "==================================================================================================\n",
      "Total params: 34,275,849\n",
      "Trainable params: 7,664,289\n",
      "Non-trainable params: 26,611,560\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "FeasibilityInput (InputLayer [(None, 160)]             0         \n",
      "_________________________________________________________________\n",
      "feasibility_layer_0 (Dense)  (None, 48)                7728      \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 48)                192       \n",
      "_________________________________________________________________\n",
      "feasibility_drop_0 (Dropout) (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "feasibility_output (Dense)   (None, 1)                 49        \n",
      "=================================================================\n",
      "Total params: 7,969\n",
      "Trainable params: 7,873\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n",
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024/1024 [==============================] - 154s 150ms/sample - loss: 2.9737 - sparse_categorical_accuracy: 0.1944 - val_loss: 1.9856 - val_sparse_categorical_accuracy: 0.3210\n",
      "Epoch 2/2\n",
      "1024/1024 [==============================] - 129s 126ms/sample - loss: 1.7482 - sparse_categorical_accuracy: 0.3006 - val_loss: 1.9891 - val_sparse_categorical_accuracy: 0.3560\n",
      "Train on 1024 samples, validate on 256 samples\n",
      "Epoch 1/2\n",
      "WARNING:tensorflow:Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "1024/1024 [==============================] - 1s 814us/sample - loss: 0.7308 - binary_accuracy: 0.6201 - val_loss: 0.6752 - val_binary_accuracy: 0.5781\n",
      "Epoch 2/2\n",
      "1024/1024 [==============================] - 0s 161us/sample - loss: 0.6436 - binary_accuracy: 0.6650 - val_loss: 0.6736 - val_binary_accuracy: 0.5664\n"
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                Experiment_No=0,\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                ndim = 100,\n",
    "                tpu_enabled=0,\n",
    "                num_training_samples=1024,\n",
    "                num_validation_samples = 256,\n",
    "                num_epochs = 2,\n",
    "                batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context output shape (None, None, 80)\n",
      "question: what does ban bossy encourage\n",
      "Predicted Answer: ncaa parochial \n",
      "Actual answer: <s> leadership in girls </s>\n",
      "question: what area had high windows just below ground level\n",
      "Predicted Answer: ncaa hemisphere \n",
      "Actual answer: <s> </s>\n",
      "question: what happens if a plant looses roots or its shoots\n",
      "Predicted Answer: kirby   </s> \n",
      "Actual answer: <s> can often regrow it </s>\n",
      "question: what was typically worn after the loss of a loved one in the roman republic\n",
      "Predicted Answer: ncaa hemisphere \n",
      "Actual answer: <s> the toga pulla </s>\n",
      "question: what instruments are used in armenian folk music\n",
      "Predicted Answer: none department \n",
      "Actual answer: <s> the duduk the dhol the zurna and the kanun </s>\n",
      "question: why does a litigant initiate a lawsuit under the civil rights act of 1801\n",
      "Predicted Answer: ncaa hemisphere \n",
      "Actual answer: <s> </s>\n",
      "question: at what rate have glaciers travelled during surges\n",
      "Predicted Answer: ncaa hemisphere \n",
      "Actual answer: <s> 90 m 300 ft per day </s>\n",
      "question: what totals a thirs of the glaciers length\n",
      "Predicted Answer: ncaa parochial \n",
      "Actual answer: <s> </s>\n",
      "question: what color does liverpool fans wear\n",
      "Predicted Answer: ncaa hemisphere \n",
      "Actual answer: <s> </s>\n",
      "question: what other industry is a large part of houston s economy\n",
      "Predicted Answer: ncaa kanun \n",
      "Actual answer: <s> houston ship channel </s>\n"
     ]
    }
   ],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_decoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=0,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = 100)\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_decoder_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
