{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be run only when on Google Colab\n",
    "%tensorflow_version 2.x\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: tqdm in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "    \"\"\"\n",
    "      Load the data to a pandas dataframe.\n",
    "      Dataframe Columns:\n",
    "      title\n",
    "      para_index\n",
    "      context\n",
    "      q_index\n",
    "      q_id\n",
    "      q_isimpossible\n",
    "      q_question\n",
    "      q_anscount - number of answers\n",
    "      q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for pdata in data['data']:\n",
    "        for para in pdata['paragraphs']:\n",
    "            for q in para['qas']:\n",
    "                result.append({\n",
    "                                'title' : pdata['title'],\n",
    "                                'context' : para['context'],\n",
    "                                'q_id' : q['id'],\n",
    "                                'q_isimpossible' : q['is_impossible'],\n",
    "                                'q_question' : q['question'],\n",
    "                                'q_anscount' : len(q['answers']),\n",
    "                                'q_answers' : [a for a in q['answers']],\n",
    "                                'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "                                'context_lowercase': para['context'].lower(),\n",
    "                                'q_question_lowercase' : q['question'].lower(),\n",
    "                                'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']]\n",
    "                               })\n",
    "    return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "    \"\"\"\n",
    "    Download a file from google drive with the shared link\n",
    "    \"\"\" \n",
    "    data = load_json_file(filename)\n",
    "    return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 250\n",
    "question_maxlen = 20\n",
    "answer_maxlen = 15\n",
    "ndim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "            if i <= num_words:\n",
    "                vocab[word] = i\n",
    "        else:\n",
    "            vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def calc_answer_span(context,answer):\n",
    "    \n",
    "    ans_len = len(answer)\n",
    "    \n",
    "    if ans_len!=0 and answer[0] in context:\n",
    "        indices = [i for i, x in enumerate(context) if x == answer[0]]\n",
    "        try:\n",
    "            if(len(indices)>1):\n",
    "                start = [i for i in indices if (context[i:i+ans_len] == answer) ]\n",
    "                end = start[0] + ans_len - 1\n",
    "                return (start[0],end)\n",
    "            else:\n",
    "                start = context.index(answer[0])\n",
    "                end = start + ans_len - 1\n",
    "                return (start,end)\n",
    "        except:\n",
    "            return (-1,-1)\n",
    "    else:\n",
    "        return (-1,-1)\n",
    "\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    qid_original = train_lists[0]\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_span = [calc_answer_span(context_seq[i],answer_seq[i]) for i,x in enumerate(context_seq)]\n",
    "    answer_start_index = [item[0] for item in answer_span]\n",
    "    answer_end_index =  [item[1] for item in answer_span]\n",
    "    answer_start_seq = []\n",
    "    answer_end_seq = []\n",
    "    for i,x in enumerate(answer_start_index):\n",
    "        start = np.zeros(context_maxlen,dtype = \"int32\")\n",
    "        end   = np.zeros(context_maxlen,dtype = \"int32\")\n",
    "        #last space reserved for the question where there are no answers\n",
    "        if (answer_start_index[i] < context_maxlen and answer_start_index[i] != -1):\n",
    "            start[answer_start_index[i]] = 1\n",
    "        if (answer_end_index[i] < context_maxlen and answer_end_index[i] != -1):\n",
    "            end[answer_end_index[i]] = 1\n",
    "        answer_start_seq.append(start)\n",
    "        answer_end_seq.append(end)\n",
    "        \n",
    "    answer_input_seq = [i for i in answer_seq]\n",
    "    answer_target_seq = answer_input_seq\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    #Adding 0 to last position special for no answer questions\n",
    "    #context_seq_padded = pad_sequences(context_seq_padded,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    answer_start_seq_padded = pad_sequences(answer_start_seq,context_maxlen,padding='post', truncating='post') \n",
    "    answer_end_seq_padded = pad_sequences(answer_end_seq,context_maxlen,padding='post', truncating='post')\n",
    "    #context_match_question = []\n",
    "    #for i,a in enumerate(context_seq_padded):\n",
    "    #    exact = [[1]if ecw in question_seq_padded[i] and ecw !=0 else [0] for ecw in a]\n",
    "    #    context_match_question.append(exact)\n",
    "    \n",
    "    #context_match_question_padded = np.array(context_match_question)\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    qid = [qid_original[i] for i in indices]\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible = answer_impossible[indices]\n",
    "    answer_start_seq_padded = answer_start_seq_padded[indices]\n",
    "    answer_end_seq_padded = answer_end_seq_padded[indices]\n",
    "    #context_match_question_padded = context_match_question_padded[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_qid = qid[:train_samples]\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible[:train_samples]\n",
    "    train_answer_start_seq_padded = answer_start_seq_padded[:train_samples]\n",
    "    train_answer_end_seq_padded = answer_end_seq_padded[:train_samples]\n",
    "    #train_context_match_question_padded = context_match_question_padded[:train_samples]\n",
    "    val_qid = qid[train_samples:end_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible[train_samples:end_samples]\n",
    "    val_answer_start_seq_padded = answer_start_seq_padded[train_samples:end_samples]\n",
    "    val_answer_end_seq_padded = answer_end_seq_padded[train_samples:end_samples]\n",
    "    #val_context_match_question_padded = context_match_question_padded[train_samples:end_samples]\n",
    "    return (train_qid,train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            train_answer_start_seq_padded,train_answer_end_seq_padded,\n",
    "            #train_context_match_question_padded,\n",
    "            val_qid,val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible,\n",
    "            val_answer_start_seq_padded,val_answer_end_seq_padded\n",
    "            #,val_context_match_question_padded\n",
    "           )\n",
    "\n",
    "train_qid,train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\\\n",
    "train_answer_start_seq_padded,train_answer_end_seq_padded,\\\n",
    "val_qid,val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible,\\\n",
    "val_answer_start_seq_padded,val_answer_end_seq_padded\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88702, 100)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Model with attention in every step of the answer decoder\n",
    "class BahdanauAttention_model2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention_model2, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Answer Module which is custom as we need to feed output of each time sequence with attention to next \n",
    "# time sequence\n",
    "class answer_module(tf.keras.Model):\n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      ndim,\n",
    "                      num_unit_gru,\n",
    "                      num_layers_gru,\n",
    "                      dropout_rate,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(answer_module, self).__init__()\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.WSTART = tf.keras.layers.Dense(2*self.num_unit_gru)\n",
    "        self.WEND = tf.keras.layers.Dense(2*self.num_unit_gru)        \n",
    "    \n",
    "    def call(self,\n",
    "             question,\n",
    "             context):\n",
    "        \n",
    "        context = tf.transpose(context,[0,2,1])\n",
    "        ################ start prediction logit ######################\n",
    "        start = self.WSTART(question)\n",
    "        hidden_start_time_axis = tf.expand_dims(start, 1)\n",
    "        \n",
    "        start_logit = tf.squeeze(tf.matmul(hidden_start_time_axis,context),axis=1)\n",
    "        start_logit = tf.math.exp(start_logit)\n",
    "            \n",
    "        ################ end prediction logit ######################\n",
    "        end = self.WEND(question)\n",
    "\n",
    "        hidden_end_time_axis = tf.expand_dims(end, 1)\n",
    "        \n",
    "        # squeeze remooves time slice we added before\n",
    "        # final shape = (batch_size,decoder_timesteps)\n",
    "        end_logit = tf.squeeze(tf.matmul(hidden_end_time_axis,context),axis=1)\n",
    "        end_logit = tf.math.exp(end_logit)\n",
    "        \n",
    "        \n",
    "        return start_logit,end_logit \n",
    "\n",
    "#Encoder Module which combines the context,question into episodic memory and emits context outputs and \n",
    "#question outputs\n",
    "class encoder_module(tf.keras.Model):    \n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      max_context_length,\n",
    "                      max_question_length,\n",
    "                      max_answer_length,\n",
    "                      num_unit_gru = 64,\n",
    "                      num_layers_gru = 2,\n",
    "                      ndim =100,\n",
    "                      num_episodes = 2,\n",
    "                      dropout_rate = 0.5,\n",
    "                      num_episodic_network_unit = 64,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(encoder_module, self).__init__()\n",
    "        #Context Module\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers_gru = num_layers_gru\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ndim = ndim\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_question_length = max_question_length\n",
    "        self.max_answer_length = max_answer_length\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_episodic_network_unit = num_episodic_network_unit\n",
    "        self.context_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                         self.ndim,\n",
    "                                                         mask_zero=True,\n",
    "                                                         weights =[self.embedding_matrix],\n",
    "                                                         trainable = False,\n",
    "                                                         name='Context_Embedding')\n",
    "        self.context_output_layers = []\n",
    "        self.context_batch_normalization_layers = []\n",
    "        for i in range(self.num_layers_gru):\n",
    "            self.context_output_layers.append(layers.Bidirectional(\n",
    "                                                layers.LSTM(self.num_unit_gru,\n",
    "                                                           dropout=self.dropout_rate,\n",
    "                                                           recurrent_dropout= self.dropout_rate,\n",
    "                                                           recurrent_initializer='glorot_uniform',\n",
    "                                                           return_sequences=True,\n",
    "                                                           kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                              l1_regularizer_weight,\n",
    "                                                                              l2_regularizer_weight),\n",
    "                                                           bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                              l1_regularizer_weight,\n",
    "                                                                              l2_regularizer_weight)),\n",
    "                                                           merge_mode='concat',\n",
    "                                                           name='Context_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.context_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        #Question Module\n",
    "        self.question_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                          self.ndim,\n",
    "                                                          mask_zero=True,\n",
    "                                                          weights =[self.embedding_matrix],\n",
    "                                                          trainable = False,\n",
    "                                                          name='Question_Embedding')\n",
    "          \n",
    "        self.question_output_layers = []\n",
    "        self.question_batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "            self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                 layers.LSTM(self.num_unit_gru,\n",
    "                                                            dropout=self.dropout_rate,\n",
    "                                                            recurrent_dropout= self.dropout_rate,\n",
    "                                                            recurrent_initializer='glorot_uniform',\n",
    "                                                            return_sequences=True,\n",
    "                                                            kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                l1_regularizer_weight,\n",
    "                                                                                l2_regularizer_weight),\n",
    "                                                            bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                l1_regularizer_weight,\n",
    "                                                                                l2_regularizer_weight)),\n",
    "                                                             merge_mode='concat',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.question_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "            \n",
    "        self.question_attention_layer = layers.Dense(1)\n",
    "        #Episodic Memory \n",
    "        #self.episodic_weight_layer = layers.Dense(self.num_unit_gru,use_bias=False)\n",
    "        #self.episodic_tanh_layer = layers.Dense(self.num_episodic_network_unit,activation='tanh')\n",
    "        #self.episodic_score_layer = layers.Dense(1)\n",
    "        #Self alignment\n",
    "        self.Align_W1 = tf.keras.layers.Dense(1,activation=\"relu\")\n",
    "        self.Align_W2 = tf.keras.layers.Dense(1,activation=\"relu\")\n",
    "    def call(self,context_input,question_input):\n",
    "        #context Module\n",
    "        context_embeddings = self.context_embeddings_layer(context_input)\n",
    "        #Question Module\n",
    "        question_embeddings = self.question_embeddings_layer(question_input)\n",
    "        #self alignment\n",
    "        context_relu = self.Align_W1(context_embeddings)\n",
    "        question_relu = self.Align_W2(question_embeddings)\n",
    "        self_align_logits = tf.matmul(context_relu,tf.transpose(question_relu,[0,2,1]))\n",
    "        self_align_scores = tf.nn.softmax(self_align_logits)\n",
    "        self_align_embeddings = tf.matmul(self_align_scores,question_embeddings)\n",
    "        context_concat_embeddings = tf.concat(values=[context_embeddings,self_align_embeddings],axis=-1)\n",
    "        \n",
    "        #print(\"context_embeddings.shape:\",context_embeddings.shape)\n",
    "        #print(\"question_embeddings.shape:\",question_embeddings.shape)\n",
    "        #print(\"context_relu.shape:\",context_relu.shape)\n",
    "        #print(\"question_relu.shape:\",question_relu.shape)\n",
    "        #print(\"self_align_logits.shape:\",self_align_logits.shape)\n",
    "        #print(\"self_align_scores.shape:\",self_align_scores.shape)\n",
    "        #print(\"self_align_embeddings.shape:\",self_align_embeddings.shape)\n",
    "        #print(\"context_concat_embeddings.shape:\",context_concat_embeddings.shape)\n",
    "        for i in range(len(self.context_output_layers)):\n",
    "            if i==0:\n",
    "                context_outputs = self.context_output_layers[i](context_concat_embeddings)\n",
    "            else:\n",
    "                context_outputs = self.context_output_layers[i](context_outputs)\n",
    "            context_outputs = self.context_batch_normalization_layers[i](context_outputs)\n",
    "\n",
    "        \n",
    "        for i in range(len(self.question_output_layers)):\n",
    "            if i==0:\n",
    "                question_outputs = self.question_output_layers[i](question_embeddings)\n",
    "            else:\n",
    "                question_outputs = self.question_output_layers[i](question_outputs)\n",
    "            question_outputs = self.question_batch_normalization_layers[i](question_outputs) \n",
    "        \n",
    "        #Calculate the self attention for question        \n",
    "        question_score = self.question_attention_layer(question_outputs)\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        question_attention_weights = tf.nn.softmax(question_score, axis=1)\n",
    "        question_outputs = question_attention_weights * question_outputs\n",
    "        question_outputs = tf.reduce_sum(question_outputs, axis=1)\n",
    "        \n",
    "        #Episodic Memory \n",
    "        #m = tf.identity(question_outputs)\n",
    "        #for i in range(self.num_episodes):\n",
    "        #    m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "        #                          tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "        #    q_increased = tf.tile(tf.keras.backend.expand_dims(question_outputs,1),\n",
    "        #                          tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "        #    c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "        #    c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "        #    c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "        #    c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "        #    c_dot_q = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(question_outputs),1), \n",
    "        #                        context_outputs,\n",
    "        #                        transpose_b=True)\n",
    "        #    c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "        #    c_dot_m = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(m),1), \n",
    "        #                        context_outputs,transpose_b=True)\n",
    "        #    c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "        #    z = tf.concat([context_outputs,\n",
    "        #                            m_increased,\n",
    "        #                            q_increased,\n",
    "        #                            c_mul_q,\n",
    "        #                            c_mul_m,\n",
    "        #                            c_minus_q,\n",
    "        #                            c_minus_m,\n",
    "        #                            c_dot_q,\n",
    "        #                            c_dot_m],axis=-1)\n",
    "        #    score = self.episodic_score_layer(self.episodic_tanh_layer(z))\n",
    "        #    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        #    m_value = attention_weights * context_outputs\n",
    "        #    m = tf.reduce_sum(m_value, axis=1)\n",
    "        #concatenate episodic memory with question\n",
    "        #concatenated_tensor = tf.concat(values=[m,question_outputs],axis=1)\n",
    "        return (question_outputs,context_outputs)\n",
    "    \n",
    "                \n",
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64,\n",
    "                  l1_regularizer_weight = .01,\n",
    "                  l2_regularizer_weight = .01):\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        weight_layer = layers.Dense(query.shape[1],use_bias=False)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(weight_layer(query),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(weight_layer(m),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    \"\"\"\n",
    "    #encoder Model\n",
    "    encoder_model = encoder_module(embedding_matrix,\n",
    "                                   vocab_size,\n",
    "                                   max_context_length,\n",
    "                                   max_question_length,\n",
    "                                   max_answer_length,\n",
    "                                   num_unit_gru,\n",
    "                                   num_layers_gru,\n",
    "                                   ndim,\n",
    "                                   num_episodes,\n",
    "                                   dropout_rate,\n",
    "                                   num_episodic_network_unit,\n",
    "                                   l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                   l2_regularizer_weight = l2_regularizer_weight\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    #Model([context_input,question_input], [m,concatenated_tensor,question_outputs,context_outputs])\n",
    "    #answer_module\n",
    "    answer_model = answer_module(embedding_matrix,\n",
    "                                 vocab_size,\n",
    "                                 ndim,\n",
    "                                 num_unit_gru,\n",
    "                                 num_layers_gru,\n",
    "                                 dropout_rate,\n",
    "                                 l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                 l2_regularizer_weight = l2_regularizer_weight)\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    #feasibility_episodic_memory_input = Input(shape=(num_unit_gru,), name=\"FeasibilityEpisodicMemoryInput\")\n",
    "    #feasibility_start_logits_input = Input(shape=(max_context_length,),name=\"FeasibilityStartLogitInput\")\n",
    "    #feasibility_end_logits_input = Input(shape=(max_context_length),name=\"FeasibilityEndLogitInput\")\n",
    "    feasibility_context_input = Input(shape=(None,2*num_unit_gru,),name='feasibilityContext_Input')\n",
    "    feasibility_question_input = Input(shape=(2*num_unit_gru,),name='feasibilityQuestion_Input')\n",
    "    #create attention between Context and Question\n",
    "    q_with_time_axis = tf.keras.backend.expand_dims(feasibility_question_input,1)\n",
    "    attentionContextQuestion = layers.AdditiveAttention()([q_with_time_axis,\n",
    "                                                          feasibility_context_input])\n",
    "    attentionContextQuestionReduced = tf.keras.backend.sum(attentionContextQuestion, axis=1)\n",
    "    #create Episodic memory \n",
    "    #Episodic Memory \n",
    "    episodic_weight_layer = layers.Dense(2*num_unit_gru,use_bias=False)\n",
    "    episodic_tanh_layer = layers.Dense(num_episodic_network_unit,activation='tanh')\n",
    "    episodic_score_layer = layers.Dense(1)\n",
    "    m = tf.identity(feasibility_question_input)\n",
    "    for i in range(num_episodes):\n",
    "        m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                              tf.constant([1,max_context_length,1],tf.int32))\n",
    "        q_increased = tf.tile(tf.keras.backend.expand_dims(feasibility_question_input,1),\n",
    "                              tf.constant([1,max_context_length,1],tf.int32))\n",
    "        c_mul_q = layers.multiply([feasibility_context_input,q_increased])\n",
    "        c_mul_m = layers.multiply([feasibility_context_input,m_increased])\n",
    "        c_minus_q =tf.keras.backend.abs(layers.subtract([feasibility_context_input,q_increased]))\n",
    "        c_minus_m = tf.keras.backend.abs(layers.subtract([feasibility_context_input,m_increased]))\n",
    "        c_dot_q = tf.matmul(tf.keras.backend.expand_dims(episodic_weight_layer(feasibility_question_input),1), \n",
    "                            feasibility_context_input,\n",
    "                            transpose_b=True)\n",
    "        c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "        c_dot_m = tf.matmul(tf.keras.backend.expand_dims(episodic_weight_layer(m),1), \n",
    "                            feasibility_context_input,transpose_b=True)\n",
    "        c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "        z = tf.concat([feasibility_context_input,\n",
    "                                m_increased,\n",
    "                                q_increased,\n",
    "                                c_mul_q,\n",
    "                                c_mul_m,\n",
    "                                c_minus_q,\n",
    "                                c_minus_m,\n",
    "                                c_dot_q,\n",
    "                                c_dot_m],axis=-1)\n",
    "        score = episodic_score_layer(episodic_tanh_layer(z))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        m_value = attention_weights * feasibility_context_input\n",
    "        m = tf.reduce_sum(m_value, axis=1)\n",
    "    \n",
    "    \n",
    "    #feasibility_dense_input = tf.concat([m,\n",
    "    #                                     feasibility_start_logits_input,\n",
    "    #                                     feasibility_end_logits_input],\n",
    "    #                                    axis=-1)\n",
    "    feasibility_dense_input = tf.concat([m,\n",
    "                                         attentionContextQuestionReduced],\n",
    "                                        axis=-1)\n",
    "    for i in range(num_dense_layers_feasibility):        \n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(feasibility_dense_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',\n",
    "                                      name='feasibility_output',\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                    l1_regularizer_weight,\n",
    "                                                                    l2_regularizer_weight),\n",
    "                                      bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                    l1_regularizer_weight,\n",
    "                                                                    l2_regularizer_weight))(dropout_layer)\n",
    "    feasibility_model = Model([feasibility_question_input,\n",
    "                               feasibility_context_input],\n",
    "                               feasibility_output)\n",
    "    \n",
    "    return (answer_model,encoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sentence(context_input_seq,\n",
    "                     question_input_seq,\n",
    "                     encoder_model,\n",
    "                     answer_model):\n",
    "    decoded_sentence = \"\"\n",
    "    question_output,context_output = encoder_model(context_input_seq,question_input_seq) \n",
    "    \n",
    "    start_logits,end_logits = answer_model(question_output,context_output)\n",
    "    \n",
    "    #Do a outer matrix multiplication of the logits \n",
    "    # we need to get the start and end index with highest multiplication of start and end probs\n",
    "    outer = tf.matmul(tf.expand_dims(start_logits, axis=2),tf.expand_dims(end_logits, axis=0)) \n",
    "    outer = tf.linalg.band_part(outer, 0, answer_maxlen)\n",
    "    start_position = tf.argmax(tf.reduce_max(outer, axis=2),axis=1)    \n",
    "    end_position = tf.argmax(tf.reduce_max(outer, axis=1),axis=1)\n",
    "    #print(start_position.shape)\n",
    "    #print(end_position.shape)\n",
    "    #print(start_position)\n",
    "    #print(end_position)\n",
    "    \n",
    "    for i in range(start_position[0],end_position[0]+1):\n",
    "        sampled_token_index = context_input_seq[0,i]\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \"\"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        if i == start_position[0]:\n",
    "            decoded_sentence += sampled_char\n",
    "        else:\n",
    "            decoded_sentence += \" \"+sampled_char\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 3,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 2,\n",
       "  'num_episodic_network_unit': 96,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.0001,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment2': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 4,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment4': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 3,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 128,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.001},\n",
       " 'Experiment5': {'num_unit_gru': 128,\n",
       "  'num_layers_gru': 3,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment6': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 4,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 80,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment7': {'num_unit_gru': 128,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 64,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment1': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 3,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 2,\n",
    "                                  'num_episodic_network_unit': 96,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.0001,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment2': {'num_unit_gru': 100,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 64,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment3': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 4,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 64,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment4': {'num_unit_gru': 100,\n",
    "                                  'num_layers_gru': 3,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 48,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 128,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.001},\n",
    "                  'Experiment5': {'num_unit_gru': 128,\n",
    "                                  'num_layers_gru': 3,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment6': {'num_unit_gru': 64,\n",
    "                                  'num_layers_gru': 4,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 80,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment7': {'num_unit_gru': 128,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 64,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01}\n",
    "                }\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    print(\"Running \",ExperimentNo)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "        \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                      l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                      l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        #encoder_model.compile(optimizer=adam_optim,\n",
    "        #                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        #encoder_model.summary()\n",
    "        #answer_model.compile(optimizer=adam_optim,\n",
    "        #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        #                        )\n",
    "        #\n",
    "        #answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        #feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' +\n",
    "                                                                     os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = \n",
    "                                                        num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                         l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                         l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            #encoder_model.compile(optimizer=adam_optim,\n",
    "            #                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "            #encoder_model.summary()\n",
    "            #\n",
    "            #answer_model.compile(optimizer=adam_optim,\n",
    "            #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            #                    )\n",
    "\n",
    "            #answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            #feasibility_model.summary()\n",
    "    #Train the Answer Model and Encoder Model\n",
    "    answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    answer_loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "    @tf.function\n",
    "    def answer_loss_function(real_start, real_end,pred_start,pred_end):\n",
    "        \n",
    "        start_loss = answer_loss_object(real_start,pred_start)\n",
    "        end_loss = answer_loss_object(real_end,pred_end)\n",
    "        return tf.reduce_mean(start_loss+end_loss)\n",
    "\n",
    "    @tf.function\n",
    "    def answer_train_step(inp,ques,targ_start,targ_end,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        batch_loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            question_output,context_output = encoder_model(inp,ques)\n",
    "            start_logits,end_logits = answer_model(question_output,context_output)\n",
    "            loss += answer_loss_function(targ_start,targ_end,start_logits,end_logits)\n",
    "            batch_loss = loss\n",
    "            train_acc_metric(targ_start,start_logits)\n",
    "            train_acc_metric(targ_end,end_logits)\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def answer_val_step(inp,ques,targ_start,targ_end,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        question_output,context_output = encoder_model(inp,ques)\n",
    "        start_logits,end_logits = answer_model(question_output,context_output)\n",
    "        loss += answer_loss_function(targ_start,targ_end,start_logits,end_logits)\n",
    "        batch_loss = loss\n",
    "        val_acc_metric(targ_start,start_logits)\n",
    "        val_acc_metric(targ_end,end_logits)\n",
    "        \n",
    "        return batch_loss\n",
    "    #Create batches for training only include where its possible to answer\n",
    "    #\n",
    "    #c = b==1\n",
    "    #a[c][:3]\n",
    "    train_ans_cond = train_answer_impossible==0\n",
    "    val_ans_cond = val_answer_impossible==0\n",
    "    #make sure that training samples and validation samples are divisible by 1024\n",
    "    num_training_samples_upd = int((train_context_padded_seq[train_ans_cond][:num_training_samples].shape[0]//1024)\n",
    "                                   *1024)\n",
    "    \n",
    "    num_validation_samples_upd = int((val_context_padded_seq[val_ans_cond][:num_validation_samples].shape[0]//1024)\n",
    "                                   *1024)\n",
    "    TRAIN_BUFFER_SIZE = train_context_padded_seq[train_ans_cond][:num_training_samples_upd].shape[0]\n",
    "    VAL_BUFFER_SIZE = val_context_padded_seq[val_ans_cond][:num_validation_samples_upd].shape[0]\n",
    "    steps_per_epoch = TRAIN_BUFFER_SIZE//batch_size\n",
    "    steps_per_epoch_val = VAL_BUFFER_SIZE//batch_size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_context_padded_seq[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_question_seq_padded[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_answer_seq_padded[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_answer_input_seq_padded[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_answer_target_seq_padded[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_answer_impossible[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_answer_start_seq_padded[train_ans_cond][:num_training_samples_upd],\n",
    "                                                        train_answer_end_seq_padded[train_ans_cond][:num_training_samples_upd]\n",
    "                                                       ))\\\n",
    "                                   .shuffle(TRAIN_BUFFER_SIZE,reshuffle_each_iteration=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_context_padded_seq[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_question_seq_padded[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_answer_seq_padded[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_answer_input_seq_padded[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_answer_target_seq_padded[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_answer_impossible[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_answer_start_seq_padded[val_ans_cond][:num_validation_samples_upd],\n",
    "                                                      val_answer_end_seq_padded[val_ans_cond][:num_validation_samples_upd]\n",
    "                                                     ))\n",
    "    val_dataset = val_dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    #Run Epochs\n",
    "    \n",
    "    history_answer_model = {'loss':[],\n",
    "                            'categorical_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_categorical_accuracy':[]}\n",
    "    print(\"\\nTraining the answer model:\")\n",
    "    #tqdm.write(\"\\nTraining the answer model:\")\n",
    "    #Value to store best validation accuracy\n",
    "    max_answer_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "        #tqdm.write('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible,\n",
    "                     batch_train_answer_start_seq_padded,\n",
    "                     batch_train_answer_end_seq_padded)) in tqdm(enumerate(train_dataset.take(steps_per_epoch)),\n",
    "                                                             total = num_training_samples_upd//batch_size,\n",
    "                                                             desc=\"[Training Answer]\"):\n",
    "            batch_loss = answer_train_step(batch_train_context_padded_seq,\n",
    "                                           batch_train_question_seq_padded,\n",
    "                                           batch_train_answer_start_seq_padded,\n",
    "                                           batch_train_answer_end_seq_padded,\n",
    "                                           encoder_model,\n",
    "                                           answer_model)\n",
    "            total_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = train_acc_metric.result()\n",
    "        print('Epoch {} - Train Loss {:.4f} Train Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                 epoch_training_loss, \n",
    "                                                                                 epoch_training_accuracy))\n",
    "        train_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        #print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible,\n",
    "                     batch_val_answer_start_seq_padded,\n",
    "                     batch_val_answer_end_seq_padded)) in tqdm(enumerate(val_dataset.take(steps_per_epoch_val)),\n",
    "                                                           total = num_validation_samples_upd//batch_size,\n",
    "                                                           desc=\"[Validating Answer]\"):\n",
    "            batch_loss = answer_val_step(batch_val_context_padded_seq,\n",
    "                                         batch_val_question_seq_padded,\n",
    "                                         batch_val_answer_start_seq_padded,\n",
    "                                         batch_val_answer_end_seq_padded,\n",
    "                                         encoder_model,\n",
    "                                         answer_model)\n",
    "            total_val_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = val_acc_metric.result()\n",
    "        print('Epoch {} - Validation Loss {:.4f} Validation Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                           epoch_val_loss, \n",
    "                                                                                           epoch_val_accuracy))\n",
    "        val_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        #print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_answer_model['loss'].append(epoch_training_loss)\n",
    "        history_answer_model['categorical_accuracy'].append(epoch_training_accuracy)\n",
    "        history_answer_model['val_loss'].append(epoch_val_loss)\n",
    "        history_answer_model['val_categorical_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        #Save only the best model\n",
    "        if epoch_val_accuracy >= max_answer_val_accuracy:\n",
    "            max_answer_val_accuracy = epoch_val_accuracy\n",
    "            answer_model.save_weights(ExperimentNo+'span_model_LSTM_answer_model.h5')\n",
    "            encoder_model.save_weights(ExperimentNo+'span_model_LSTM_encoder_model.h5')\n",
    "            print('Best Epoch so far! Epoch No {}'.format(epoch + 1))\n",
    "        \n",
    "        print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "        #tqdm.write('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "        \n",
    "    #answer_model.save_weights(ExperimentNo+'span_model_LSTM_answer_model.h5')\n",
    "    #encoder_model.save_weights(ExperimentNo+'span_model_LSTM_encoder_model.h5')\n",
    "    with open(ExperimentNo+'span_model_LSTM_'+'history_answer_model', 'wb') as file_history:\n",
    "        pickle.dump(history_answer_model, file_history)\n",
    "        \n",
    "    \n",
    "    #Train the Feasibility Model\n",
    "    #load the best encoder model weights\n",
    "    encoder_model.load_weights(ExperimentNo+'span_model_LSTM_encoder_model.h5')\n",
    "    #Create batches for training including where answer is impossible\n",
    "    \n",
    "    TRAIN_BUFFER_SIZE = train_context_padded_seq[:num_training_samples].shape[0]\n",
    "    VAL_BUFFER_SIZE = val_context_padded_seq[:num_validation_samples].shape[0]\n",
    "    steps_per_epoch = TRAIN_BUFFER_SIZE//batch_size\n",
    "    steps_per_epoch_val = VAL_BUFFER_SIZE//batch_size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_context_padded_seq[:num_training_samples],\n",
    "                                                        train_question_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_input_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_target_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_impossible[:num_training_samples],\n",
    "                                                        train_answer_start_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_end_seq_padded[:num_training_samples]\n",
    "                                                       ))\\\n",
    "                                   .shuffle(TRAIN_BUFFER_SIZE,reshuffle_each_iteration=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_context_padded_seq[:num_validation_samples],\n",
    "                                                      val_question_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_input_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_target_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_impossible[:num_validation_samples],\n",
    "                                                      val_answer_start_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_end_seq_padded[:num_validation_samples]\n",
    "                                                     ))\n",
    "    val_dataset = val_dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    feasibility_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    feasibility_loss_object = tf.keras.losses.BinaryCrossentropy(reduction='none')\n",
    "    feasibility_train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    feasibility_val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    @tf.function\n",
    "    def feasibility_loss_function(y_true,y_pred):\n",
    "        \n",
    "        loss = feasibility_loss_object(y_true,y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    @tf.function\n",
    "    def feasibility_train_step(inp,ques,feasibility_true,encoder_model,answer_model,feasibility_model):\n",
    "        loss = 0\n",
    "        batch_loss = 0\n",
    "        question_output,context_output = encoder_model(inp,ques)\n",
    "        #start_logits,end_logits = answer_model(question_output,context_output)        \n",
    "        with tf.GradientTape() as tape:            \n",
    "            feasibility_pred = feasibility_model([question_output,\n",
    "                                                  context_output])\n",
    "            loss += feasibility_loss_function(feasibility_true,feasibility_pred)\n",
    "            batch_loss = loss\n",
    "            feasibility_train_acc_metric(feasibility_true,feasibility_pred)\n",
    "            variables = feasibility_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            feasibility_optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def feasibility_val_step(inp,ques,feasibility_true,encoder_model,answer_model,feasibility_model):\n",
    "        loss = 0\n",
    "        question_output,context_output = encoder_model(inp,ques)\n",
    "        #start_logits,end_logits = answer_model(question_output,context_output)         \n",
    "        feasibility_pred = feasibility_model([question_output,\n",
    "                                              context_output])\n",
    "        loss += feasibility_loss_function(feasibility_true,feasibility_pred)\n",
    "        batch_loss = loss\n",
    "        feasibility_val_acc_metric(feasibility_true,feasibility_pred)\n",
    "        return batch_loss\n",
    "    #Run Epochs\n",
    "    \n",
    "    history_feasibility_model = {'loss':[],\n",
    "                            'binary_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_binary_accuracy':[]}\n",
    "    print(\"Training the Feasibility model:\")\n",
    "    #tqdm.write(\"\\nTraining the Feasibility model:\")\n",
    "    #Value to store best validation accuracy\n",
    "    max_feasibility_val_accuracy = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "        #tqdm.write('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible,\n",
    "                     batch_train_answer_start_seq_padded,\n",
    "                     batch_train_answer_end_seq_padded)) in tqdm(enumerate(train_dataset.take(steps_per_epoch)),\n",
    "                                                             total = num_training_samples//batch_size,\n",
    "                                                             desc=\"[Training Feasibility]\"):\n",
    "            batch_loss = feasibility_train_step(batch_train_context_padded_seq,\n",
    "                                                batch_train_question_seq_padded,\n",
    "                                                batch_train_answer_impossible,\n",
    "                                                encoder_model,\n",
    "                                                answer_model,\n",
    "                                                feasibility_model)\n",
    "            total_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = feasibility_train_acc_metric.result()\n",
    "        print('Epoch {} - Train Loss {:.4f} Train Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                 epoch_training_loss, \n",
    "                                                                                 epoch_training_accuracy))\n",
    "        feasibility_train_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        #print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible,\n",
    "                     batch_val_answer_start_seq_padded,\n",
    "                     batch_val_answer_end_seq_padded)) in tqdm(enumerate(val_dataset.take(steps_per_epoch_val)),\n",
    "                                                           total = num_validation_samples//batch_size,\n",
    "                                                           desc=\"[Validating Feasibility]\"):\n",
    "            batch_loss = feasibility_val_step(batch_val_context_padded_seq,\n",
    "                                              batch_val_question_seq_padded,\n",
    "                                              batch_val_answer_impossible,\n",
    "                                              encoder_model,\n",
    "                                              answer_model,\n",
    "                                              feasibility_model)\n",
    "            total_val_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = feasibility_val_acc_metric.result()\n",
    "        print('Epoch {} - Validation Loss {:.4f} Validation Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                           epoch_val_loss, \n",
    "                                                                                           epoch_val_accuracy))\n",
    "        feasibility_val_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        #print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_feasibility_model['loss'].append(epoch_training_loss)\n",
    "        history_feasibility_model['binary_accuracy'].append(epoch_training_accuracy)\n",
    "        history_feasibility_model['val_loss'].append(epoch_val_loss)\n",
    "        history_feasibility_model['val_binary_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        #Save only the best model\n",
    "        if epoch_val_accuracy >= max_feasibility_val_accuracy:\n",
    "            max_feasibility_val_accuracy = epoch_val_accuracy\n",
    "            feasibility_model.save(ExperimentNo+'span_model_LSTM_feasibility_model.h5')\n",
    "            print('Best Epoch so far! Epoch No {}'.format(epoch + 1))\n",
    "        print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "        #tqdm.write('\\nTime taken for epoch {} sec'.format(time.time() - start))\n",
    "    \n",
    "    #feasibility_model.save(ExperimentNo+'span_model_LSTM_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'span_model_LSTM_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model, file_history)\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    train_question_output,\\\n",
    "    train_context_output = encoder_model(train_context_padded_seq[:num_training_samples],\n",
    "                                         train_question_seq_padded[:num_training_samples])\n",
    "    \n",
    "    train_start_logits,train_end_logits = answer_model(train_question_output,train_context_output)\n",
    "    \n",
    "    val_question_output,\\\n",
    "    val_context_output = encoder_model(val_context_padded_seq[:num_validation_samples],\n",
    "                                                           val_question_seq_padded[:num_validation_samples])\n",
    "    \n",
    "    val_start_logits,val_end_logits = answer_model(val_question_output,val_context_output)\n",
    "    \n",
    "    \n",
    "    print(\"training the feasibility model\")\n",
    "    history_feasibility_model = feasibility_model.fit([train_question_output,\n",
    "                                                       train_context_output,\n",
    "                                                       train_start_logits,\n",
    "                                                       train_end_logits],\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            ([val_question_output,\n",
    "                                                              val_context_output,\n",
    "                                                              val_start_logits,\n",
    "                                                              val_end_logits],\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "\n",
    "    feasibility_model.save(ExperimentNo+'span_model_LSTM_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'span_model_LSTM_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model.history, file_history)\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                          l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                          l2_regularizer_weight = l2_regularizer_weight )\n",
    "\n",
    "    # train on 1 row so that weights can be loaded \n",
    "    #Train the Answer Model and Encoder Model\n",
    "    inference_answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    inference_answer_loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, \n",
    "                                                                           reduction='none')\n",
    "    @tf.function\n",
    "    def inference_answer_loss_function(real_start, real_end,pred_start,pred_end):\n",
    "        \n",
    "        start_loss = inference_answer_loss_object(real_start,pred_start)\n",
    "        end_loss = inference_answer_loss_object(real_end,pred_end)\n",
    "        return tf.reduce_mean(start_loss+end_loss)\n",
    "\n",
    "    @tf.function\n",
    "    def inference_answer_train_step(inp,ques,targ_start,targ_end,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            question_output,context_output = encoder_model(inp,ques)\n",
    "            start_logits,end_logits = answer_model(question_output,context_output)\n",
    "            loss += inference_answer_loss_function(targ_start,targ_end,start_logits,end_logits)\n",
    "            batch_loss = loss/inp.shape[0]\n",
    "            #train_acc_metric(targ_start,start_logits)\n",
    "            #train_acc_metric(targ_end,end_logits)\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            inference_answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    inference_answer_train_step(train_context_padded_seq[:1],\n",
    "                              train_question_seq_padded[:1],\n",
    "                              train_answer_start_seq_padded[:1],\n",
    "                              train_answer_end_seq_padded[:1],\n",
    "                              inference_encoder_model,\n",
    "                              inference_answer_model)\n",
    "        \n",
    "    inference_answer_model.load_weights(ExperimentNo+'span_model_LSTM_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'span_model_LSTM_encoder_model.h5')\n",
    "    inference_feasibility_model.load_weights(ExperimentNo+'span_model_LSTM_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running  Experiment0\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Training Answer]:   0%|          | 0/480 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the answer model:\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]:  36%|███▌      | 172/480 [04:07<07:15,  1.42s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-fb4f41b8622e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                     \u001b[0mnum_validation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                     batch_size = 128)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-82612ec187f5>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(Experiment_Dic, Experiment_No, embedding_matrix, ndim, tpu_enabled, num_training_samples, num_validation_samples, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                            \u001b[0mbatch_train_answer_end_seq_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                                            \u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                                            answer_model)\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m#if batch % 50 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "for i in range(8):\n",
    "    run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                    Experiment_No=i,\n",
    "                    embedding_matrix=embedding_matrix,\n",
    "                    ndim = ndim,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024*60,\n",
    "                    num_validation_samples = 1024*6,\n",
    "                    num_epochs = 25,\n",
    "                    batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=0,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: which former enemy wanted an alliance with the romans\n",
      "Question Infeasible to answer prediction: [[0.33498928]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: seleucid\n",
      "Actual answer: philip\n",
      "question: by what year did japanese strategists expand their concept of the greater east asia co prosperity sphere\n",
      "Question Infeasible to answer prediction: [[0.2944325]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: 1935\n",
      "Actual answer: 1940\n",
      "question: what did the british railway stop using in 1954\n",
      "Question Infeasible to answer prediction: [[0.32720292]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: 1928 by the international astronomical union observations at the greenwich observatory\n",
      "Actual answer: \n",
      "question: how old is the primitive plying insect found by the university\n",
      "Question Infeasible to answer prediction: [[0.33295247]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: rhyniognatha\n",
      "Actual answer: 300 million year old\n",
      "question: what role did women fill in the church\n",
      "Question Infeasible to answer prediction: [[0.31995904]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: spinning or brewing\n",
      "Actual answer: nuns\n",
      "question: what was the title of buckcherry s 2007 hit single\n",
      "Question Infeasible to answer prediction: [[0.32527098]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: s josh homme\n",
      "Actual answer: sorry\n",
      "question: how are the words bitumen and asphalt frequently used\n",
      "Question Infeasible to answer prediction: [[0.35176083]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: brea\n",
      "Actual answer: interchangeably\n",
      "question: what particular pollutant was schwarzenegger addressing with his 2006 executive order\n",
      "Question Infeasible to answer prediction: [[0.33250982]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: 17\n",
      "Actual answer: carbon dioxide emissions\n",
      "question: which famous prince was a patron at the royal school of mines\n",
      "Question Infeasible to answer prediction: [[0.32362834]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: sir henry de la beche\n",
      "Actual answer: prince albert\n",
      "question: what is the common application of gaas\n",
      "Question Infeasible to answer prediction: [[0.35432285]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: p–n–p\n",
      "Actual answer: high frequency applications\n"
     ]
    }
   ],
   "source": [
    "#Some Examples from Training Set\n",
    "for seq_index in range(500,510):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sentence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_answer_model)\n",
    "    question_output,context_output = inference_encoder_model(context_input_seq,question_input_seq)\n",
    "    #start_logits,end_logits = inference_answer_model(question_output,context_output)         \n",
    "    feasibility_pred = inference_feasibility_model.predict([question_output,\n",
    "                                                            context_output])\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print(\"Question Infeasible to answer prediction:\",feasibility_pred)\n",
    "    print(\"Question Infeasible to answer actual value\",train_answer_impossible[seq_index: seq_index+ 1])\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what failed to rally the supporters of the failed revolt\n",
      "Question Infeasible to answer prediction: [[0.33819032]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: 717\n",
      "Actual answer: \n",
      "question: how long is the term for vice chair of the supervisory board\n",
      "Question Infeasible to answer prediction: [[0.30136347]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: central bank ncb\n",
      "Actual answer: chosen from among the members of the ecb s executive board\n",
      "question: what does not apply to any branch of government\n",
      "Question Infeasible to answer prediction: [[0.33663788]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: side of this debate that separation of powers means that powers are shared among different branches\n",
      "Actual answer: \n",
      "question: what is the name of the famous bank located in boston\n",
      "Question Infeasible to answer prediction: [[0.315433]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: f kennedy federal office building the thomas p\n",
      "Actual answer: federal reserve bank of boston\n",
      "question: which was released first cd r or cd rw\n",
      "Question Infeasible to answer prediction: [[0.36916053]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: rom\n",
      "Actual answer: \n",
      "question: what script was used in the northwest of india\n",
      "Question Infeasible to answer prediction: [[0.3407157]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: subcontinent sometime between the fourth and eighth centuries the gupta script derived from brahmi\n",
      "Actual answer: kharosthi\n",
      "question: modern romance developed into what type of narrative\n",
      "Question Infeasible to answer prediction: [[0.3771278]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: instructional literature proliferated\n",
      "Actual answer: \n",
      "question: what medicine is used to remove blockages\n",
      "Question Infeasible to answer prediction: [[0.34102046]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: myocardial infarction\n",
      "Actual answer: \n",
      "question: to what standard are hokken dialects similar to\n",
      "Question Infeasible to answer prediction: [[0.36868644]]\n",
      "Question Infeasible to answer actual value [1]\n",
      "Predicted Answer: 文讀 bûn\n",
      "Actual answer: \n",
      "question: what remains a controversy regarding how to refer to the indigenous peoples of the americas\n",
      "Question Infeasible to answer prediction: [[0.30608332]]\n",
      "Question Infeasible to answer actual value [0]\n",
      "Predicted Answer: earlier explorers and colonists\n",
      "Actual answer: the native american name\n"
     ]
    }
   ],
   "source": [
    "#Some examples from validation set\n",
    "for seq_index in range(800,810):\n",
    "    # Take one sequence (part of the Validation test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = val_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = val_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sentence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_answer_model)\n",
    "    question_output,context_output = inference_encoder_model(context_input_seq,question_input_seq)\n",
    "    #start_logits,end_logits = inference_answer_model(question_output,context_output)         \n",
    "    feasibility_pred = inference_feasibility_model.predict([question_output,\n",
    "                                                            context_output])\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in val_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print(\"Question Infeasible to answer prediction:\",feasibility_pred)\n",
    "    print(\"Question Infeasible to answer actual value\",val_answer_impossible[seq_index: seq_index+ 1])\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in val_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_experiments(ExperimentNumber):\n",
    "    ExperimentNo = \"Experiment\"+str(ExperimentNumber)\n",
    "    if ( os.path.exists(ExperimentNo+'span_model_LSTM_'+'history_feasibility_model') \n",
    "       and os.path.exists(ExperimentNo+'span_model_LSTM_'+'history_answer_model') ) :\n",
    "        \n",
    "        with open(ExperimentNo+'span_model_LSTM_'+'history_feasibility_model', 'rb') as file_history:\n",
    "            feasibility_history = pickle.load( file_history )\n",
    "\n",
    "        with open(ExperimentNo+'span_model_LSTM_'+'history_answer_model', 'rb') as file_history:\n",
    "            answer_history = pickle.load( file_history )\n",
    "\n",
    "        fig, axs = plt.subplots(2, 2,figsize=(15,15))\n",
    "        fig.suptitle('++++++++++++++++++++++++'+ExperimentNo+'++++++++++++++++++++++',fontsize=14)\n",
    "        #plot Answer Module\n",
    "        # Plot training & validation accuracy values\n",
    "        axs[0, 0].plot(answer_history['categorical_accuracy'])\n",
    "        axs[0, 0].plot(answer_history['val_categorical_accuracy'])\n",
    "        axs[0, 0].set_title(ExperimentNo+' Answer Model accuracy')\n",
    "        axs[0, 0].set_ylabel('Categorical Cross Entropy Accuracy')\n",
    "        axs[0, 0].set_xlabel('Epoch')\n",
    "        axs[0, 0].legend(['Train', 'Val'], loc='upper left')\n",
    "        #plt.show()\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        axs[0, 1].plot(answer_history['loss'])\n",
    "        axs[0, 1].plot(answer_history['val_loss'])\n",
    "        axs[0, 1].set_title(ExperimentNo+' Answer Model loss')\n",
    "        axs[0, 1].set_ylabel('Categorical Cross Entropy Loss')\n",
    "        axs[0, 1].set_xlabel('Epoch')\n",
    "        axs[0, 1].legend(['Train', 'Val'], loc='upper left')\n",
    "        #plt.show()\n",
    "\n",
    "\n",
    "        #plot Feasibility Module\n",
    "        # Plot training & validation accuracy values\n",
    "        axs[1, 0].plot(feasibility_history['binary_accuracy'])\n",
    "        axs[1, 0].plot(feasibility_history['val_binary_accuracy'])\n",
    "        axs[1, 0].set_title(ExperimentNo+' Feasibility Model accuracy')\n",
    "        axs[1, 0].set_ylabel('Binary Cross Entropy Accuracy')\n",
    "        axs[1, 0].set_xlabel('Epoch')\n",
    "        axs[1, 0].legend(['Train', 'Val'], loc='upper left')\n",
    "        #plt.show()\n",
    "\n",
    "        # Plot training & validation loss values\n",
    "        axs[1, 1].plot(feasibility_history['loss'])\n",
    "        axs[1, 1].plot(feasibility_history['val_loss'])\n",
    "        axs[1, 1].set_title(ExperimentNo+' Feasibility Model loss')\n",
    "        axs[1, 1].set_ylabel('Binary Cross Entropy Loss')\n",
    "        axs[1, 1].set_xlabel('Epoch')\n",
    "        axs[1, 1].legend(['Train', 'Val'], loc='upper left')\n",
    "        #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range (8):\n",
    "    plot_experiments(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
