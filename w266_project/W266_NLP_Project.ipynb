{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 13056,
     "status": "ok",
     "timestamp": 1584123191146,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "LIyS_i--ghlY",
    "outputId": "0be9e53a-d3ec-4910-e2df-556293b2b72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.x selected.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1584123192145,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "jHPBK1duiyNd",
    "outputId": "0ef0946c-03d4-4fa0-c152-716b1eb14dc3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 382
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1584123194641,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "xX4Md1_Ci_Bb",
    "outputId": "a374a962-21b7-4924-f5f4-1d409fcca576"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Squad2.0',\n",
       " 'dev-v2.0.json',\n",
       " 'train-v2.0.json',\n",
       " 'w266_common',\n",
       " '__pycache__',\n",
       " 'data',\n",
       " 'model.h5',\n",
       " 'encoder_model.h5',\n",
       " 'decoder_model.h5',\n",
       " 'hhhhhhh.txt',\n",
       " 'history_tpu_history',\n",
       " 'tpu_encoder_model.h5',\n",
       " 'tpu_decoder_model.h5',\n",
       " 'tpu_model.h5',\n",
       " 'glove_helper.py',\n",
       " 'tpu_answer_model_2.h5',\n",
       " 'tpu_encoder_model_2.h5',\n",
       " 'tpu_decoder_model_2.h5',\n",
       " 'tpu_feasibility_model_2.h5',\n",
       " 'tpu_history_answer_model2',\n",
       " 'tpu_history_feasibility_model2']"
      ]
     },
     "execution_count": 3,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4466,
     "status": "ok",
     "timestamp": 1584123202496,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "_yVNnCN9gSdI",
    "outputId": "2e8bbb0f-2f70-4450-e3da-ad9c7d396846"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "  \"\"\"\n",
    "  Load the data to a pandas dataframe.\n",
    "  Dataframe Columns:\n",
    "    title\n",
    "    para_index\n",
    "    context\n",
    "    q_index\n",
    "    q_id\n",
    "    q_isimpossible\n",
    "    q_question\n",
    "    q_anscount - number of answers\n",
    "    q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for pdata in data['data']:\n",
    "    for para in pdata['paragraphs']:\n",
    "      for q in para['qas']:\n",
    "        result.append({\n",
    "            'title' : pdata['title'],\n",
    "            'context' : para['context'],\n",
    "            'q_id' : q['id'],\n",
    "            'q_isimpossible' : q['is_impossible'],\n",
    "            'q_question' : q['question'],\n",
    "            'q_anscount' : len(q['answers']),\n",
    "            'q_answers' : [a for a in q['answers']],\n",
    "            'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "            'context_lowercase': para['context'].lower(),\n",
    "            'q_question_lowercase' : q['question'].lower(),\n",
    "            'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']],\n",
    "            \n",
    "        })\n",
    "\n",
    "  return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "  \"\"\"\n",
    "  Download a file from google drive with the shared link\n",
    "  \"\"\" \n",
    "  data = load_json_file(filename)\n",
    "  return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijHd2kHMiU06"
   },
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60642,
     "status": "ok",
     "timestamp": 1584123271465,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "Wx9DgUqBncsq",
    "outputId": "d50ea2a9-2333-462e-f652-b143388c346b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 60642,
     "status": "ok",
     "timestamp": 1584123271465,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "Wx9DgUqBncsq",
    "outputId": "d50ea2a9-2333-462e-f652-b143388c346b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "          if i <= num_words:\n",
    "            vocab[word] = i\n",
    "        else:\n",
    "          vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible_shuffled = answer_impossible[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible_shuffled[:train_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible_shuffled[train_samples:end_samples]\n",
    "    return (train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible)\n",
    "\n",
    "train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,\\\n",
    "train_answer_impossible,\\\n",
    "val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,\\\n",
    "val_answer_impossible\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 17627,
     "status": "ok",
     "timestamp": 1584123292207,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "zp2hk-eImJ5M",
    "outputId": "f1c4c5f7-9f83-4e06-e9b6-81f1b2b827fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "ndim = 100\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMAi-3h74Je1"
   },
   "outputs": [],
   "source": [
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  attentionType = 0, # 0 means Luong's 1 means BahdanauUnits\n",
    "                  BahdanauUnits = 32):\n",
    "    \n",
    "    class BahdanauAttention(layers.Layer):\n",
    "        def __init__(self, units):\n",
    "            super(BahdanauAttention, self).__init__()\n",
    "            self.W1 = layers.Dense(units)\n",
    "            self.W2 = layers.Dense(units)\n",
    "            self.V =  layers.Dense(1)\n",
    "\n",
    "        def call(self, inputs):\n",
    "            query = inputs[0]\n",
    "            values = inputs[1]\n",
    "            #print(\"query shape\",query.shape)\n",
    "            #print(\"vaues shape\",values.shape)\n",
    "            # query hidden state shape == (batch_size, hidden size)\n",
    "            # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "            # values shape == (batch_size, max_len, hidden size)\n",
    "            # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "            #query_with_time_axis = tf.expand_dims(query, 1)\n",
    "            #print(\"query_with_time_axis shape\",query_with_time_axis.shape)\n",
    "            # score shape == (batch_size, max_length, 1)\n",
    "            # we get 1 at the last axis because we are applying score to self.V\n",
    "            # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "            score = self.V(keras.activations.tanh(\n",
    "            self.W1(query) + self.W2(values)))\n",
    "\n",
    "            # attention_weights shape == (batch_size, max_length, 1)\n",
    "            attention_weights = keras.activations.softmax(score, axis=1)\n",
    "\n",
    "            # context_vector shape after sum == (batch_size, hidden_size)\n",
    "            context_vector = attention_weights * values\n",
    "            #context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "            #print(\"context vector shape\",context_vector.shape)\n",
    "\n",
    "            return context_vector\n",
    "\n",
    "    def create_memory_using_attention(num_episodes,query,context_outputs,attention_type,BahdanauUnits=32):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        Dense_Layer_concat_M_q = layers.Dense(units=query.shape[1],activation='tanh')\n",
    "        if attention_type == 0:\n",
    "            #use keras attention which is Luong's\n",
    "            attention_layer = layers.Attention()\n",
    "        else:\n",
    "            # use BahdanauAttention\n",
    "            attention_layer = BahdanauAttention(BahdanauUnits)\n",
    "        for i in range(num_episodes):\n",
    "            m_plus_q =layers.concatenate(inputs=[m,query],axis=1)\n",
    "            if attention_type ==0:\n",
    "                m_plus_q = Dense_Layer_concat_M_q(m_plus_q)\n",
    "                m_plus_q = layers.BatchNormalization()(m_plus_q)\n",
    "            m_plus_q_with_time_axis = tf.keras.backend.expand_dims(m_plus_q, 1)\n",
    "            context_with_attention = attention_layer([m_plus_q_with_time_axis,context_outputs])\n",
    "            m = tf.keras.backend.sum(context_with_attention, axis=1)\n",
    "\n",
    "\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,ndim,mask_zero=True,name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,dropout=dropout_rate,\n",
    "                                                      recurrent_dropout= dropout_rate,\n",
    "                                                      return_sequences=True),name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,ndim,mask_zero=True,name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,dropout=dropout_rate,\n",
    "                                                    recurrent_dropout= dropout_rate,\n",
    "                                                    return_sequences=True),\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,dropout=dropout_rate,\n",
    "                                                    recurrent_dropout= dropout_rate,\n",
    "                                                    return_sequences=False),\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,dropout=dropout_rate,\n",
    "                                                    recurrent_dropout= dropout_rate,\n",
    "                                                    return_sequences=False),\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,dropout=dropout_rate,\n",
    "                                                    recurrent_dropout= dropout_rate,\n",
    "                                                    return_sequences=True),\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_memory_using_attention(num_episodes,question_outputs,context_outputs,attentionType,BahdanauUnits)\n",
    "    #print(m.shape)\n",
    "    #print(context_outputs.shape)\n",
    "    #print(question_outputs.shape)\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],name='Concatenation_Memory_Question',axis=1)\n",
    "    #answer_module\n",
    "\n",
    "    answer_input = Input(shape=(None,),dtype='int32',name='Answer_Input')\n",
    "    answer_embeddings = layers.Embedding(vocab_size+1,ndim,mask_zero=True,name='Answer_Embedding')(answer_input)\n",
    "    for i in range(num_layers_gru):\n",
    "        answer_decoder_layers = layers.GRU(concatenated_tensor.shape[1],dropout=dropout_rate,\n",
    "                                           recurrent_dropout= dropout_rate,\n",
    "                                           return_sequences=True,\n",
    "                                           return_state=True,\n",
    "                                           name='Answer_GRU_Layer'+str(i)\n",
    "                                           )\n",
    "        if i==0:\n",
    "            answer_outputs,_ = answer_decoder_layers(answer_embeddings,initial_state=concatenated_tensor)\n",
    "        else:\n",
    "            answer_outputs,_ = answer_decoder_layers(answer_outputs)\n",
    "        answer_outputs = layers.BatchNormalization()(answer_outputs)\n",
    "    answer_decoder_dense = layers.TimeDistributed(layers.Dense(vocab_size+1, activation='softmax')\n",
    "                                                  ,name='Answer_output')\n",
    "    answer_decoder_outputs = answer_decoder_dense(answer_outputs)\n",
    "\n",
    "    answer_model = Model([context_input,question_input,answer_input],answer_decoder_outputs)\n",
    "    answer_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    answer_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    answer_model.get_layer(\"Answer_Embedding\").set_weights([embedding_matrix])\n",
    "    answer_model.get_layer(\"Answer_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    feasibility_input = Input(shape=(concatenated_tensor.shape[1],), name=\"FeasibilityInput\")\n",
    "    for i in range(num_dense_layers_feasibility):\n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                            activation='relu',name='feasibility_layer_'+str(i))(feasibility_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                            activation='relu',name='feasibility_layer_'+str(i))(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',name='feasibility_output')(dropout_layer)\n",
    "    feasibility_model = Model(feasibility_input,feasibility_output)\n",
    "\n",
    "    encoder_model = Model([context_input,question_input], concatenated_tensor)\n",
    "    decoder_inputs = answer_input\n",
    "    decoder_state_input_h = Input(shape=(None,), name=\"DecoderStateInput\")\n",
    "\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        decoder_layers = answer_model.get_layer('Answer_GRU_Layer'+str(i))\n",
    "        if i==0:\n",
    "            decoder_outputs, decoder_state_h = decoder_layers(answer_embeddings,initial_state=decoder_state_input_h)\n",
    "        else:\n",
    "            decoder_outputs, decoder_state_h = decoder_layers(decoder_outputs)\n",
    "\n",
    "    decoder_dense =  answer_model.get_layer('Answer_output')(decoder_outputs)\n",
    "\n",
    "    decoder_model = Model(\n",
    "                        [decoder_inputs] + [decoder_state_input_h],\n",
    "                        [decoder_dense] + [decoder_state_h])\n",
    "    return (answer_model,encoder_model,decoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vMAi-3h74Je1"
   },
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sequence(context_input_seq,\n",
    "                    question_input_seq,\n",
    "                    encoder_model,\n",
    "                    decoder_model):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict([context_input_seq,question_input_seq])\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    current_step = 0\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0] = vocab[\"<s>\"]\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h = decoder_model.predict([target_seq] + [states_value])\n",
    "        current_step += 1\n",
    "        # Sample a token\n",
    "        #print(output_tokens.shape)\n",
    "        #print(output_tokens[0,0,0])\n",
    "        #print(output_tokens[0,0,32984])\n",
    "        sampled_token_index = np.argmax(output_tokens[0, 0, :])\n",
    "        #print(sampled_token_index)\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \" \"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == '</s>' or len(decoded_sentence) > answer_maxlen:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update states\n",
    "        states_value = h\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10612846,
     "status": "ok",
     "timestamp": 1584133914057,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "5TFJqcOrkEnL",
    "outputId": "a71f1c8c-811f-4859-8ef0-e46ff0415601"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.89.189.58:8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Initializing the TPU system: 10.89.189.58:8470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Clearing out eager caches\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished initializing TPU system.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Found TPU system:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Workers: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context output shape (None, None, 160)\n",
      "Train on 20480 samples, validate on 1024 samples\n",
      "Epoch 1/200\n",
      "20480/20480 [==============================] - 109s 5ms/sample - loss: 0.6281 - sparse_categorical_accuracy: 0.2449 - val_loss: 0.4041 - val_sparse_categorical_accuracy: 0.5046\n",
      "Epoch 2/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4928 - sparse_categorical_accuracy: 0.3226 - val_loss: 0.3942 - val_sparse_categorical_accuracy: 0.5430\n",
      "Epoch 3/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4815 - sparse_categorical_accuracy: 0.3515 - val_loss: 0.3973 - val_sparse_categorical_accuracy: 0.3919\n",
      "Epoch 4/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4625 - sparse_categorical_accuracy: 0.4360 - val_loss: 0.4274 - val_sparse_categorical_accuracy: 0.3436\n",
      "Epoch 5/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4532 - sparse_categorical_accuracy: 0.4486 - val_loss: 0.4519 - val_sparse_categorical_accuracy: 0.3345\n",
      "Epoch 6/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4490 - sparse_categorical_accuracy: 0.4517 - val_loss: 0.4631 - val_sparse_categorical_accuracy: 0.3295\n",
      "Epoch 7/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4461 - sparse_categorical_accuracy: 0.4534 - val_loss: 0.4768 - val_sparse_categorical_accuracy: 0.3288\n",
      "Epoch 8/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4437 - sparse_categorical_accuracy: 0.4545 - val_loss: 0.4858 - val_sparse_categorical_accuracy: 0.3286\n",
      "Epoch 9/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4420 - sparse_categorical_accuracy: 0.4546 - val_loss: 0.4895 - val_sparse_categorical_accuracy: 0.3286\n",
      "Epoch 10/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4403 - sparse_categorical_accuracy: 0.4555 - val_loss: 0.4938 - val_sparse_categorical_accuracy: 0.3286\n",
      "Epoch 11/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4387 - sparse_categorical_accuracy: 0.4558 - val_loss: 0.4998 - val_sparse_categorical_accuracy: 0.3281\n",
      "Epoch 12/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4375 - sparse_categorical_accuracy: 0.4560 - val_loss: 0.5033 - val_sparse_categorical_accuracy: 0.3286\n",
      "Epoch 13/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4356 - sparse_categorical_accuracy: 0.4564 - val_loss: 0.5082 - val_sparse_categorical_accuracy: 0.3279\n",
      "Epoch 14/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4338 - sparse_categorical_accuracy: 0.4567 - val_loss: 0.5103 - val_sparse_categorical_accuracy: 0.3269\n",
      "Epoch 15/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4303 - sparse_categorical_accuracy: 0.4577 - val_loss: 0.5073 - val_sparse_categorical_accuracy: 0.3176\n",
      "Epoch 16/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4224 - sparse_categorical_accuracy: 0.4734 - val_loss: 0.5183 - val_sparse_categorical_accuracy: 0.2981\n",
      "Epoch 17/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4180 - sparse_categorical_accuracy: 0.4770 - val_loss: 0.5263 - val_sparse_categorical_accuracy: 0.2969\n",
      "Epoch 18/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4132 - sparse_categorical_accuracy: 0.4791 - val_loss: 0.5386 - val_sparse_categorical_accuracy: 0.2940\n",
      "Epoch 19/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4072 - sparse_categorical_accuracy: 0.4830 - val_loss: 0.5497 - val_sparse_categorical_accuracy: 0.2909\n",
      "Epoch 20/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.4014 - sparse_categorical_accuracy: 0.4853 - val_loss: 0.5691 - val_sparse_categorical_accuracy: 0.2864\n",
      "Epoch 21/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3962 - sparse_categorical_accuracy: 0.4872 - val_loss: 0.5804 - val_sparse_categorical_accuracy: 0.2838\n",
      "Epoch 22/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3897 - sparse_categorical_accuracy: 0.4884 - val_loss: 0.5959 - val_sparse_categorical_accuracy: 0.2828\n",
      "Epoch 23/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3840 - sparse_categorical_accuracy: 0.4891 - val_loss: 0.5977 - val_sparse_categorical_accuracy: 0.2821\n",
      "Epoch 24/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3773 - sparse_categorical_accuracy: 0.4897 - val_loss: 0.6154 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 25/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3704 - sparse_categorical_accuracy: 0.4902 - val_loss: 0.6224 - val_sparse_categorical_accuracy: 0.2800\n",
      "Epoch 26/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3639 - sparse_categorical_accuracy: 0.4912 - val_loss: 0.6321 - val_sparse_categorical_accuracy: 0.2802\n",
      "Epoch 27/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3575 - sparse_categorical_accuracy: 0.4910 - val_loss: 0.6363 - val_sparse_categorical_accuracy: 0.2792\n",
      "Epoch 28/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3509 - sparse_categorical_accuracy: 0.4917 - val_loss: 0.6364 - val_sparse_categorical_accuracy: 0.2788\n",
      "Epoch 29/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3446 - sparse_categorical_accuracy: 0.4927 - val_loss: 0.6450 - val_sparse_categorical_accuracy: 0.2797\n",
      "Epoch 30/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3384 - sparse_categorical_accuracy: 0.4934 - val_loss: 0.6549 - val_sparse_categorical_accuracy: 0.2785\n",
      "Epoch 31/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3326 - sparse_categorical_accuracy: 0.4934 - val_loss: 0.6583 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 32/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3268 - sparse_categorical_accuracy: 0.4938 - val_loss: 0.6714 - val_sparse_categorical_accuracy: 0.2790\n",
      "Epoch 33/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3211 - sparse_categorical_accuracy: 0.4945 - val_loss: 0.6715 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 34/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3154 - sparse_categorical_accuracy: 0.4953 - val_loss: 0.6812 - val_sparse_categorical_accuracy: 0.2790\n",
      "Epoch 35/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3099 - sparse_categorical_accuracy: 0.4956 - val_loss: 0.6783 - val_sparse_categorical_accuracy: 0.2783\n",
      "Epoch 36/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.3044 - sparse_categorical_accuracy: 0.4965 - val_loss: 0.6853 - val_sparse_categorical_accuracy: 0.2785\n",
      "Epoch 37/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2990 - sparse_categorical_accuracy: 0.4976 - val_loss: 0.6940 - val_sparse_categorical_accuracy: 0.2781\n",
      "Epoch 38/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2942 - sparse_categorical_accuracy: 0.4984 - val_loss: 0.6996 - val_sparse_categorical_accuracy: 0.2778\n",
      "Epoch 39/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2889 - sparse_categorical_accuracy: 0.4990 - val_loss: 0.7083 - val_sparse_categorical_accuracy: 0.2769\n",
      "Epoch 40/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2838 - sparse_categorical_accuracy: 0.5009 - val_loss: 0.7094 - val_sparse_categorical_accuracy: 0.2766\n",
      "Epoch 41/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2793 - sparse_categorical_accuracy: 0.5025 - val_loss: 0.7145 - val_sparse_categorical_accuracy: 0.2769\n",
      "Epoch 42/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2743 - sparse_categorical_accuracy: 0.5046 - val_loss: 0.7166 - val_sparse_categorical_accuracy: 0.2764\n",
      "Epoch 43/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2699 - sparse_categorical_accuracy: 0.5065 - val_loss: 0.7231 - val_sparse_categorical_accuracy: 0.2759\n",
      "Epoch 44/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2652 - sparse_categorical_accuracy: 0.5093 - val_loss: 0.7259 - val_sparse_categorical_accuracy: 0.2766\n",
      "Epoch 45/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2609 - sparse_categorical_accuracy: 0.5119 - val_loss: 0.7327 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 46/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2565 - sparse_categorical_accuracy: 0.5159 - val_loss: 0.7387 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 47/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2523 - sparse_categorical_accuracy: 0.5186 - val_loss: 0.7446 - val_sparse_categorical_accuracy: 0.2745\n",
      "Epoch 48/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2483 - sparse_categorical_accuracy: 0.5212 - val_loss: 0.7458 - val_sparse_categorical_accuracy: 0.2745\n",
      "Epoch 49/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2444 - sparse_categorical_accuracy: 0.5247 - val_loss: 0.7449 - val_sparse_categorical_accuracy: 0.2750\n",
      "Epoch 50/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2403 - sparse_categorical_accuracy: 0.5288 - val_loss: 0.7551 - val_sparse_categorical_accuracy: 0.2757\n",
      "Epoch 51/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2366 - sparse_categorical_accuracy: 0.5321 - val_loss: 0.7574 - val_sparse_categorical_accuracy: 0.2728\n",
      "Epoch 52/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2331 - sparse_categorical_accuracy: 0.5337 - val_loss: 0.7616 - val_sparse_categorical_accuracy: 0.2742\n",
      "Epoch 53/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2298 - sparse_categorical_accuracy: 0.5388 - val_loss: 0.7679 - val_sparse_categorical_accuracy: 0.2723\n",
      "Epoch 54/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2260 - sparse_categorical_accuracy: 0.5423 - val_loss: 0.7665 - val_sparse_categorical_accuracy: 0.2716\n",
      "Epoch 55/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2226 - sparse_categorical_accuracy: 0.5462 - val_loss: 0.7713 - val_sparse_categorical_accuracy: 0.2714\n",
      "Epoch 56/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2200 - sparse_categorical_accuracy: 0.5486 - val_loss: 0.7705 - val_sparse_categorical_accuracy: 0.2726\n",
      "Epoch 57/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2166 - sparse_categorical_accuracy: 0.5521 - val_loss: 0.7768 - val_sparse_categorical_accuracy: 0.2704\n",
      "Epoch 58/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2137 - sparse_categorical_accuracy: 0.5553 - val_loss: 0.7772 - val_sparse_categorical_accuracy: 0.2709\n",
      "Epoch 59/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2110 - sparse_categorical_accuracy: 0.5579 - val_loss: 0.7813 - val_sparse_categorical_accuracy: 0.2714\n",
      "Epoch 60/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2078 - sparse_categorical_accuracy: 0.5620 - val_loss: 0.7823 - val_sparse_categorical_accuracy: 0.2688\n",
      "Epoch 61/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2054 - sparse_categorical_accuracy: 0.5648 - val_loss: 0.7873 - val_sparse_categorical_accuracy: 0.2697\n",
      "Epoch 62/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2030 - sparse_categorical_accuracy: 0.5674 - val_loss: 0.7917 - val_sparse_categorical_accuracy: 0.2697\n",
      "Epoch 63/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.2002 - sparse_categorical_accuracy: 0.5706 - val_loss: 0.7943 - val_sparse_categorical_accuracy: 0.2676\n",
      "Epoch 64/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1979 - sparse_categorical_accuracy: 0.5736 - val_loss: 0.7957 - val_sparse_categorical_accuracy: 0.2690\n",
      "Epoch 65/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1958 - sparse_categorical_accuracy: 0.5759 - val_loss: 0.7975 - val_sparse_categorical_accuracy: 0.2673\n",
      "Epoch 66/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1935 - sparse_categorical_accuracy: 0.5772 - val_loss: 0.8015 - val_sparse_categorical_accuracy: 0.2688\n",
      "Epoch 67/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1910 - sparse_categorical_accuracy: 0.5816 - val_loss: 0.8041 - val_sparse_categorical_accuracy: 0.2671\n",
      "Epoch 68/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1889 - sparse_categorical_accuracy: 0.5827 - val_loss: 0.8036 - val_sparse_categorical_accuracy: 0.2673\n",
      "Epoch 69/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1867 - sparse_categorical_accuracy: 0.5867 - val_loss: 0.8074 - val_sparse_categorical_accuracy: 0.2669\n",
      "Epoch 70/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1851 - sparse_categorical_accuracy: 0.5886 - val_loss: 0.8112 - val_sparse_categorical_accuracy: 0.2657\n",
      "Epoch 71/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1824 - sparse_categorical_accuracy: 0.5922 - val_loss: 0.8082 - val_sparse_categorical_accuracy: 0.2647\n",
      "Epoch 72/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1808 - sparse_categorical_accuracy: 0.5929 - val_loss: 0.8139 - val_sparse_categorical_accuracy: 0.2692\n",
      "Epoch 73/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1788 - sparse_categorical_accuracy: 0.5963 - val_loss: 0.8172 - val_sparse_categorical_accuracy: 0.2645\n",
      "Epoch 74/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1772 - sparse_categorical_accuracy: 0.5968 - val_loss: 0.8171 - val_sparse_categorical_accuracy: 0.2650\n",
      "Epoch 75/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1748 - sparse_categorical_accuracy: 0.6013 - val_loss: 0.8163 - val_sparse_categorical_accuracy: 0.2638\n",
      "Epoch 76/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1729 - sparse_categorical_accuracy: 0.6049 - val_loss: 0.8212 - val_sparse_categorical_accuracy: 0.2647\n",
      "Epoch 77/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1716 - sparse_categorical_accuracy: 0.6060 - val_loss: 0.8252 - val_sparse_categorical_accuracy: 0.2650\n",
      "Epoch 78/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1699 - sparse_categorical_accuracy: 0.6082 - val_loss: 0.8261 - val_sparse_categorical_accuracy: 0.2633\n",
      "Epoch 79/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1684 - sparse_categorical_accuracy: 0.6091 - val_loss: 0.8231 - val_sparse_categorical_accuracy: 0.2654\n",
      "Epoch 80/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1667 - sparse_categorical_accuracy: 0.6125 - val_loss: 0.8267 - val_sparse_categorical_accuracy: 0.2642\n",
      "Epoch 81/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1648 - sparse_categorical_accuracy: 0.6153 - val_loss: 0.8324 - val_sparse_categorical_accuracy: 0.2619\n",
      "Epoch 82/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1636 - sparse_categorical_accuracy: 0.6182 - val_loss: 0.8335 - val_sparse_categorical_accuracy: 0.2616\n",
      "Epoch 83/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1618 - sparse_categorical_accuracy: 0.6194 - val_loss: 0.8342 - val_sparse_categorical_accuracy: 0.2616\n",
      "Epoch 84/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1605 - sparse_categorical_accuracy: 0.6219 - val_loss: 0.8356 - val_sparse_categorical_accuracy: 0.2614\n",
      "Epoch 85/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1595 - sparse_categorical_accuracy: 0.6238 - val_loss: 0.8354 - val_sparse_categorical_accuracy: 0.2616\n",
      "Epoch 86/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1580 - sparse_categorical_accuracy: 0.6264 - val_loss: 0.8368 - val_sparse_categorical_accuracy: 0.2619\n",
      "Epoch 87/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1563 - sparse_categorical_accuracy: 0.6280 - val_loss: 0.8395 - val_sparse_categorical_accuracy: 0.2590\n",
      "Epoch 88/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1549 - sparse_categorical_accuracy: 0.6303 - val_loss: 0.8400 - val_sparse_categorical_accuracy: 0.2604\n",
      "Epoch 89/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1538 - sparse_categorical_accuracy: 0.6321 - val_loss: 0.8462 - val_sparse_categorical_accuracy: 0.2571\n",
      "Epoch 90/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1526 - sparse_categorical_accuracy: 0.6345 - val_loss: 0.8478 - val_sparse_categorical_accuracy: 0.2571\n",
      "Epoch 91/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1511 - sparse_categorical_accuracy: 0.6357 - val_loss: 0.8480 - val_sparse_categorical_accuracy: 0.2588\n",
      "Epoch 92/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1500 - sparse_categorical_accuracy: 0.6369 - val_loss: 0.8470 - val_sparse_categorical_accuracy: 0.2592\n",
      "Epoch 93/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1488 - sparse_categorical_accuracy: 0.6395 - val_loss: 0.8504 - val_sparse_categorical_accuracy: 0.2566\n",
      "Epoch 94/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1478 - sparse_categorical_accuracy: 0.6398 - val_loss: 0.8495 - val_sparse_categorical_accuracy: 0.2559\n",
      "Epoch 95/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1461 - sparse_categorical_accuracy: 0.6451 - val_loss: 0.8527 - val_sparse_categorical_accuracy: 0.2564\n",
      "Epoch 96/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1453 - sparse_categorical_accuracy: 0.6455 - val_loss: 0.8542 - val_sparse_categorical_accuracy: 0.2592\n",
      "Epoch 97/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1447 - sparse_categorical_accuracy: 0.6467 - val_loss: 0.8542 - val_sparse_categorical_accuracy: 0.2557\n",
      "Epoch 98/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1433 - sparse_categorical_accuracy: 0.6482 - val_loss: 0.8529 - val_sparse_categorical_accuracy: 0.2597\n",
      "Epoch 99/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1425 - sparse_categorical_accuracy: 0.6498 - val_loss: 0.8555 - val_sparse_categorical_accuracy: 0.2597\n",
      "Epoch 100/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1411 - sparse_categorical_accuracy: 0.6512 - val_loss: 0.8564 - val_sparse_categorical_accuracy: 0.2571\n",
      "Epoch 101/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1403 - sparse_categorical_accuracy: 0.6544 - val_loss: 0.8576 - val_sparse_categorical_accuracy: 0.2538\n",
      "Epoch 102/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1392 - sparse_categorical_accuracy: 0.6559 - val_loss: 0.8592 - val_sparse_categorical_accuracy: 0.2566\n",
      "Epoch 103/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1387 - sparse_categorical_accuracy: 0.6569 - val_loss: 0.8609 - val_sparse_categorical_accuracy: 0.2540\n",
      "Epoch 104/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1376 - sparse_categorical_accuracy: 0.6584 - val_loss: 0.8638 - val_sparse_categorical_accuracy: 0.2561\n",
      "Epoch 105/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1362 - sparse_categorical_accuracy: 0.6604 - val_loss: 0.8625 - val_sparse_categorical_accuracy: 0.2528\n",
      "Epoch 106/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1357 - sparse_categorical_accuracy: 0.6606 - val_loss: 0.8648 - val_sparse_categorical_accuracy: 0.2578\n",
      "Epoch 107/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1346 - sparse_categorical_accuracy: 0.6638 - val_loss: 0.8645 - val_sparse_categorical_accuracy: 0.2549\n",
      "Epoch 108/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1339 - sparse_categorical_accuracy: 0.6641 - val_loss: 0.8638 - val_sparse_categorical_accuracy: 0.2552\n",
      "Epoch 109/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1331 - sparse_categorical_accuracy: 0.6666 - val_loss: 0.8643 - val_sparse_categorical_accuracy: 0.2549\n",
      "Epoch 110/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1321 - sparse_categorical_accuracy: 0.6665 - val_loss: 0.8651 - val_sparse_categorical_accuracy: 0.2521\n",
      "Epoch 111/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1308 - sparse_categorical_accuracy: 0.6704 - val_loss: 0.8691 - val_sparse_categorical_accuracy: 0.2538\n",
      "Epoch 112/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1305 - sparse_categorical_accuracy: 0.6703 - val_loss: 0.8687 - val_sparse_categorical_accuracy: 0.2561\n",
      "Epoch 113/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1292 - sparse_categorical_accuracy: 0.6716 - val_loss: 0.8726 - val_sparse_categorical_accuracy: 0.2545\n",
      "Epoch 114/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1287 - sparse_categorical_accuracy: 0.6720 - val_loss: 0.8717 - val_sparse_categorical_accuracy: 0.2547\n",
      "Epoch 115/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1278 - sparse_categorical_accuracy: 0.6746 - val_loss: 0.8742 - val_sparse_categorical_accuracy: 0.2526\n",
      "Epoch 116/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1271 - sparse_categorical_accuracy: 0.6765 - val_loss: 0.8745 - val_sparse_categorical_accuracy: 0.2497\n",
      "Epoch 117/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1259 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.8748 - val_sparse_categorical_accuracy: 0.2518\n",
      "Epoch 118/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1256 - sparse_categorical_accuracy: 0.6792 - val_loss: 0.8757 - val_sparse_categorical_accuracy: 0.2497\n",
      "Epoch 119/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1246 - sparse_categorical_accuracy: 0.6807 - val_loss: 0.8748 - val_sparse_categorical_accuracy: 0.2509\n",
      "Epoch 120/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1242 - sparse_categorical_accuracy: 0.6800 - val_loss: 0.8742 - val_sparse_categorical_accuracy: 0.2523\n",
      "Epoch 121/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1232 - sparse_categorical_accuracy: 0.6831 - val_loss: 0.8772 - val_sparse_categorical_accuracy: 0.2538\n",
      "Epoch 122/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1228 - sparse_categorical_accuracy: 0.6850 - val_loss: 0.8758 - val_sparse_categorical_accuracy: 0.2542\n",
      "Epoch 123/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1219 - sparse_categorical_accuracy: 0.6846 - val_loss: 0.8756 - val_sparse_categorical_accuracy: 0.2533\n",
      "Epoch 124/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1213 - sparse_categorical_accuracy: 0.6876 - val_loss: 0.8796 - val_sparse_categorical_accuracy: 0.2545\n",
      "Epoch 125/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1208 - sparse_categorical_accuracy: 0.6891 - val_loss: 0.8798 - val_sparse_categorical_accuracy: 0.2502\n",
      "Epoch 126/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1202 - sparse_categorical_accuracy: 0.6893 - val_loss: 0.8804 - val_sparse_categorical_accuracy: 0.2497\n",
      "Epoch 127/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1194 - sparse_categorical_accuracy: 0.6913 - val_loss: 0.8813 - val_sparse_categorical_accuracy: 0.2514\n",
      "Epoch 128/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1186 - sparse_categorical_accuracy: 0.6916 - val_loss: 0.8811 - val_sparse_categorical_accuracy: 0.2507\n",
      "Epoch 129/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1179 - sparse_categorical_accuracy: 0.6927 - val_loss: 0.8803 - val_sparse_categorical_accuracy: 0.2549\n",
      "Epoch 130/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1175 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.8848 - val_sparse_categorical_accuracy: 0.2473\n",
      "Epoch 131/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1169 - sparse_categorical_accuracy: 0.6949 - val_loss: 0.8831 - val_sparse_categorical_accuracy: 0.2530\n",
      "Epoch 132/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1162 - sparse_categorical_accuracy: 0.6953 - val_loss: 0.8825 - val_sparse_categorical_accuracy: 0.2499\n",
      "Epoch 133/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1153 - sparse_categorical_accuracy: 0.6980 - val_loss: 0.8833 - val_sparse_categorical_accuracy: 0.2468\n",
      "Epoch 134/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1150 - sparse_categorical_accuracy: 0.6979 - val_loss: 0.8843 - val_sparse_categorical_accuracy: 0.2461\n",
      "Epoch 135/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1146 - sparse_categorical_accuracy: 0.6992 - val_loss: 0.8841 - val_sparse_categorical_accuracy: 0.2487\n",
      "Epoch 136/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1138 - sparse_categorical_accuracy: 0.7003 - val_loss: 0.8838 - val_sparse_categorical_accuracy: 0.2492\n",
      "Epoch 137/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1131 - sparse_categorical_accuracy: 0.7013 - val_loss: 0.8859 - val_sparse_categorical_accuracy: 0.2466\n",
      "Epoch 138/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1127 - sparse_categorical_accuracy: 0.7024 - val_loss: 0.8865 - val_sparse_categorical_accuracy: 0.2487\n",
      "Epoch 139/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1122 - sparse_categorical_accuracy: 0.7045 - val_loss: 0.8852 - val_sparse_categorical_accuracy: 0.2521\n",
      "Epoch 140/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1118 - sparse_categorical_accuracy: 0.7045 - val_loss: 0.8886 - val_sparse_categorical_accuracy: 0.2492\n",
      "Epoch 141/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1113 - sparse_categorical_accuracy: 0.7048 - val_loss: 0.8887 - val_sparse_categorical_accuracy: 0.2457\n",
      "Epoch 142/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1106 - sparse_categorical_accuracy: 0.7060 - val_loss: 0.8888 - val_sparse_categorical_accuracy: 0.2492\n",
      "Epoch 143/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1102 - sparse_categorical_accuracy: 0.7072 - val_loss: 0.8868 - val_sparse_categorical_accuracy: 0.2473\n",
      "Epoch 144/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1095 - sparse_categorical_accuracy: 0.7096 - val_loss: 0.8884 - val_sparse_categorical_accuracy: 0.2473\n",
      "Epoch 145/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1099 - sparse_categorical_accuracy: 0.7072 - val_loss: 0.8901 - val_sparse_categorical_accuracy: 0.2485\n",
      "Epoch 146/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1086 - sparse_categorical_accuracy: 0.7110 - val_loss: 0.8899 - val_sparse_categorical_accuracy: 0.2471\n",
      "Epoch 147/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1080 - sparse_categorical_accuracy: 0.7128 - val_loss: 0.8892 - val_sparse_categorical_accuracy: 0.2445\n",
      "Epoch 148/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1076 - sparse_categorical_accuracy: 0.7127 - val_loss: 0.8894 - val_sparse_categorical_accuracy: 0.2495\n",
      "Epoch 149/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1072 - sparse_categorical_accuracy: 0.7132 - val_loss: 0.8911 - val_sparse_categorical_accuracy: 0.2459\n",
      "Epoch 150/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1066 - sparse_categorical_accuracy: 0.7151 - val_loss: 0.8912 - val_sparse_categorical_accuracy: 0.2492\n",
      "Epoch 151/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1064 - sparse_categorical_accuracy: 0.7147 - val_loss: 0.8949 - val_sparse_categorical_accuracy: 0.2442\n",
      "Epoch 152/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1061 - sparse_categorical_accuracy: 0.7158 - val_loss: 0.8928 - val_sparse_categorical_accuracy: 0.2476\n",
      "Epoch 153/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1057 - sparse_categorical_accuracy: 0.7161 - val_loss: 0.8927 - val_sparse_categorical_accuracy: 0.2478\n",
      "Epoch 154/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1055 - sparse_categorical_accuracy: 0.7167 - val_loss: 0.8924 - val_sparse_categorical_accuracy: 0.2471\n",
      "Epoch 155/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1045 - sparse_categorical_accuracy: 0.7185 - val_loss: 0.8908 - val_sparse_categorical_accuracy: 0.2452\n",
      "Epoch 156/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1044 - sparse_categorical_accuracy: 0.7178 - val_loss: 0.8923 - val_sparse_categorical_accuracy: 0.2442\n",
      "Epoch 157/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1038 - sparse_categorical_accuracy: 0.7178 - val_loss: 0.8946 - val_sparse_categorical_accuracy: 0.2421\n",
      "Epoch 158/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1034 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.8929 - val_sparse_categorical_accuracy: 0.2449\n",
      "Epoch 159/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1030 - sparse_categorical_accuracy: 0.7217 - val_loss: 0.8940 - val_sparse_categorical_accuracy: 0.2457\n",
      "Epoch 160/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1028 - sparse_categorical_accuracy: 0.7220 - val_loss: 0.8950 - val_sparse_categorical_accuracy: 0.2461\n",
      "Epoch 161/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1021 - sparse_categorical_accuracy: 0.7236 - val_loss: 0.8941 - val_sparse_categorical_accuracy: 0.2428\n",
      "Epoch 162/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1020 - sparse_categorical_accuracy: 0.7231 - val_loss: 0.8930 - val_sparse_categorical_accuracy: 0.2490\n",
      "Epoch 163/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1017 - sparse_categorical_accuracy: 0.7233 - val_loss: 0.8952 - val_sparse_categorical_accuracy: 0.2433\n",
      "Epoch 164/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1015 - sparse_categorical_accuracy: 0.7239 - val_loss: 0.8945 - val_sparse_categorical_accuracy: 0.2478\n",
      "Epoch 165/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1011 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.8938 - val_sparse_categorical_accuracy: 0.2499\n",
      "Epoch 166/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1005 - sparse_categorical_accuracy: 0.7253 - val_loss: 0.8969 - val_sparse_categorical_accuracy: 0.2457\n",
      "Epoch 167/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0998 - sparse_categorical_accuracy: 0.7272 - val_loss: 0.8970 - val_sparse_categorical_accuracy: 0.2435\n",
      "Epoch 168/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.1001 - sparse_categorical_accuracy: 0.7256 - val_loss: 0.8968 - val_sparse_categorical_accuracy: 0.2449\n",
      "Epoch 169/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0995 - sparse_categorical_accuracy: 0.7280 - val_loss: 0.8978 - val_sparse_categorical_accuracy: 0.2468\n",
      "Epoch 170/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0992 - sparse_categorical_accuracy: 0.7288 - val_loss: 0.8969 - val_sparse_categorical_accuracy: 0.2442\n",
      "Epoch 171/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0986 - sparse_categorical_accuracy: 0.7297 - val_loss: 0.8973 - val_sparse_categorical_accuracy: 0.2457\n",
      "Epoch 172/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0980 - sparse_categorical_accuracy: 0.7312 - val_loss: 0.8992 - val_sparse_categorical_accuracy: 0.2437\n",
      "Epoch 173/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0980 - sparse_categorical_accuracy: 0.7316 - val_loss: 0.8995 - val_sparse_categorical_accuracy: 0.2437\n",
      "Epoch 174/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0978 - sparse_categorical_accuracy: 0.7328 - val_loss: 0.8979 - val_sparse_categorical_accuracy: 0.2485\n",
      "Epoch 175/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0976 - sparse_categorical_accuracy: 0.7311 - val_loss: 0.9023 - val_sparse_categorical_accuracy: 0.2437\n",
      "Epoch 176/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0967 - sparse_categorical_accuracy: 0.7347 - val_loss: 0.9000 - val_sparse_categorical_accuracy: 0.2423\n",
      "Epoch 177/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0968 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.8979 - val_sparse_categorical_accuracy: 0.2468\n",
      "Epoch 178/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0962 - sparse_categorical_accuracy: 0.7351 - val_loss: 0.9022 - val_sparse_categorical_accuracy: 0.2466\n",
      "Epoch 179/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0959 - sparse_categorical_accuracy: 0.7362 - val_loss: 0.9011 - val_sparse_categorical_accuracy: 0.2461\n",
      "Epoch 180/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0959 - sparse_categorical_accuracy: 0.7352 - val_loss: 0.9007 - val_sparse_categorical_accuracy: 0.2445\n",
      "Epoch 181/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0956 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.9013 - val_sparse_categorical_accuracy: 0.2437\n",
      "Epoch 182/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0952 - sparse_categorical_accuracy: 0.7369 - val_loss: 0.9015 - val_sparse_categorical_accuracy: 0.2428\n",
      "Epoch 183/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0947 - sparse_categorical_accuracy: 0.7375 - val_loss: 0.9021 - val_sparse_categorical_accuracy: 0.2452\n",
      "Epoch 184/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0944 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.9019 - val_sparse_categorical_accuracy: 0.2418\n",
      "Epoch 185/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0940 - sparse_categorical_accuracy: 0.7398 - val_loss: 0.9017 - val_sparse_categorical_accuracy: 0.2461\n",
      "Epoch 186/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0937 - sparse_categorical_accuracy: 0.7394 - val_loss: 0.9023 - val_sparse_categorical_accuracy: 0.2454\n",
      "Epoch 187/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0940 - sparse_categorical_accuracy: 0.7390 - val_loss: 0.9026 - val_sparse_categorical_accuracy: 0.2430\n",
      "Epoch 188/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0934 - sparse_categorical_accuracy: 0.7408 - val_loss: 0.9028 - val_sparse_categorical_accuracy: 0.2430\n",
      "Epoch 189/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0927 - sparse_categorical_accuracy: 0.7410 - val_loss: 0.9041 - val_sparse_categorical_accuracy: 0.2461\n",
      "Epoch 190/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0926 - sparse_categorical_accuracy: 0.7413 - val_loss: 0.9017 - val_sparse_categorical_accuracy: 0.2447\n",
      "Epoch 191/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0922 - sparse_categorical_accuracy: 0.7427 - val_loss: 0.9027 - val_sparse_categorical_accuracy: 0.2452\n",
      "Epoch 192/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0921 - sparse_categorical_accuracy: 0.7438 - val_loss: 0.9037 - val_sparse_categorical_accuracy: 0.2430\n",
      "Epoch 193/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0919 - sparse_categorical_accuracy: 0.7440 - val_loss: 0.9022 - val_sparse_categorical_accuracy: 0.2447\n",
      "Epoch 194/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0915 - sparse_categorical_accuracy: 0.7436 - val_loss: 0.9022 - val_sparse_categorical_accuracy: 0.2447\n",
      "Epoch 195/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0912 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.9044 - val_sparse_categorical_accuracy: 0.2457\n",
      "Epoch 196/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0912 - sparse_categorical_accuracy: 0.7456 - val_loss: 0.9050 - val_sparse_categorical_accuracy: 0.2421\n",
      "Epoch 197/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0910 - sparse_categorical_accuracy: 0.7457 - val_loss: 0.9077 - val_sparse_categorical_accuracy: 0.2433\n",
      "Epoch 198/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0906 - sparse_categorical_accuracy: 0.7457 - val_loss: 0.9054 - val_sparse_categorical_accuracy: 0.2454\n",
      "Epoch 199/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0901 - sparse_categorical_accuracy: 0.7466 - val_loss: 0.9058 - val_sparse_categorical_accuracy: 0.2445\n",
      "Epoch 200/200\n",
      "20480/20480 [==============================] - 52s 3ms/sample - loss: 0.0900 - sparse_categorical_accuracy: 0.7468 - val_loss: 0.9074 - val_sparse_categorical_accuracy: 0.2414\n",
      "Train on 20480 samples, validate on 1024 samples\n",
      "Epoch 1/200\n",
      "20480/20480 [==============================] - 3s 160us/sample - loss: 0.6458 - binary_accuracy: 0.6357 - val_loss: 0.6100 - val_binary_accuracy: 0.6689\n",
      "Epoch 2/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.6092 - binary_accuracy: 0.6707 - val_loss: 0.6034 - val_binary_accuracy: 0.6797\n",
      "Epoch 3/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.6026 - binary_accuracy: 0.6766 - val_loss: 0.6009 - val_binary_accuracy: 0.6904\n",
      "Epoch 4/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5986 - binary_accuracy: 0.6786 - val_loss: 0.6026 - val_binary_accuracy: 0.6846\n",
      "Epoch 5/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5947 - binary_accuracy: 0.6826 - val_loss: 0.6014 - val_binary_accuracy: 0.6826\n",
      "Epoch 6/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5892 - binary_accuracy: 0.6823 - val_loss: 0.6004 - val_binary_accuracy: 0.6855\n",
      "Epoch 7/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5859 - binary_accuracy: 0.6883 - val_loss: 0.5996 - val_binary_accuracy: 0.6797\n",
      "Epoch 8/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5856 - binary_accuracy: 0.6895 - val_loss: 0.5990 - val_binary_accuracy: 0.6787\n",
      "Epoch 9/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5823 - binary_accuracy: 0.6905 - val_loss: 0.5979 - val_binary_accuracy: 0.6797\n",
      "Epoch 10/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.5782 - binary_accuracy: 0.6919 - val_loss: 0.5979 - val_binary_accuracy: 0.6807\n",
      "Epoch 11/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.5745 - binary_accuracy: 0.6975 - val_loss: 0.5978 - val_binary_accuracy: 0.6807\n",
      "Epoch 12/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5741 - binary_accuracy: 0.6957 - val_loss: 0.5979 - val_binary_accuracy: 0.6787\n",
      "Epoch 13/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5708 - binary_accuracy: 0.6967 - val_loss: 0.5986 - val_binary_accuracy: 0.6807\n",
      "Epoch 14/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5680 - binary_accuracy: 0.6982 - val_loss: 0.5982 - val_binary_accuracy: 0.6807\n",
      "Epoch 15/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5660 - binary_accuracy: 0.6972 - val_loss: 0.5970 - val_binary_accuracy: 0.6816\n",
      "Epoch 16/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5636 - binary_accuracy: 0.7017 - val_loss: 0.5928 - val_binary_accuracy: 0.6846\n",
      "Epoch 17/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5599 - binary_accuracy: 0.7024 - val_loss: 0.5973 - val_binary_accuracy: 0.6709\n",
      "Epoch 18/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.5587 - binary_accuracy: 0.7041 - val_loss: 0.6003 - val_binary_accuracy: 0.6846\n",
      "Epoch 19/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5539 - binary_accuracy: 0.7079 - val_loss: 0.5967 - val_binary_accuracy: 0.6738\n",
      "Epoch 20/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5534 - binary_accuracy: 0.7086 - val_loss: 0.5992 - val_binary_accuracy: 0.6787\n",
      "Epoch 21/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5510 - binary_accuracy: 0.7125 - val_loss: 0.5902 - val_binary_accuracy: 0.6885\n",
      "Epoch 22/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5479 - binary_accuracy: 0.7130 - val_loss: 0.6017 - val_binary_accuracy: 0.6758\n",
      "Epoch 23/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.5459 - binary_accuracy: 0.7120 - val_loss: 0.6081 - val_binary_accuracy: 0.6826\n",
      "Epoch 24/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.5455 - binary_accuracy: 0.7146 - val_loss: 0.6010 - val_binary_accuracy: 0.6826\n",
      "Epoch 25/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5396 - binary_accuracy: 0.7174 - val_loss: 0.6054 - val_binary_accuracy: 0.6748\n",
      "Epoch 26/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.5355 - binary_accuracy: 0.7213 - val_loss: 0.6037 - val_binary_accuracy: 0.6836\n",
      "Epoch 27/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5344 - binary_accuracy: 0.7202 - val_loss: 0.5975 - val_binary_accuracy: 0.6836\n",
      "Epoch 28/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5322 - binary_accuracy: 0.7217 - val_loss: 0.6053 - val_binary_accuracy: 0.6709\n",
      "Epoch 29/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.5283 - binary_accuracy: 0.7264 - val_loss: 0.6047 - val_binary_accuracy: 0.6816\n",
      "Epoch 30/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5262 - binary_accuracy: 0.7282 - val_loss: 0.5981 - val_binary_accuracy: 0.6758\n",
      "Epoch 31/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.5226 - binary_accuracy: 0.7262 - val_loss: 0.6180 - val_binary_accuracy: 0.6787\n",
      "Epoch 32/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.5218 - binary_accuracy: 0.7305 - val_loss: 0.6213 - val_binary_accuracy: 0.6631\n",
      "Epoch 33/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5187 - binary_accuracy: 0.7305 - val_loss: 0.6184 - val_binary_accuracy: 0.6709\n",
      "Epoch 34/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.5157 - binary_accuracy: 0.7334 - val_loss: 0.6190 - val_binary_accuracy: 0.6758\n",
      "Epoch 35/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.5129 - binary_accuracy: 0.7323 - val_loss: 0.6107 - val_binary_accuracy: 0.6699\n",
      "Epoch 36/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5110 - binary_accuracy: 0.7355 - val_loss: 0.6159 - val_binary_accuracy: 0.6699\n",
      "Epoch 37/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.5084 - binary_accuracy: 0.7363 - val_loss: 0.6297 - val_binary_accuracy: 0.6787\n",
      "Epoch 38/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.5014 - binary_accuracy: 0.7426 - val_loss: 0.6290 - val_binary_accuracy: 0.6729\n",
      "Epoch 39/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.5030 - binary_accuracy: 0.7398 - val_loss: 0.6198 - val_binary_accuracy: 0.6758\n",
      "Epoch 40/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.5004 - binary_accuracy: 0.7446 - val_loss: 0.6265 - val_binary_accuracy: 0.6748\n",
      "Epoch 41/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.4937 - binary_accuracy: 0.7485 - val_loss: 0.6241 - val_binary_accuracy: 0.6709\n",
      "Epoch 42/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4930 - binary_accuracy: 0.7472 - val_loss: 0.6427 - val_binary_accuracy: 0.6514\n",
      "Epoch 43/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4916 - binary_accuracy: 0.7477 - val_loss: 0.6260 - val_binary_accuracy: 0.6533\n",
      "Epoch 44/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.4910 - binary_accuracy: 0.7483 - val_loss: 0.6478 - val_binary_accuracy: 0.6709\n",
      "Epoch 45/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4873 - binary_accuracy: 0.7502 - val_loss: 0.6273 - val_binary_accuracy: 0.6592\n",
      "Epoch 46/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4855 - binary_accuracy: 0.7512 - val_loss: 0.6444 - val_binary_accuracy: 0.6572\n",
      "Epoch 47/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.4823 - binary_accuracy: 0.7541 - val_loss: 0.6473 - val_binary_accuracy: 0.6533\n",
      "Epoch 48/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.4785 - binary_accuracy: 0.7539 - val_loss: 0.6532 - val_binary_accuracy: 0.6621\n",
      "Epoch 49/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4768 - binary_accuracy: 0.7558 - val_loss: 0.6557 - val_binary_accuracy: 0.6592\n",
      "Epoch 50/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.4763 - binary_accuracy: 0.7555 - val_loss: 0.6416 - val_binary_accuracy: 0.6680\n",
      "Epoch 51/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.4760 - binary_accuracy: 0.7577 - val_loss: 0.6665 - val_binary_accuracy: 0.6621\n",
      "Epoch 52/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4707 - binary_accuracy: 0.7613 - val_loss: 0.6655 - val_binary_accuracy: 0.6631\n",
      "Epoch 53/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.4669 - binary_accuracy: 0.7641 - val_loss: 0.6532 - val_binary_accuracy: 0.6475\n",
      "Epoch 54/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.4636 - binary_accuracy: 0.7636 - val_loss: 0.6707 - val_binary_accuracy: 0.6562\n",
      "Epoch 55/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.4639 - binary_accuracy: 0.7625 - val_loss: 0.6807 - val_binary_accuracy: 0.6592\n",
      "Epoch 56/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.4588 - binary_accuracy: 0.7658 - val_loss: 0.6818 - val_binary_accuracy: 0.6484\n",
      "Epoch 57/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4577 - binary_accuracy: 0.7701 - val_loss: 0.6748 - val_binary_accuracy: 0.6475\n",
      "Epoch 58/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4553 - binary_accuracy: 0.7720 - val_loss: 0.6913 - val_binary_accuracy: 0.6582\n",
      "Epoch 59/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4523 - binary_accuracy: 0.7713 - val_loss: 0.6914 - val_binary_accuracy: 0.6484\n",
      "Epoch 60/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4535 - binary_accuracy: 0.7713 - val_loss: 0.6689 - val_binary_accuracy: 0.6631\n",
      "Epoch 61/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4476 - binary_accuracy: 0.7735 - val_loss: 0.6973 - val_binary_accuracy: 0.6475\n",
      "Epoch 62/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4468 - binary_accuracy: 0.7760 - val_loss: 0.7109 - val_binary_accuracy: 0.6738\n",
      "Epoch 63/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.4437 - binary_accuracy: 0.7759 - val_loss: 0.7009 - val_binary_accuracy: 0.6504\n",
      "Epoch 64/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.4419 - binary_accuracy: 0.7766 - val_loss: 0.7279 - val_binary_accuracy: 0.6631\n",
      "Epoch 65/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.4411 - binary_accuracy: 0.7760 - val_loss: 0.7033 - val_binary_accuracy: 0.6533\n",
      "Epoch 66/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4333 - binary_accuracy: 0.7810 - val_loss: 0.7166 - val_binary_accuracy: 0.6523\n",
      "Epoch 67/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4378 - binary_accuracy: 0.7800 - val_loss: 0.7099 - val_binary_accuracy: 0.6445\n",
      "Epoch 68/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.4328 - binary_accuracy: 0.7837 - val_loss: 0.7459 - val_binary_accuracy: 0.6670\n",
      "Epoch 69/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4305 - binary_accuracy: 0.7872 - val_loss: 0.7231 - val_binary_accuracy: 0.6396\n",
      "Epoch 70/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4287 - binary_accuracy: 0.7865 - val_loss: 0.7314 - val_binary_accuracy: 0.6680\n",
      "Epoch 71/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.4291 - binary_accuracy: 0.7850 - val_loss: 0.7279 - val_binary_accuracy: 0.6465\n",
      "Epoch 72/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.4229 - binary_accuracy: 0.7857 - val_loss: 0.7429 - val_binary_accuracy: 0.6416\n",
      "Epoch 73/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4236 - binary_accuracy: 0.7854 - val_loss: 0.7397 - val_binary_accuracy: 0.6533\n",
      "Epoch 74/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.4230 - binary_accuracy: 0.7888 - val_loss: 0.7510 - val_binary_accuracy: 0.6553\n",
      "Epoch 75/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4217 - binary_accuracy: 0.7885 - val_loss: 0.7460 - val_binary_accuracy: 0.6484\n",
      "Epoch 76/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4179 - binary_accuracy: 0.7894 - val_loss: 0.7421 - val_binary_accuracy: 0.6543\n",
      "Epoch 77/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4169 - binary_accuracy: 0.7906 - val_loss: 0.7842 - val_binary_accuracy: 0.6729\n",
      "Epoch 78/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.4150 - binary_accuracy: 0.7919 - val_loss: 0.7468 - val_binary_accuracy: 0.6592\n",
      "Epoch 79/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4146 - binary_accuracy: 0.7942 - val_loss: 0.7682 - val_binary_accuracy: 0.6611\n",
      "Epoch 80/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4103 - binary_accuracy: 0.7940 - val_loss: 0.7839 - val_binary_accuracy: 0.6611\n",
      "Epoch 81/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4081 - binary_accuracy: 0.8001 - val_loss: 0.7660 - val_binary_accuracy: 0.6533\n",
      "Epoch 82/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.4073 - binary_accuracy: 0.7954 - val_loss: 0.8019 - val_binary_accuracy: 0.6650\n",
      "Epoch 83/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.4066 - binary_accuracy: 0.7973 - val_loss: 0.7731 - val_binary_accuracy: 0.6445\n",
      "Epoch 84/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4048 - binary_accuracy: 0.7977 - val_loss: 0.7956 - val_binary_accuracy: 0.6631\n",
      "Epoch 85/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.4031 - binary_accuracy: 0.7991 - val_loss: 0.8022 - val_binary_accuracy: 0.6436\n",
      "Epoch 86/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3994 - binary_accuracy: 0.8020 - val_loss: 0.8312 - val_binary_accuracy: 0.6465\n",
      "Epoch 87/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3976 - binary_accuracy: 0.8026 - val_loss: 0.8107 - val_binary_accuracy: 0.6455\n",
      "Epoch 88/200\n",
      "20480/20480 [==============================] - 1s 32us/sample - loss: 0.3963 - binary_accuracy: 0.8006 - val_loss: 0.8101 - val_binary_accuracy: 0.6406\n",
      "Epoch 89/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3974 - binary_accuracy: 0.8012 - val_loss: 0.8390 - val_binary_accuracy: 0.6475\n",
      "Epoch 90/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3924 - binary_accuracy: 0.8030 - val_loss: 0.8547 - val_binary_accuracy: 0.6514\n",
      "Epoch 91/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3905 - binary_accuracy: 0.8018 - val_loss: 0.8295 - val_binary_accuracy: 0.6465\n",
      "Epoch 92/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3922 - binary_accuracy: 0.8045 - val_loss: 0.8878 - val_binary_accuracy: 0.6641\n",
      "Epoch 93/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3888 - binary_accuracy: 0.8046 - val_loss: 0.8167 - val_binary_accuracy: 0.6621\n",
      "Epoch 94/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3885 - binary_accuracy: 0.8062 - val_loss: 0.8416 - val_binary_accuracy: 0.6475\n",
      "Epoch 95/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3868 - binary_accuracy: 0.8071 - val_loss: 0.8372 - val_binary_accuracy: 0.6445\n",
      "Epoch 96/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3867 - binary_accuracy: 0.8067 - val_loss: 0.8742 - val_binary_accuracy: 0.6602\n",
      "Epoch 97/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3819 - binary_accuracy: 0.8108 - val_loss: 0.8475 - val_binary_accuracy: 0.6562\n",
      "Epoch 98/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3795 - binary_accuracy: 0.8126 - val_loss: 0.9018 - val_binary_accuracy: 0.6523\n",
      "Epoch 99/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3809 - binary_accuracy: 0.8102 - val_loss: 0.9063 - val_binary_accuracy: 0.6631\n",
      "Epoch 100/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3830 - binary_accuracy: 0.8092 - val_loss: 0.8743 - val_binary_accuracy: 0.6387\n",
      "Epoch 101/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3790 - binary_accuracy: 0.8101 - val_loss: 0.8677 - val_binary_accuracy: 0.6553\n",
      "Epoch 102/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3752 - binary_accuracy: 0.8131 - val_loss: 0.8644 - val_binary_accuracy: 0.6475\n",
      "Epoch 103/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3759 - binary_accuracy: 0.8146 - val_loss: 0.9283 - val_binary_accuracy: 0.6543\n",
      "Epoch 104/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3803 - binary_accuracy: 0.8106 - val_loss: 0.9106 - val_binary_accuracy: 0.6494\n",
      "Epoch 105/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3746 - binary_accuracy: 0.8175 - val_loss: 0.9602 - val_binary_accuracy: 0.6582\n",
      "Epoch 106/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3746 - binary_accuracy: 0.8156 - val_loss: 0.9400 - val_binary_accuracy: 0.6582\n",
      "Epoch 107/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3697 - binary_accuracy: 0.8147 - val_loss: 0.9206 - val_binary_accuracy: 0.6387\n",
      "Epoch 108/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3731 - binary_accuracy: 0.8166 - val_loss: 0.9288 - val_binary_accuracy: 0.6611\n",
      "Epoch 109/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3704 - binary_accuracy: 0.8167 - val_loss: 0.9125 - val_binary_accuracy: 0.6377\n",
      "Epoch 110/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3673 - binary_accuracy: 0.8137 - val_loss: 0.9008 - val_binary_accuracy: 0.6299\n",
      "Epoch 111/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3678 - binary_accuracy: 0.8166 - val_loss: 0.9243 - val_binary_accuracy: 0.6504\n",
      "Epoch 112/200\n",
      "20480/20480 [==============================] - 1s 43us/sample - loss: 0.3628 - binary_accuracy: 0.8195 - val_loss: 0.9560 - val_binary_accuracy: 0.6533\n",
      "Epoch 113/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3608 - binary_accuracy: 0.8220 - val_loss: 0.9636 - val_binary_accuracy: 0.6357\n",
      "Epoch 114/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3600 - binary_accuracy: 0.8227 - val_loss: 0.9559 - val_binary_accuracy: 0.6572\n",
      "Epoch 115/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3592 - binary_accuracy: 0.8220 - val_loss: 0.9569 - val_binary_accuracy: 0.6543\n",
      "Epoch 116/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3605 - binary_accuracy: 0.8211 - val_loss: 0.9710 - val_binary_accuracy: 0.6426\n",
      "Epoch 117/200\n",
      "20480/20480 [==============================] - 1s 32us/sample - loss: 0.3580 - binary_accuracy: 0.8222 - val_loss: 0.9847 - val_binary_accuracy: 0.6455\n",
      "Epoch 118/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3587 - binary_accuracy: 0.8210 - val_loss: 0.9853 - val_binary_accuracy: 0.6465\n",
      "Epoch 119/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3576 - binary_accuracy: 0.8218 - val_loss: 0.9754 - val_binary_accuracy: 0.6387\n",
      "Epoch 120/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3544 - binary_accuracy: 0.8213 - val_loss: 1.0114 - val_binary_accuracy: 0.6367\n",
      "Epoch 121/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3568 - binary_accuracy: 0.8252 - val_loss: 1.0019 - val_binary_accuracy: 0.6377\n",
      "Epoch 122/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3519 - binary_accuracy: 0.8270 - val_loss: 0.9896 - val_binary_accuracy: 0.6465\n",
      "Epoch 123/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3498 - binary_accuracy: 0.8247 - val_loss: 1.0229 - val_binary_accuracy: 0.6543\n",
      "Epoch 124/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3495 - binary_accuracy: 0.8262 - val_loss: 1.0421 - val_binary_accuracy: 0.6514\n",
      "Epoch 125/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3499 - binary_accuracy: 0.8248 - val_loss: 1.0326 - val_binary_accuracy: 0.6416\n",
      "Epoch 126/200\n",
      "20480/20480 [==============================] - 1s 40us/sample - loss: 0.3522 - binary_accuracy: 0.8251 - val_loss: 1.0333 - val_binary_accuracy: 0.6445\n",
      "Epoch 127/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3474 - binary_accuracy: 0.8272 - val_loss: 1.0234 - val_binary_accuracy: 0.6377\n",
      "Epoch 128/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3470 - binary_accuracy: 0.8279 - val_loss: 1.0985 - val_binary_accuracy: 0.6504\n",
      "Epoch 129/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3449 - binary_accuracy: 0.8302 - val_loss: 1.0829 - val_binary_accuracy: 0.6377\n",
      "Epoch 130/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3467 - binary_accuracy: 0.8286 - val_loss: 1.0507 - val_binary_accuracy: 0.6289\n",
      "Epoch 131/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3491 - binary_accuracy: 0.8265 - val_loss: 1.0692 - val_binary_accuracy: 0.6631\n",
      "Epoch 132/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3431 - binary_accuracy: 0.8300 - val_loss: 1.0595 - val_binary_accuracy: 0.6416\n",
      "Epoch 133/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3441 - binary_accuracy: 0.8323 - val_loss: 1.0517 - val_binary_accuracy: 0.6328\n",
      "Epoch 134/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3458 - binary_accuracy: 0.8269 - val_loss: 1.0334 - val_binary_accuracy: 0.6348\n",
      "Epoch 135/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3429 - binary_accuracy: 0.8298 - val_loss: 1.0881 - val_binary_accuracy: 0.6357\n",
      "Epoch 136/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3370 - binary_accuracy: 0.8297 - val_loss: 1.1227 - val_binary_accuracy: 0.6328\n",
      "Epoch 137/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3390 - binary_accuracy: 0.8316 - val_loss: 1.1179 - val_binary_accuracy: 0.6357\n",
      "Epoch 138/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3326 - binary_accuracy: 0.8329 - val_loss: 1.1428 - val_binary_accuracy: 0.6367\n",
      "Epoch 139/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3394 - binary_accuracy: 0.8294 - val_loss: 1.1470 - val_binary_accuracy: 0.6475\n",
      "Epoch 140/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3332 - binary_accuracy: 0.8354 - val_loss: 1.0993 - val_binary_accuracy: 0.6475\n",
      "Epoch 141/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3398 - binary_accuracy: 0.8324 - val_loss: 1.0864 - val_binary_accuracy: 0.6406\n",
      "Epoch 142/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3343 - binary_accuracy: 0.8337 - val_loss: 1.0895 - val_binary_accuracy: 0.6377\n",
      "Epoch 143/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3314 - binary_accuracy: 0.8366 - val_loss: 1.1833 - val_binary_accuracy: 0.6523\n",
      "Epoch 144/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3293 - binary_accuracy: 0.8366 - val_loss: 1.1310 - val_binary_accuracy: 0.6436\n",
      "Epoch 145/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3392 - binary_accuracy: 0.8355 - val_loss: 1.0596 - val_binary_accuracy: 0.6191\n",
      "Epoch 146/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.3306 - binary_accuracy: 0.8352 - val_loss: 1.1522 - val_binary_accuracy: 0.6338\n",
      "Epoch 147/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3267 - binary_accuracy: 0.8371 - val_loss: 1.1286 - val_binary_accuracy: 0.6445\n",
      "Epoch 148/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3290 - binary_accuracy: 0.8361 - val_loss: 1.2057 - val_binary_accuracy: 0.6377\n",
      "Epoch 149/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3269 - binary_accuracy: 0.8351 - val_loss: 1.1757 - val_binary_accuracy: 0.6523\n",
      "Epoch 150/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3252 - binary_accuracy: 0.8363 - val_loss: 1.1299 - val_binary_accuracy: 0.6299\n",
      "Epoch 151/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3236 - binary_accuracy: 0.8384 - val_loss: 1.1768 - val_binary_accuracy: 0.6377\n",
      "Epoch 152/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3218 - binary_accuracy: 0.8398 - val_loss: 1.2443 - val_binary_accuracy: 0.6387\n",
      "Epoch 153/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3204 - binary_accuracy: 0.8409 - val_loss: 1.2249 - val_binary_accuracy: 0.6357\n",
      "Epoch 154/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3306 - binary_accuracy: 0.8344 - val_loss: 1.1829 - val_binary_accuracy: 0.6309\n",
      "Epoch 155/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3268 - binary_accuracy: 0.8385 - val_loss: 1.2246 - val_binary_accuracy: 0.6406\n",
      "Epoch 156/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3189 - binary_accuracy: 0.8404 - val_loss: 1.2533 - val_binary_accuracy: 0.6357\n",
      "Epoch 157/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3233 - binary_accuracy: 0.8381 - val_loss: 1.2485 - val_binary_accuracy: 0.6338\n",
      "Epoch 158/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3236 - binary_accuracy: 0.8398 - val_loss: 1.2191 - val_binary_accuracy: 0.6309\n",
      "Epoch 159/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3172 - binary_accuracy: 0.8429 - val_loss: 1.1970 - val_binary_accuracy: 0.6455\n",
      "Epoch 160/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3198 - binary_accuracy: 0.8421 - val_loss: 1.2639 - val_binary_accuracy: 0.6328\n",
      "Epoch 161/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3229 - binary_accuracy: 0.8377 - val_loss: 1.2605 - val_binary_accuracy: 0.6357\n",
      "Epoch 162/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3229 - binary_accuracy: 0.8393 - val_loss: 1.2820 - val_binary_accuracy: 0.6348\n",
      "Epoch 163/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3133 - binary_accuracy: 0.8459 - val_loss: 1.2721 - val_binary_accuracy: 0.6357\n",
      "Epoch 164/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3133 - binary_accuracy: 0.8454 - val_loss: 1.3109 - val_binary_accuracy: 0.6406\n",
      "Epoch 165/200\n",
      "20480/20480 [==============================] - 1s 33us/sample - loss: 0.3118 - binary_accuracy: 0.8453 - val_loss: 1.2971 - val_binary_accuracy: 0.6338\n",
      "Epoch 166/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3165 - binary_accuracy: 0.8420 - val_loss: 1.3751 - val_binary_accuracy: 0.6377\n",
      "Epoch 167/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.3142 - binary_accuracy: 0.8446 - val_loss: 1.2635 - val_binary_accuracy: 0.6270\n",
      "Epoch 168/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3148 - binary_accuracy: 0.8435 - val_loss: 1.3147 - val_binary_accuracy: 0.6406\n",
      "Epoch 169/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3152 - binary_accuracy: 0.8427 - val_loss: 1.2991 - val_binary_accuracy: 0.6387\n",
      "Epoch 170/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3125 - binary_accuracy: 0.8454 - val_loss: 1.3087 - val_binary_accuracy: 0.6260\n",
      "Epoch 171/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3167 - binary_accuracy: 0.8418 - val_loss: 1.2949 - val_binary_accuracy: 0.6289\n",
      "Epoch 172/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3103 - binary_accuracy: 0.8445 - val_loss: 1.3644 - val_binary_accuracy: 0.6318\n",
      "Epoch 173/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3118 - binary_accuracy: 0.8451 - val_loss: 1.3398 - val_binary_accuracy: 0.6377\n",
      "Epoch 174/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.3100 - binary_accuracy: 0.8456 - val_loss: 1.3882 - val_binary_accuracy: 0.6523\n",
      "Epoch 175/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3081 - binary_accuracy: 0.8446 - val_loss: 1.3067 - val_binary_accuracy: 0.6328\n",
      "Epoch 176/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3075 - binary_accuracy: 0.8474 - val_loss: 1.3167 - val_binary_accuracy: 0.6201\n",
      "Epoch 177/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3094 - binary_accuracy: 0.8473 - val_loss: 1.3313 - val_binary_accuracy: 0.6338\n",
      "Epoch 178/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3005 - binary_accuracy: 0.8491 - val_loss: 1.3870 - val_binary_accuracy: 0.6387\n",
      "Epoch 179/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3055 - binary_accuracy: 0.8488 - val_loss: 1.3591 - val_binary_accuracy: 0.6270\n",
      "Epoch 180/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.3011 - binary_accuracy: 0.8501 - val_loss: 1.4224 - val_binary_accuracy: 0.6309\n",
      "Epoch 181/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3010 - binary_accuracy: 0.8488 - val_loss: 1.4155 - val_binary_accuracy: 0.6318\n",
      "Epoch 182/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.3015 - binary_accuracy: 0.8496 - val_loss: 1.4374 - val_binary_accuracy: 0.6377\n",
      "Epoch 183/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3037 - binary_accuracy: 0.8495 - val_loss: 1.3142 - val_binary_accuracy: 0.6260\n",
      "Epoch 184/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3060 - binary_accuracy: 0.8465 - val_loss: 1.3737 - val_binary_accuracy: 0.6240\n",
      "Epoch 185/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.3064 - binary_accuracy: 0.8466 - val_loss: 1.4718 - val_binary_accuracy: 0.6270\n",
      "Epoch 186/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.3069 - binary_accuracy: 0.8466 - val_loss: 1.3939 - val_binary_accuracy: 0.6377\n",
      "Epoch 187/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.2993 - binary_accuracy: 0.8490 - val_loss: 1.3995 - val_binary_accuracy: 0.6338\n",
      "Epoch 188/200\n",
      "20480/20480 [==============================] - 1s 39us/sample - loss: 0.2993 - binary_accuracy: 0.8509 - val_loss: 1.4653 - val_binary_accuracy: 0.6406\n",
      "Epoch 189/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.2965 - binary_accuracy: 0.8529 - val_loss: 1.4757 - val_binary_accuracy: 0.6367\n",
      "Epoch 190/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.2965 - binary_accuracy: 0.8511 - val_loss: 1.4559 - val_binary_accuracy: 0.6318\n",
      "Epoch 191/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.2980 - binary_accuracy: 0.8515 - val_loss: 1.5429 - val_binary_accuracy: 0.6377\n",
      "Epoch 192/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.2921 - binary_accuracy: 0.8558 - val_loss: 1.4942 - val_binary_accuracy: 0.6260\n",
      "Epoch 193/200\n",
      "20480/20480 [==============================] - 1s 37us/sample - loss: 0.2934 - binary_accuracy: 0.8546 - val_loss: 1.4972 - val_binary_accuracy: 0.6318\n",
      "Epoch 194/200\n",
      "20480/20480 [==============================] - 1s 41us/sample - loss: 0.2938 - binary_accuracy: 0.8531 - val_loss: 1.5013 - val_binary_accuracy: 0.6221\n",
      "Epoch 195/200\n",
      "20480/20480 [==============================] - 1s 34us/sample - loss: 0.2937 - binary_accuracy: 0.8545 - val_loss: 1.4772 - val_binary_accuracy: 0.6279\n",
      "Epoch 196/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.2912 - binary_accuracy: 0.8566 - val_loss: 1.5085 - val_binary_accuracy: 0.6318\n",
      "Epoch 197/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.2955 - binary_accuracy: 0.8528 - val_loss: 1.5316 - val_binary_accuracy: 0.6475\n",
      "Epoch 198/200\n",
      "20480/20480 [==============================] - 1s 35us/sample - loss: 0.2919 - binary_accuracy: 0.8511 - val_loss: 1.5538 - val_binary_accuracy: 0.6309\n",
      "Epoch 199/200\n",
      "20480/20480 [==============================] - 1s 36us/sample - loss: 0.2903 - binary_accuracy: 0.8565 - val_loss: 1.5172 - val_binary_accuracy: 0.6299\n",
      "Epoch 200/200\n",
      "20480/20480 [==============================] - 1s 38us/sample - loss: 0.2975 - binary_accuracy: 0.8536 - val_loss: 1.5085 - val_binary_accuracy: 0.6230\n"
     ]
    }
   ],
   "source": [
    "#When TPU ENABLED\n",
    "resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' + os.environ['COLAB_TPU_ADDR'])\n",
    "tf.config.experimental_connect_to_cluster(resolver)\n",
    "tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "\n",
    "with strategy.scope():\n",
    "    tpu_answer_model,tpu_encoder_model,tpu_decoder_model,tpu_feasibility_model = create_models(embedding_matrix,\n",
    "                                                                                      num_unit_gru = 80,\n",
    "                                                                                      num_layers_gru = 2,\n",
    "                                                                                      ndim =100,\n",
    "                                                                                      num_episodes = 2,\n",
    "                                                                                      num_dense_layer_feasibility_units = 32,\n",
    "                                                                                      dropout_rate = 0.5,\n",
    "                                                                                      num_dense_layers_feasibility = 2,\n",
    "                                                                                      attentionType = 0, # 0 means Luong's 1 means BahdanauUnits\n",
    "                                                                                      BahdanauUnits = 64)\n",
    "\n",
    "    tpu_answer_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "                           )\n",
    "  \n",
    "    tpu_feasibility_model.compile(optimizer='adam',\n",
    "                           loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                           metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                           )\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "tpu_history_answer_model = tpu_answer_model.fit({'Context_Input':train_context_padded_seq[:20480],\n",
    "                                                 'Question_Input':train_question_seq_padded[:20480],\n",
    "                                                 'Answer_Input':train_answer_input_seq_padded[:20480] },\n",
    "                                                {'Answer_output':train_answer_target_seq_padded[:20480] },\n",
    "                                                epochs=200,batch_size=32*8,\n",
    "                                                validation_data=([val_context_padded_seq[:1024],val_question_seq_padded[:1024],train_answer_input_seq_padded[:1024]],\n",
    "                                                                 val_answer_target_seq_padded[:1024])\n",
    "                                                )\n",
    "\n",
    "\n",
    "encoder_prediction = tpu_encoder_model.predict([train_context_padded_seq[:20480],train_question_seq_padded[:20480]])\n",
    "encoder_validation_prediction = tpu_encoder_model.predict([val_context_padded_seq[:1024],val_question_seq_padded[:1024]])\n",
    "tpu_history_feasibility_model = tpu_feasibility_model.fit(encoder_prediction,train_answer_impossible[:20480],\n",
    "                                                          epochs=200,batch_size=32*8,\n",
    "                                                          validation_data = (encoder_validation_prediction,val_answer_impossible[:1024])\n",
    "                                                          )\n",
    "\n",
    "\n",
    "tpu_answer_model.save('tpu_answer_model_2.h5')\n",
    "tpu_encoder_model.save('tpu_encoder_model_2.h5')\n",
    "tpu_decoder_model.save('tpu_decoder_model_2.h5')\n",
    "tpu_feasibility_model.save('tpu_feasibility_model_2.h5')\n",
    "with open('tpu_history_answer_model2', 'wb') as file_history:\n",
    "        pickle.dump(tpu_history_answer_model.history, file_history)\n",
    "with open('tpu_history_feasibility_model2', 'wb') as file_history:\n",
    "        pickle.dump(tpu_history_feasibility_model.history, file_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 361160,
     "status": "error",
     "timestamp": 1584087607050,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "SUILvDAOvLaR",
    "outputId": "98e526b4-9d52-44e6-9aaa-d4e357d6f728"
   },
   "outputs": [],
   "source": [
    "#When GPU ENABLED\n",
    "gpu_answer_model,gpu_encoder_model,gpu_decoder_model,gpu_feasibility_model = create_models(embedding_matrix,\n",
    "                                                                                      num_unit_gru = 16,\n",
    "                                                                                      num_layers_gru = 2,\n",
    "                                                                                      ndim =100,\n",
    "                                                                                      num_episodes = 2,\n",
    "                                                                                      num_dense_layer_feasibility_units = 16,\n",
    "                                                                                      dropout_rate = 0.5,\n",
    "                                                                                      num_dense_layers_feasibility = 2,\n",
    "                                                                                      attentionType = 0, # 0 means Luong's 1 means BahdanauUnits\n",
    "                                                                                      BahdanauUnits = 64)\n",
    "\n",
    "adam_optim = keras.optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "gpu_answer_model.compile(optimizer=adam_optim,\n",
    "                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "                           )\n",
    "\n",
    "gpu_answer_model.summary()\n",
    "gpu_feasibility_model.compile(optimizer=adam_optim,\n",
    "                           loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                           metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                           )\n",
    "gpu_feasibility_model.summary()\n",
    "gpu_history_answer_model = gpu_answer_model.fit({'Context_Input':train_context_padded_seq[:100],\n",
    "                                                 'Question_Input':train_question_seq_padded[:100],\n",
    "                                                 'Answer_Input':train_answer_input_seq_padded[:100] },\n",
    "                                                {'Answer_output':train_answer_target_seq_padded[:100] },\n",
    "                                                epochs=200,batch_size=10,\n",
    "                                                validation_data=([val_context_padded_seq[:20],val_question_seq_padded[:20],train_answer_input_seq_padded[:20]],\n",
    "                                                                 val_answer_target_seq_padded[:20]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 486
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 361160,
     "status": "error",
     "timestamp": 1584087607050,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "SUILvDAOvLaR",
    "outputId": "98e526b4-9d52-44e6-9aaa-d4e357d6f728"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 100 samples, validate on 20 samples\n",
      "Epoch 1/200\n",
      "100/100 [==============================] - 1s 9ms/sample - loss: 0.9869 - binary_accuracy: 0.4700 - val_loss: 3.8154 - val_binary_accuracy: 0.5500\n",
      "Epoch 2/200\n",
      "100/100 [==============================] - 0s 314us/sample - loss: 0.8409 - binary_accuracy: 0.5500 - val_loss: 6.9284 - val_binary_accuracy: 0.5500\n",
      "Epoch 3/200\n",
      "100/100 [==============================] - 0s 397us/sample - loss: 0.6492 - binary_accuracy: 0.6600 - val_loss: 2.5105 - val_binary_accuracy: 0.5500\n",
      "Epoch 4/200\n",
      "100/100 [==============================] - 0s 399us/sample - loss: 0.6546 - binary_accuracy: 0.6600 - val_loss: 1.2679 - val_binary_accuracy: 0.5500\n",
      "Epoch 5/200\n",
      "100/100 [==============================] - 0s 476us/sample - loss: 0.6130 - binary_accuracy: 0.6700 - val_loss: 1.4152 - val_binary_accuracy: 0.5500\n",
      "Epoch 6/200\n",
      "100/100 [==============================] - 0s 416us/sample - loss: 0.5686 - binary_accuracy: 0.7400 - val_loss: 1.3996 - val_binary_accuracy: 0.5500\n",
      "Epoch 7/200\n",
      "100/100 [==============================] - 0s 440us/sample - loss: 0.6191 - binary_accuracy: 0.6800 - val_loss: 1.4374 - val_binary_accuracy: 0.5500\n",
      "Epoch 8/200\n",
      "100/100 [==============================] - 0s 358us/sample - loss: 0.6176 - binary_accuracy: 0.6900 - val_loss: 1.1908 - val_binary_accuracy: 0.5500\n",
      "Epoch 9/200\n",
      "100/100 [==============================] - 0s 323us/sample - loss: 0.5107 - binary_accuracy: 0.7600 - val_loss: 1.3664 - val_binary_accuracy: 0.5500\n",
      "Epoch 10/200\n",
      "100/100 [==============================] - 0s 363us/sample - loss: 0.6011 - binary_accuracy: 0.6700 - val_loss: 1.5888 - val_binary_accuracy: 0.5500\n",
      "Epoch 11/200\n",
      "100/100 [==============================] - 0s 365us/sample - loss: 0.4677 - binary_accuracy: 0.8100 - val_loss: 1.7186 - val_binary_accuracy: 0.5500\n",
      "Epoch 12/200\n",
      "100/100 [==============================] - 0s 333us/sample - loss: 0.5500 - binary_accuracy: 0.7600 - val_loss: 1.7231 - val_binary_accuracy: 0.5500\n",
      "Epoch 13/200\n",
      "100/100 [==============================] - 0s 377us/sample - loss: 0.5125 - binary_accuracy: 0.7300 - val_loss: 1.8798 - val_binary_accuracy: 0.5500\n",
      "Epoch 14/200\n",
      "100/100 [==============================] - 0s 366us/sample - loss: 0.5038 - binary_accuracy: 0.7600 - val_loss: 1.9890 - val_binary_accuracy: 0.5500\n",
      "Epoch 15/200\n",
      "100/100 [==============================] - 0s 387us/sample - loss: 0.4371 - binary_accuracy: 0.8300 - val_loss: 1.8864 - val_binary_accuracy: 0.5500\n",
      "Epoch 16/200\n",
      "100/100 [==============================] - 0s 381us/sample - loss: 0.5927 - binary_accuracy: 0.7300 - val_loss: 2.2248 - val_binary_accuracy: 0.5500\n",
      "Epoch 17/200\n",
      "100/100 [==============================] - 0s 424us/sample - loss: 0.4818 - binary_accuracy: 0.7200 - val_loss: 2.1663 - val_binary_accuracy: 0.5500\n",
      "Epoch 18/200\n",
      "100/100 [==============================] - 0s 395us/sample - loss: 0.4440 - binary_accuracy: 0.7900 - val_loss: 2.2113 - val_binary_accuracy: 0.6000\n",
      "Epoch 19/200\n",
      "100/100 [==============================] - 0s 387us/sample - loss: 0.4981 - binary_accuracy: 0.7500 - val_loss: 1.9953 - val_binary_accuracy: 0.6000\n",
      "Epoch 20/200\n",
      "100/100 [==============================] - 0s 393us/sample - loss: 0.4390 - binary_accuracy: 0.8100 - val_loss: 1.8126 - val_binary_accuracy: 0.6000\n",
      "Epoch 21/200\n",
      "100/100 [==============================] - 0s 396us/sample - loss: 0.4407 - binary_accuracy: 0.8500 - val_loss: 1.6025 - val_binary_accuracy: 0.6000\n",
      "Epoch 22/200\n",
      "100/100 [==============================] - 0s 358us/sample - loss: 0.3846 - binary_accuracy: 0.8100 - val_loss: 1.6168 - val_binary_accuracy: 0.6000\n",
      "Epoch 23/200\n",
      "100/100 [==============================] - 0s 367us/sample - loss: 0.3956 - binary_accuracy: 0.8200 - val_loss: 1.7125 - val_binary_accuracy: 0.6000\n",
      "Epoch 24/200\n",
      "100/100 [==============================] - 0s 366us/sample - loss: 0.4654 - binary_accuracy: 0.8000 - val_loss: 1.6574 - val_binary_accuracy: 0.6000\n",
      "Epoch 25/200\n",
      "100/100 [==============================] - 0s 366us/sample - loss: 0.5399 - binary_accuracy: 0.7400 - val_loss: 1.2701 - val_binary_accuracy: 0.6000\n",
      "Epoch 26/200\n",
      "100/100 [==============================] - 0s 345us/sample - loss: 0.4052 - binary_accuracy: 0.8200 - val_loss: 1.2432 - val_binary_accuracy: 0.6500\n",
      "Epoch 27/200\n",
      "100/100 [==============================] - 0s 457us/sample - loss: 0.4394 - binary_accuracy: 0.7900 - val_loss: 1.1851 - val_binary_accuracy: 0.6500\n",
      "Epoch 28/200\n",
      "100/100 [==============================] - 0s 401us/sample - loss: 0.4136 - binary_accuracy: 0.8000 - val_loss: 1.1337 - val_binary_accuracy: 0.6000\n",
      "Epoch 29/200\n",
      "100/100 [==============================] - 0s 389us/sample - loss: 0.3533 - binary_accuracy: 0.8700 - val_loss: 1.1962 - val_binary_accuracy: 0.6500\n",
      "Epoch 30/200\n",
      "100/100 [==============================] - 0s 388us/sample - loss: 0.3302 - binary_accuracy: 0.8800 - val_loss: 1.2693 - val_binary_accuracy: 0.6500\n",
      "Epoch 31/200\n",
      "100/100 [==============================] - 0s 343us/sample - loss: 0.4278 - binary_accuracy: 0.8200 - val_loss: 1.2498 - val_binary_accuracy: 0.6500\n",
      "Epoch 32/200\n",
      "100/100 [==============================] - 0s 422us/sample - loss: 0.3672 - binary_accuracy: 0.8400 - val_loss: 1.2182 - val_binary_accuracy: 0.5500\n",
      "Epoch 33/200\n",
      "100/100 [==============================] - 0s 403us/sample - loss: 0.3905 - binary_accuracy: 0.8100 - val_loss: 1.2284 - val_binary_accuracy: 0.5500\n",
      "Epoch 34/200\n",
      "100/100 [==============================] - 0s 407us/sample - loss: 0.3624 - binary_accuracy: 0.8200 - val_loss: 1.3690 - val_binary_accuracy: 0.5500\n",
      "Epoch 35/200\n",
      "100/100 [==============================] - 0s 389us/sample - loss: 0.4092 - binary_accuracy: 0.8200 - val_loss: 1.3578 - val_binary_accuracy: 0.5500\n",
      "Epoch 36/200\n",
      "100/100 [==============================] - 0s 406us/sample - loss: 0.3551 - binary_accuracy: 0.8900 - val_loss: 1.3923 - val_binary_accuracy: 0.5000\n",
      "Epoch 37/200\n",
      "100/100 [==============================] - 0s 412us/sample - loss: 0.3558 - binary_accuracy: 0.7900 - val_loss: 1.3551 - val_binary_accuracy: 0.6500\n",
      "Epoch 38/200\n",
      "100/100 [==============================] - 0s 387us/sample - loss: 0.3736 - binary_accuracy: 0.7900 - val_loss: 1.3912 - val_binary_accuracy: 0.5500\n",
      "Epoch 39/200\n",
      "100/100 [==============================] - 0s 386us/sample - loss: 0.3956 - binary_accuracy: 0.8200 - val_loss: 1.3474 - val_binary_accuracy: 0.5500\n",
      "Epoch 40/200\n",
      "100/100 [==============================] - 0s 353us/sample - loss: 0.3533 - binary_accuracy: 0.8100 - val_loss: 1.2897 - val_binary_accuracy: 0.5500\n",
      "Epoch 41/200\n",
      "100/100 [==============================] - 0s 334us/sample - loss: 0.4036 - binary_accuracy: 0.8100 - val_loss: 1.2839 - val_binary_accuracy: 0.5500\n",
      "Epoch 42/200\n",
      "100/100 [==============================] - 0s 332us/sample - loss: 0.3567 - binary_accuracy: 0.8700 - val_loss: 1.2537 - val_binary_accuracy: 0.5000\n",
      "Epoch 43/200\n",
      "100/100 [==============================] - 0s 333us/sample - loss: 0.3697 - binary_accuracy: 0.8700 - val_loss: 1.3676 - val_binary_accuracy: 0.5000\n",
      "Epoch 44/200\n",
      "100/100 [==============================] - 0s 313us/sample - loss: 0.3572 - binary_accuracy: 0.8400 - val_loss: 1.4046 - val_binary_accuracy: 0.5000\n",
      "Epoch 45/200\n",
      "100/100 [==============================] - 0s 393us/sample - loss: 0.3424 - binary_accuracy: 0.8200 - val_loss: 1.4173 - val_binary_accuracy: 0.6000\n",
      "Epoch 46/200\n",
      "100/100 [==============================] - 0s 368us/sample - loss: 0.3623 - binary_accuracy: 0.8200 - val_loss: 1.3228 - val_binary_accuracy: 0.6000\n",
      "Epoch 47/200\n",
      "100/100 [==============================] - 0s 309us/sample - loss: 0.3316 - binary_accuracy: 0.8600 - val_loss: 1.4081 - val_binary_accuracy: 0.6000\n",
      "Epoch 48/200\n",
      "100/100 [==============================] - 0s 301us/sample - loss: 0.3658 - binary_accuracy: 0.8300 - val_loss: 1.2992 - val_binary_accuracy: 0.6000\n",
      "Epoch 49/200\n",
      "100/100 [==============================] - 0s 360us/sample - loss: 0.4335 - binary_accuracy: 0.7700 - val_loss: 1.2080 - val_binary_accuracy: 0.6000\n",
      "Epoch 50/200\n",
      "100/100 [==============================] - 0s 309us/sample - loss: 0.3369 - binary_accuracy: 0.8800 - val_loss: 1.2894 - val_binary_accuracy: 0.5500\n",
      "Epoch 51/200\n",
      "100/100 [==============================] - 0s 338us/sample - loss: 0.3444 - binary_accuracy: 0.8100 - val_loss: 1.2167 - val_binary_accuracy: 0.5500\n",
      "Epoch 52/200\n",
      "100/100 [==============================] - 0s 304us/sample - loss: 0.3918 - binary_accuracy: 0.7900 - val_loss: 1.2325 - val_binary_accuracy: 0.6000\n",
      "Epoch 53/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.3649 - binary_accuracy: 0.8400 - val_loss: 1.5268 - val_binary_accuracy: 0.4500\n",
      "Epoch 54/200\n",
      "100/100 [==============================] - 0s 331us/sample - loss: 0.2857 - binary_accuracy: 0.8300 - val_loss: 1.5491 - val_binary_accuracy: 0.5500\n",
      "Epoch 55/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.2328 - binary_accuracy: 0.9000 - val_loss: 1.6073 - val_binary_accuracy: 0.6500\n",
      "Epoch 56/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.4184 - binary_accuracy: 0.8100 - val_loss: 1.4392 - val_binary_accuracy: 0.6500\n",
      "Epoch 57/200\n",
      "100/100 [==============================] - 0s 309us/sample - loss: 0.2695 - binary_accuracy: 0.8600 - val_loss: 1.3097 - val_binary_accuracy: 0.4000\n",
      "Epoch 58/200\n",
      "100/100 [==============================] - 0s 363us/sample - loss: 0.3773 - binary_accuracy: 0.8600 - val_loss: 1.8971 - val_binary_accuracy: 0.3500\n",
      "Epoch 59/200\n",
      "100/100 [==============================] - 0s 346us/sample - loss: 0.3547 - binary_accuracy: 0.8400 - val_loss: 1.4074 - val_binary_accuracy: 0.4000\n",
      "Epoch 60/200\n",
      "100/100 [==============================] - 0s 390us/sample - loss: 0.3343 - binary_accuracy: 0.8700 - val_loss: 1.4510 - val_binary_accuracy: 0.6000\n",
      "Epoch 61/200\n",
      "100/100 [==============================] - 0s 334us/sample - loss: 0.1587 - binary_accuracy: 0.9600 - val_loss: 1.7196 - val_binary_accuracy: 0.6000\n",
      "Epoch 62/200\n",
      "100/100 [==============================] - 0s 339us/sample - loss: 0.4640 - binary_accuracy: 0.8500 - val_loss: 1.7913 - val_binary_accuracy: 0.5500\n",
      "Epoch 63/200\n",
      "100/100 [==============================] - 0s 324us/sample - loss: 0.3308 - binary_accuracy: 0.8700 - val_loss: 1.8040 - val_binary_accuracy: 0.5000\n",
      "Epoch 64/200\n",
      "100/100 [==============================] - 0s 340us/sample - loss: 0.3015 - binary_accuracy: 0.8700 - val_loss: 2.4023 - val_binary_accuracy: 0.4500\n",
      "Epoch 65/200\n",
      "100/100 [==============================] - 0s 373us/sample - loss: 0.2848 - binary_accuracy: 0.9100 - val_loss: 2.6385 - val_binary_accuracy: 0.4500\n",
      "Epoch 66/200\n",
      "100/100 [==============================] - 0s 368us/sample - loss: 0.2432 - binary_accuracy: 0.9000 - val_loss: 2.4215 - val_binary_accuracy: 0.4500\n",
      "Epoch 67/200\n",
      "100/100 [==============================] - 0s 420us/sample - loss: 0.2455 - binary_accuracy: 0.8900 - val_loss: 2.1760 - val_binary_accuracy: 0.5000\n",
      "Epoch 68/200\n",
      "100/100 [==============================] - 0s 369us/sample - loss: 0.3294 - binary_accuracy: 0.8500 - val_loss: 2.2317 - val_binary_accuracy: 0.5000\n",
      "Epoch 69/200\n",
      "100/100 [==============================] - 0s 407us/sample - loss: 0.3581 - binary_accuracy: 0.8600 - val_loss: 2.4132 - val_binary_accuracy: 0.5000\n",
      "Epoch 70/200\n",
      "100/100 [==============================] - 0s 387us/sample - loss: 0.3692 - binary_accuracy: 0.8300 - val_loss: 2.2258 - val_binary_accuracy: 0.5500\n",
      "Epoch 71/200\n",
      "100/100 [==============================] - 0s 409us/sample - loss: 0.2198 - binary_accuracy: 0.9200 - val_loss: 1.6814 - val_binary_accuracy: 0.6000\n",
      "Epoch 72/200\n",
      "100/100 [==============================] - 0s 401us/sample - loss: 0.2689 - binary_accuracy: 0.8900 - val_loss: 2.8706 - val_binary_accuracy: 0.4000\n",
      "Epoch 73/200\n",
      "100/100 [==============================] - 0s 361us/sample - loss: 0.2467 - binary_accuracy: 0.8700 - val_loss: 3.8118 - val_binary_accuracy: 0.3500\n",
      "Epoch 74/200\n",
      "100/100 [==============================] - 0s 387us/sample - loss: 0.2873 - binary_accuracy: 0.8400 - val_loss: 3.0685 - val_binary_accuracy: 0.3000\n",
      "Epoch 75/200\n",
      "100/100 [==============================] - 0s 341us/sample - loss: 0.2733 - binary_accuracy: 0.8600 - val_loss: 2.8622 - val_binary_accuracy: 0.4500\n",
      "Epoch 76/200\n",
      "100/100 [==============================] - 0s 304us/sample - loss: 0.2497 - binary_accuracy: 0.8600 - val_loss: 2.9736 - val_binary_accuracy: 0.4500\n",
      "Epoch 77/200\n",
      "100/100 [==============================] - 0s 337us/sample - loss: 0.2677 - binary_accuracy: 0.9000 - val_loss: 2.9795 - val_binary_accuracy: 0.3500\n",
      "Epoch 78/200\n",
      "100/100 [==============================] - 0s 346us/sample - loss: 0.4302 - binary_accuracy: 0.8200 - val_loss: 2.9292 - val_binary_accuracy: 0.4500\n",
      "Epoch 79/200\n",
      "100/100 [==============================] - 0s 364us/sample - loss: 0.3156 - binary_accuracy: 0.8300 - val_loss: 2.7835 - val_binary_accuracy: 0.4500\n",
      "Epoch 80/200\n",
      "100/100 [==============================] - 0s 351us/sample - loss: 0.2461 - binary_accuracy: 0.8800 - val_loss: 2.6225 - val_binary_accuracy: 0.3500\n",
      "Epoch 81/200\n",
      "100/100 [==============================] - 0s 367us/sample - loss: 0.2245 - binary_accuracy: 0.9100 - val_loss: 2.8258 - val_binary_accuracy: 0.3000\n",
      "Epoch 82/200\n",
      "100/100 [==============================] - 0s 371us/sample - loss: 0.2518 - binary_accuracy: 0.8700 - val_loss: 2.5962 - val_binary_accuracy: 0.3000\n",
      "Epoch 83/200\n",
      "100/100 [==============================] - 0s 342us/sample - loss: 0.2485 - binary_accuracy: 0.8800 - val_loss: 2.4645 - val_binary_accuracy: 0.4000\n",
      "Epoch 84/200\n",
      "100/100 [==============================] - 0s 380us/sample - loss: 0.2654 - binary_accuracy: 0.8800 - val_loss: 2.4725 - val_binary_accuracy: 0.4000\n",
      "Epoch 85/200\n",
      "100/100 [==============================] - 0s 371us/sample - loss: 0.2768 - binary_accuracy: 0.8800 - val_loss: 2.6344 - val_binary_accuracy: 0.3500\n",
      "Epoch 86/200\n",
      "100/100 [==============================] - 0s 366us/sample - loss: 0.2805 - binary_accuracy: 0.8900 - val_loss: 2.4916 - val_binary_accuracy: 0.3000\n",
      "Epoch 87/200\n",
      "100/100 [==============================] - 0s 324us/sample - loss: 0.2056 - binary_accuracy: 0.9400 - val_loss: 2.5820 - val_binary_accuracy: 0.2500\n",
      "Epoch 88/200\n",
      "100/100 [==============================] - 0s 325us/sample - loss: 0.3214 - binary_accuracy: 0.8400 - val_loss: 2.2642 - val_binary_accuracy: 0.4500\n",
      "Epoch 89/200\n",
      "100/100 [==============================] - 0s 392us/sample - loss: 0.2535 - binary_accuracy: 0.8700 - val_loss: 2.3706 - val_binary_accuracy: 0.5000\n",
      "Epoch 90/200\n",
      "100/100 [==============================] - 0s 466us/sample - loss: 0.4528 - binary_accuracy: 0.7600 - val_loss: 2.4668 - val_binary_accuracy: 0.5000\n",
      "Epoch 91/200\n",
      "100/100 [==============================] - 0s 400us/sample - loss: 0.2460 - binary_accuracy: 0.8900 - val_loss: 2.5475 - val_binary_accuracy: 0.3500\n",
      "Epoch 92/200\n",
      "100/100 [==============================] - 0s 401us/sample - loss: 0.3065 - binary_accuracy: 0.9000 - val_loss: 2.2682 - val_binary_accuracy: 0.4000\n",
      "Epoch 93/200\n",
      "100/100 [==============================] - 0s 372us/sample - loss: 0.4120 - binary_accuracy: 0.8400 - val_loss: 1.8605 - val_binary_accuracy: 0.4000\n",
      "Epoch 94/200\n",
      "100/100 [==============================] - 0s 392us/sample - loss: 0.2906 - binary_accuracy: 0.8700 - val_loss: 1.8997 - val_binary_accuracy: 0.4500\n",
      "Epoch 95/200\n",
      "100/100 [==============================] - 0s 368us/sample - loss: 0.2824 - binary_accuracy: 0.8900 - val_loss: 1.9834 - val_binary_accuracy: 0.5000\n",
      "Epoch 96/200\n",
      "100/100 [==============================] - 0s 355us/sample - loss: 0.2346 - binary_accuracy: 0.8900 - val_loss: 2.1375 - val_binary_accuracy: 0.5000\n",
      "Epoch 97/200\n",
      "100/100 [==============================] - 0s 300us/sample - loss: 0.2473 - binary_accuracy: 0.8800 - val_loss: 2.3027 - val_binary_accuracy: 0.5000\n",
      "Epoch 98/200\n",
      "100/100 [==============================] - 0s 335us/sample - loss: 0.3630 - binary_accuracy: 0.8400 - val_loss: 2.2697 - val_binary_accuracy: 0.6000\n",
      "Epoch 99/200\n",
      "100/100 [==============================] - 0s 328us/sample - loss: 0.2722 - binary_accuracy: 0.8600 - val_loss: 2.3326 - val_binary_accuracy: 0.6000\n",
      "Epoch 100/200\n",
      "100/100 [==============================] - 0s 332us/sample - loss: 0.2938 - binary_accuracy: 0.8500 - val_loss: 2.3910 - val_binary_accuracy: 0.6000\n",
      "Epoch 101/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 364us/sample - loss: 0.2866 - binary_accuracy: 0.8800 - val_loss: 2.2827 - val_binary_accuracy: 0.6000\n",
      "Epoch 102/200\n",
      "100/100 [==============================] - 0s 343us/sample - loss: 0.2329 - binary_accuracy: 0.9000 - val_loss: 2.2981 - val_binary_accuracy: 0.5000\n",
      "Epoch 103/200\n",
      "100/100 [==============================] - 0s 325us/sample - loss: 0.3741 - binary_accuracy: 0.8200 - val_loss: 2.1747 - val_binary_accuracy: 0.4500\n",
      "Epoch 104/200\n",
      "100/100 [==============================] - 0s 336us/sample - loss: 0.3603 - binary_accuracy: 0.8700 - val_loss: 2.0072 - val_binary_accuracy: 0.5500\n",
      "Epoch 105/200\n",
      "100/100 [==============================] - 0s 382us/sample - loss: 0.4157 - binary_accuracy: 0.8100 - val_loss: 1.7725 - val_binary_accuracy: 0.4000\n",
      "Epoch 106/200\n",
      "100/100 [==============================] - 0s 367us/sample - loss: 0.3455 - binary_accuracy: 0.8300 - val_loss: 1.6990 - val_binary_accuracy: 0.4500\n",
      "Epoch 107/200\n",
      "100/100 [==============================] - 0s 356us/sample - loss: 0.2396 - binary_accuracy: 0.9300 - val_loss: 1.7104 - val_binary_accuracy: 0.4500\n",
      "Epoch 108/200\n",
      "100/100 [==============================] - 0s 349us/sample - loss: 0.2359 - binary_accuracy: 0.9000 - val_loss: 1.6916 - val_binary_accuracy: 0.4500\n",
      "Epoch 109/200\n",
      "100/100 [==============================] - 0s 357us/sample - loss: 0.2332 - binary_accuracy: 0.9100 - val_loss: 2.0246 - val_binary_accuracy: 0.4000\n",
      "Epoch 110/200\n",
      "100/100 [==============================] - 0s 343us/sample - loss: 0.3100 - binary_accuracy: 0.8800 - val_loss: 2.4052 - val_binary_accuracy: 0.4000\n",
      "Epoch 111/200\n",
      "100/100 [==============================] - 0s 377us/sample - loss: 0.2905 - binary_accuracy: 0.8700 - val_loss: 2.2760 - val_binary_accuracy: 0.4000\n",
      "Epoch 112/200\n",
      "100/100 [==============================] - 0s 391us/sample - loss: 0.2242 - binary_accuracy: 0.9400 - val_loss: 2.1935 - val_binary_accuracy: 0.5500\n",
      "Epoch 113/200\n",
      "100/100 [==============================] - 0s 390us/sample - loss: 0.2993 - binary_accuracy: 0.8700 - val_loss: 2.0405 - val_binary_accuracy: 0.4500\n",
      "Epoch 114/200\n",
      "100/100 [==============================] - 0s 453us/sample - loss: 0.2435 - binary_accuracy: 0.8900 - val_loss: 2.0991 - val_binary_accuracy: 0.5000\n",
      "Epoch 115/200\n",
      "100/100 [==============================] - 0s 404us/sample - loss: 0.4231 - binary_accuracy: 0.8300 - val_loss: 2.5274 - val_binary_accuracy: 0.6000\n",
      "Epoch 116/200\n",
      "100/100 [==============================] - 0s 390us/sample - loss: 0.2011 - binary_accuracy: 0.9200 - val_loss: 3.0116 - val_binary_accuracy: 0.5500\n",
      "Epoch 117/200\n",
      "100/100 [==============================] - 0s 426us/sample - loss: 0.2003 - binary_accuracy: 0.9100 - val_loss: 2.9429 - val_binary_accuracy: 0.5500\n",
      "Epoch 118/200\n",
      "100/100 [==============================] - 0s 405us/sample - loss: 0.2519 - binary_accuracy: 0.8900 - val_loss: 2.5919 - val_binary_accuracy: 0.6000\n",
      "Epoch 119/200\n",
      "100/100 [==============================] - 0s 384us/sample - loss: 0.3482 - binary_accuracy: 0.8300 - val_loss: 2.4013 - val_binary_accuracy: 0.6000\n",
      "Epoch 120/200\n",
      "100/100 [==============================] - 0s 379us/sample - loss: 0.2120 - binary_accuracy: 0.9100 - val_loss: 2.2950 - val_binary_accuracy: 0.5000\n",
      "Epoch 121/200\n",
      "100/100 [==============================] - 0s 355us/sample - loss: 0.2629 - binary_accuracy: 0.8600 - val_loss: 2.2118 - val_binary_accuracy: 0.5000\n",
      "Epoch 122/200\n",
      "100/100 [==============================] - 0s 350us/sample - loss: 0.1993 - binary_accuracy: 0.9100 - val_loss: 2.2396 - val_binary_accuracy: 0.4500\n",
      "Epoch 123/200\n",
      "100/100 [==============================] - 0s 364us/sample - loss: 0.2396 - binary_accuracy: 0.8900 - val_loss: 2.0067 - val_binary_accuracy: 0.4500\n",
      "Epoch 124/200\n",
      "100/100 [==============================] - 0s 320us/sample - loss: 0.1953 - binary_accuracy: 0.9300 - val_loss: 2.3916 - val_binary_accuracy: 0.3500\n",
      "Epoch 125/200\n",
      "100/100 [==============================] - 0s 326us/sample - loss: 0.2327 - binary_accuracy: 0.8900 - val_loss: 2.5803 - val_binary_accuracy: 0.3500\n",
      "Epoch 126/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.2473 - binary_accuracy: 0.9000 - val_loss: 2.3553 - val_binary_accuracy: 0.4500\n",
      "Epoch 127/200\n",
      "100/100 [==============================] - 0s 306us/sample - loss: 0.2356 - binary_accuracy: 0.9100 - val_loss: 2.2604 - val_binary_accuracy: 0.5000\n",
      "Epoch 128/200\n",
      "100/100 [==============================] - 0s 323us/sample - loss: 0.2303 - binary_accuracy: 0.8800 - val_loss: 2.5158 - val_binary_accuracy: 0.4500\n",
      "Epoch 129/200\n",
      "100/100 [==============================] - 0s 386us/sample - loss: 0.3093 - binary_accuracy: 0.8800 - val_loss: 2.7981 - val_binary_accuracy: 0.4500\n",
      "Epoch 130/200\n",
      "100/100 [==============================] - 0s 343us/sample - loss: 0.2538 - binary_accuracy: 0.8700 - val_loss: 3.0821 - val_binary_accuracy: 0.4500\n",
      "Epoch 131/200\n",
      "100/100 [==============================] - 0s 337us/sample - loss: 0.2379 - binary_accuracy: 0.8900 - val_loss: 2.9077 - val_binary_accuracy: 0.4500\n",
      "Epoch 132/200\n",
      "100/100 [==============================] - 0s 349us/sample - loss: 0.2186 - binary_accuracy: 0.9200 - val_loss: 2.4706 - val_binary_accuracy: 0.5000\n",
      "Epoch 133/200\n",
      "100/100 [==============================] - 0s 337us/sample - loss: 0.2303 - binary_accuracy: 0.9200 - val_loss: 2.3081 - val_binary_accuracy: 0.6000\n",
      "Epoch 134/200\n",
      "100/100 [==============================] - 0s 380us/sample - loss: 0.2016 - binary_accuracy: 0.9300 - val_loss: 2.9552 - val_binary_accuracy: 0.5000\n",
      "Epoch 135/200\n",
      "100/100 [==============================] - 0s 400us/sample - loss: 0.1747 - binary_accuracy: 0.9400 - val_loss: 2.9080 - val_binary_accuracy: 0.5000\n",
      "Epoch 136/200\n",
      "100/100 [==============================] - 0s 394us/sample - loss: 0.3531 - binary_accuracy: 0.8200 - val_loss: 2.9154 - val_binary_accuracy: 0.5000\n",
      "Epoch 137/200\n",
      "100/100 [==============================] - 0s 405us/sample - loss: 0.3264 - binary_accuracy: 0.8500 - val_loss: 2.4207 - val_binary_accuracy: 0.5000\n",
      "Epoch 138/200\n",
      "100/100 [==============================] - 0s 415us/sample - loss: 0.4133 - binary_accuracy: 0.8600 - val_loss: 2.0073 - val_binary_accuracy: 0.6000\n",
      "Epoch 139/200\n",
      "100/100 [==============================] - 0s 416us/sample - loss: 0.5446 - binary_accuracy: 0.8000 - val_loss: 1.8792 - val_binary_accuracy: 0.5500\n",
      "Epoch 140/200\n",
      "100/100 [==============================] - 0s 408us/sample - loss: 0.4438 - binary_accuracy: 0.8200 - val_loss: 1.7072 - val_binary_accuracy: 0.5500\n",
      "Epoch 141/200\n",
      "100/100 [==============================] - 0s 409us/sample - loss: 0.3701 - binary_accuracy: 0.8000 - val_loss: 1.6194 - val_binary_accuracy: 0.5500\n",
      "Epoch 142/200\n",
      "100/100 [==============================] - 0s 365us/sample - loss: 0.4418 - binary_accuracy: 0.8100 - val_loss: 1.8486 - val_binary_accuracy: 0.6000\n",
      "Epoch 143/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.3494 - binary_accuracy: 0.8000 - val_loss: 1.9539 - val_binary_accuracy: 0.6000\n",
      "Epoch 144/200\n",
      "100/100 [==============================] - 0s 362us/sample - loss: 0.3195 - binary_accuracy: 0.8100 - val_loss: 1.9688 - val_binary_accuracy: 0.5500\n",
      "Epoch 145/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.3221 - binary_accuracy: 0.8700 - val_loss: 2.0427 - val_binary_accuracy: 0.5500\n",
      "Epoch 146/200\n",
      "100/100 [==============================] - 0s 357us/sample - loss: 0.4194 - binary_accuracy: 0.7700 - val_loss: 2.1411 - val_binary_accuracy: 0.4500\n",
      "Epoch 147/200\n",
      "100/100 [==============================] - 0s 321us/sample - loss: 0.3587 - binary_accuracy: 0.8100 - val_loss: 2.0900 - val_binary_accuracy: 0.4500\n",
      "Epoch 148/200\n",
      "100/100 [==============================] - 0s 355us/sample - loss: 0.3116 - binary_accuracy: 0.8500 - val_loss: 2.1737 - val_binary_accuracy: 0.4500\n",
      "Epoch 149/200\n",
      "100/100 [==============================] - 0s 345us/sample - loss: 0.3423 - binary_accuracy: 0.8200 - val_loss: 2.1291 - val_binary_accuracy: 0.4500\n",
      "Epoch 150/200\n",
      "100/100 [==============================] - 0s 351us/sample - loss: 0.3435 - binary_accuracy: 0.8500 - val_loss: 2.2217 - val_binary_accuracy: 0.4500\n",
      "Epoch 151/200\n",
      "100/100 [==============================] - 0s 325us/sample - loss: 0.3748 - binary_accuracy: 0.8300 - val_loss: 2.2139 - val_binary_accuracy: 0.4500\n",
      "Epoch 152/200\n",
      "100/100 [==============================] - 0s 352us/sample - loss: 0.3564 - binary_accuracy: 0.8300 - val_loss: 2.0207 - val_binary_accuracy: 0.4500\n",
      "Epoch 153/200\n",
      "100/100 [==============================] - 0s 416us/sample - loss: 0.3413 - binary_accuracy: 0.8500 - val_loss: 1.8997 - val_binary_accuracy: 0.4000\n",
      "Epoch 154/200\n",
      "100/100 [==============================] - 0s 369us/sample - loss: 0.3150 - binary_accuracy: 0.8500 - val_loss: 1.9308 - val_binary_accuracy: 0.3500\n",
      "Epoch 155/200\n",
      "100/100 [==============================] - 0s 406us/sample - loss: 0.2981 - binary_accuracy: 0.8700 - val_loss: 1.9756 - val_binary_accuracy: 0.4500\n",
      "Epoch 156/200\n",
      "100/100 [==============================] - 0s 401us/sample - loss: 0.2685 - binary_accuracy: 0.8700 - val_loss: 2.1692 - val_binary_accuracy: 0.5000\n",
      "Epoch 157/200\n",
      "100/100 [==============================] - 0s 404us/sample - loss: 0.3469 - binary_accuracy: 0.8400 - val_loss: 2.3002 - val_binary_accuracy: 0.5000\n",
      "Epoch 158/200\n",
      "100/100 [==============================] - 0s 395us/sample - loss: 0.2583 - binary_accuracy: 0.9100 - val_loss: 2.3196 - val_binary_accuracy: 0.5000\n",
      "Epoch 159/200\n",
      "100/100 [==============================] - 0s 362us/sample - loss: 0.3059 - binary_accuracy: 0.8400 - val_loss: 2.4738 - val_binary_accuracy: 0.5000\n",
      "Epoch 160/200\n",
      "100/100 [==============================] - 0s 312us/sample - loss: 0.2779 - binary_accuracy: 0.8400 - val_loss: 2.6132 - val_binary_accuracy: 0.6000\n",
      "Epoch 161/200\n",
      "100/100 [==============================] - 0s 364us/sample - loss: 0.3134 - binary_accuracy: 0.8600 - val_loss: 2.7675 - val_binary_accuracy: 0.6000\n",
      "Epoch 162/200\n",
      "100/100 [==============================] - 0s 396us/sample - loss: 0.4401 - binary_accuracy: 0.7800 - val_loss: 2.6107 - val_binary_accuracy: 0.6000\n",
      "Epoch 163/200\n",
      "100/100 [==============================] - 0s 327us/sample - loss: 0.3344 - binary_accuracy: 0.8400 - val_loss: 2.2643 - val_binary_accuracy: 0.6000\n",
      "Epoch 164/200\n",
      "100/100 [==============================] - 0s 337us/sample - loss: 0.2762 - binary_accuracy: 0.8800 - val_loss: 2.2444 - val_binary_accuracy: 0.6000\n",
      "Epoch 165/200\n",
      "100/100 [==============================] - 0s 284us/sample - loss: 0.3054 - binary_accuracy: 0.8700 - val_loss: 2.1710 - val_binary_accuracy: 0.5500\n",
      "Epoch 166/200\n",
      "100/100 [==============================] - 0s 303us/sample - loss: 0.2859 - binary_accuracy: 0.8700 - val_loss: 2.3003 - val_binary_accuracy: 0.5500\n",
      "Epoch 167/200\n",
      "100/100 [==============================] - 0s 334us/sample - loss: 0.2892 - binary_accuracy: 0.8900 - val_loss: 2.4799 - val_binary_accuracy: 0.5500\n",
      "Epoch 168/200\n",
      "100/100 [==============================] - 0s 318us/sample - loss: 0.2009 - binary_accuracy: 0.9100 - val_loss: 2.7830 - val_binary_accuracy: 0.5000\n",
      "Epoch 169/200\n",
      "100/100 [==============================] - 0s 411us/sample - loss: 0.2343 - binary_accuracy: 0.9000 - val_loss: 3.0495 - val_binary_accuracy: 0.5500\n",
      "Epoch 170/200\n",
      "100/100 [==============================] - 0s 312us/sample - loss: 0.4565 - binary_accuracy: 0.8400 - val_loss: 3.0951 - val_binary_accuracy: 0.5500\n",
      "Epoch 171/200\n",
      "100/100 [==============================] - 0s 384us/sample - loss: 0.3316 - binary_accuracy: 0.8500 - val_loss: 2.7828 - val_binary_accuracy: 0.5000\n",
      "Epoch 172/200\n",
      "100/100 [==============================] - 0s 363us/sample - loss: 0.3289 - binary_accuracy: 0.8500 - val_loss: 2.5572 - val_binary_accuracy: 0.5000\n",
      "Epoch 173/200\n",
      "100/100 [==============================] - 0s 376us/sample - loss: 0.2290 - binary_accuracy: 0.9200 - val_loss: 2.6304 - val_binary_accuracy: 0.5000\n",
      "Epoch 174/200\n",
      "100/100 [==============================] - 0s 339us/sample - loss: 0.2781 - binary_accuracy: 0.8800 - val_loss: 2.6380 - val_binary_accuracy: 0.5000\n",
      "Epoch 175/200\n",
      "100/100 [==============================] - 0s 368us/sample - loss: 0.3047 - binary_accuracy: 0.8500 - val_loss: 2.6301 - val_binary_accuracy: 0.5000\n",
      "Epoch 176/200\n",
      "100/100 [==============================] - 0s 427us/sample - loss: 0.2842 - binary_accuracy: 0.8400 - val_loss: 2.5546 - val_binary_accuracy: 0.5000\n",
      "Epoch 177/200\n",
      "100/100 [==============================] - 0s 356us/sample - loss: 0.2889 - binary_accuracy: 0.8700 - val_loss: 2.6565 - val_binary_accuracy: 0.5500\n",
      "Epoch 178/200\n",
      "100/100 [==============================] - 0s 391us/sample - loss: 0.2678 - binary_accuracy: 0.8900 - val_loss: 2.7371 - val_binary_accuracy: 0.5500\n",
      "Epoch 179/200\n",
      "100/100 [==============================] - 0s 407us/sample - loss: 0.2668 - binary_accuracy: 0.8900 - val_loss: 2.8237 - val_binary_accuracy: 0.5500\n",
      "Epoch 180/200\n",
      "100/100 [==============================] - 0s 418us/sample - loss: 0.2627 - binary_accuracy: 0.8900 - val_loss: 2.8118 - val_binary_accuracy: 0.5500\n",
      "Epoch 181/200\n",
      "100/100 [==============================] - 0s 362us/sample - loss: 0.3635 - binary_accuracy: 0.8400 - val_loss: 2.6885 - val_binary_accuracy: 0.5500\n",
      "Epoch 182/200\n",
      "100/100 [==============================] - 0s 394us/sample - loss: 0.3968 - binary_accuracy: 0.8100 - val_loss: 2.3058 - val_binary_accuracy: 0.5500\n",
      "Epoch 183/200\n",
      "100/100 [==============================] - 0s 351us/sample - loss: 0.2432 - binary_accuracy: 0.9000 - val_loss: 2.2010 - val_binary_accuracy: 0.5500\n",
      "Epoch 184/200\n",
      "100/100 [==============================] - 0s 351us/sample - loss: 0.2902 - binary_accuracy: 0.9000 - val_loss: 2.2032 - val_binary_accuracy: 0.5000\n",
      "Epoch 185/200\n",
      "100/100 [==============================] - 0s 370us/sample - loss: 0.4917 - binary_accuracy: 0.7900 - val_loss: 2.4006 - val_binary_accuracy: 0.5000\n",
      "Epoch 186/200\n",
      "100/100 [==============================] - 0s 394us/sample - loss: 0.3188 - binary_accuracy: 0.8900 - val_loss: 2.3588 - val_binary_accuracy: 0.4500\n",
      "Epoch 187/200\n",
      "100/100 [==============================] - 0s 344us/sample - loss: 0.2460 - binary_accuracy: 0.8900 - val_loss: 2.3239 - val_binary_accuracy: 0.5000\n",
      "Epoch 188/200\n",
      "100/100 [==============================] - 0s 347us/sample - loss: 0.2515 - binary_accuracy: 0.9000 - val_loss: 2.2997 - val_binary_accuracy: 0.4500\n",
      "Epoch 189/200\n",
      "100/100 [==============================] - 0s 364us/sample - loss: 0.3093 - binary_accuracy: 0.8400 - val_loss: 2.3979 - val_binary_accuracy: 0.4500\n",
      "Epoch 190/200\n",
      "100/100 [==============================] - 0s 386us/sample - loss: 0.2938 - binary_accuracy: 0.8400 - val_loss: 2.2617 - val_binary_accuracy: 0.5000\n",
      "Epoch 191/200\n",
      "100/100 [==============================] - 0s 341us/sample - loss: 0.2443 - binary_accuracy: 0.8900 - val_loss: 2.2962 - val_binary_accuracy: 0.5000\n",
      "Epoch 192/200\n",
      "100/100 [==============================] - 0s 370us/sample - loss: 0.2562 - binary_accuracy: 0.9000 - val_loss: 2.4141 - val_binary_accuracy: 0.5500\n",
      "Epoch 193/200\n",
      "100/100 [==============================] - 0s 374us/sample - loss: 0.2823 - binary_accuracy: 0.9100 - val_loss: 2.5135 - val_binary_accuracy: 0.5500\n",
      "Epoch 194/200\n",
      "100/100 [==============================] - 0s 341us/sample - loss: 0.2916 - binary_accuracy: 0.8600 - val_loss: 2.5850 - val_binary_accuracy: 0.4500\n",
      "Epoch 195/200\n",
      "100/100 [==============================] - 0s 385us/sample - loss: 0.2752 - binary_accuracy: 0.8800 - val_loss: 2.5482 - val_binary_accuracy: 0.4500\n",
      "Epoch 196/200\n",
      "100/100 [==============================] - 0s 373us/sample - loss: 0.2957 - binary_accuracy: 0.8900 - val_loss: 2.6397 - val_binary_accuracy: 0.4500\n",
      "Epoch 197/200\n",
      "100/100 [==============================] - 0s 394us/sample - loss: 0.2470 - binary_accuracy: 0.8500 - val_loss: 2.9284 - val_binary_accuracy: 0.2500\n",
      "Epoch 198/200\n",
      "100/100 [==============================] - 0s 370us/sample - loss: 0.2297 - binary_accuracy: 0.9100 - val_loss: 2.9768 - val_binary_accuracy: 0.3500\n",
      "Epoch 199/200\n",
      "100/100 [==============================] - 0s 404us/sample - loss: 0.1989 - binary_accuracy: 0.9200 - val_loss: 2.8021 - val_binary_accuracy: 0.4500\n",
      "Epoch 200/200\n",
      "100/100 [==============================] - 0s 395us/sample - loss: 0.2121 - binary_accuracy: 0.9100 - val_loss: 2.7107 - val_binary_accuracy: 0.5000\n"
     ]
    }
   ],
   "source": [
    "encoder_prediction = gpu_encoder_model.predict([train_context_padded_seq[:100],train_question_seq_padded[:100]])\n",
    "encoder_validation_prediction = gpu_encoder_model.predict([val_context_padded_seq[:20],val_question_seq_padded[:20]])\n",
    "gpu_history_feasibility_model = gpu_feasibility_model.fit(encoder_prediction,train_answer_impossible[:100],\n",
    "                                                          epochs=200,batch_size=10,\n",
    "                                                          validation_data = (encoder_validation_prediction,val_answer_impossible[:20])\n",
    "                                                          )\n",
    "\n",
    "\n",
    "gpu_answer_model.save('gpu_answer_model_2.h5')\n",
    "gpu_encoder_model.save('gpu_encoder_model_2.h5')\n",
    "gpu_decoder_model.save('gpu_decoder_model_2.h5')\n",
    "gpu_feasibility_model.save('gpu_feasibility_model_2.h5')\n",
    "with open('gpu_history_answer_model2', 'wb') as file_history:\n",
    "        pickle.dump(gpu_history_answer_model.history, file_history)\n",
    "with open('gpu_history_feasibility_model2', 'wb') as file_history:\n",
    "        pickle.dump(gpu_history_feasibility_model.history, file_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "favorite_color = pickle.load( open( \"gpu_history_answer_model2\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.0021465957164764,\n",
       "  0.8581745982170105,\n",
       "  0.622495511174202,\n",
       "  0.454390549659729,\n",
       "  0.3440942779183388,\n",
       "  0.3245418131351471,\n",
       "  0.29935378283262254,\n",
       "  0.29812543243169787,\n",
       "  0.2746981203556061,\n",
       "  0.28004816025495527,\n",
       "  0.2624157935380936,\n",
       "  0.26698136031627656,\n",
       "  0.25928100645542146,\n",
       "  0.25722589492797854,\n",
       "  0.25440490916371344,\n",
       "  0.25769960433244704,\n",
       "  0.24973910450935363,\n",
       "  0.2420899584889412,\n",
       "  0.23934334218502046,\n",
       "  0.23876624479889869,\n",
       "  0.234937684237957,\n",
       "  0.2327175408601761,\n",
       "  0.22930850982666015,\n",
       "  0.22928718999028205,\n",
       "  0.22982289344072343,\n",
       "  0.225761841237545,\n",
       "  0.22956837564706803,\n",
       "  0.22473053336143495,\n",
       "  0.21714958176016808,\n",
       "  0.2204847015440464,\n",
       "  0.21618397086858748,\n",
       "  0.21761858314275742,\n",
       "  0.21991693899035453,\n",
       "  0.21742043346166612,\n",
       "  0.21079502627253532,\n",
       "  0.21146002858877183,\n",
       "  0.20340538248419762,\n",
       "  0.21868807077407837,\n",
       "  0.21210432946681976,\n",
       "  0.20876455157995225,\n",
       "  0.20554210022091865,\n",
       "  0.20476813465356827,\n",
       "  0.20861533284187317,\n",
       "  0.20637502372264863,\n",
       "  0.1961271919310093,\n",
       "  0.20238928273320198,\n",
       "  0.20183939188718797,\n",
       "  0.2030742011964321,\n",
       "  0.2143104061484337,\n",
       "  0.1948381707072258,\n",
       "  0.19374823272228242,\n",
       "  0.20006551295518876,\n",
       "  0.20202587321400642,\n",
       "  0.20065798163414,\n",
       "  0.19938291162252425,\n",
       "  0.19546501263976096,\n",
       "  0.19516662359237671,\n",
       "  0.1925404079258442,\n",
       "  0.19211626350879668,\n",
       "  0.19392872378230094,\n",
       "  0.1922631025314331,\n",
       "  0.18716222867369653,\n",
       "  0.19200122952461243,\n",
       "  0.18565103709697722,\n",
       "  0.18603680059313774,\n",
       "  0.1839045189321041,\n",
       "  0.18396297395229338,\n",
       "  0.1904423177242279,\n",
       "  0.18241689205169678,\n",
       "  0.17799924090504646,\n",
       "  0.1706346243619919,\n",
       "  0.188738926500082,\n",
       "  0.1735748790204525,\n",
       "  0.17641325294971466,\n",
       "  0.17571447491645814,\n",
       "  0.1764240197837353,\n",
       "  0.17422982081770896,\n",
       "  0.17086150944232942,\n",
       "  0.18223615735769272,\n",
       "  0.17751489132642745,\n",
       "  0.17537221312522888,\n",
       "  0.17103445157408714,\n",
       "  0.16571400612592696,\n",
       "  0.18119566068053244,\n",
       "  0.17541317716240884,\n",
       "  0.16625803634524344,\n",
       "  0.1762943372130394,\n",
       "  0.16985094249248506,\n",
       "  0.16242429241538048,\n",
       "  0.17436830550432206,\n",
       "  0.18034385815262793,\n",
       "  0.16302260830998422,\n",
       "  0.16498422473669053,\n",
       "  0.16680740490555762,\n",
       "  0.17076286152005196,\n",
       "  0.17134231626987456,\n",
       "  0.1677037350833416,\n",
       "  0.16969619393348695,\n",
       "  0.16970601975917815,\n",
       "  0.1620219349861145,\n",
       "  0.1650797814130783,\n",
       "  0.16997191607952117,\n",
       "  0.1706788994371891,\n",
       "  0.16169257909059526,\n",
       "  0.16916091442108155,\n",
       "  0.1534222297370434,\n",
       "  0.1661547414958477,\n",
       "  0.16251014024019242,\n",
       "  0.16957081854343414,\n",
       "  0.16336255818605422,\n",
       "  0.16247352212667465,\n",
       "  0.16951814517378808,\n",
       "  0.16457468941807746,\n",
       "  0.16787208691239358,\n",
       "  0.169470826536417,\n",
       "  0.16032419204711915,\n",
       "  0.15796066001057624,\n",
       "  0.16304361075162888,\n",
       "  0.1662481941282749,\n",
       "  0.16026639491319655,\n",
       "  0.16304815337061881,\n",
       "  0.15512525364756585,\n",
       "  0.1601490408182144,\n",
       "  0.1655662924051285,\n",
       "  0.15442327708005904,\n",
       "  0.1520724080502987,\n",
       "  0.15500377714633942,\n",
       "  0.15821329727768899,\n",
       "  0.16509381234645842,\n",
       "  0.16653647050261497,\n",
       "  0.1548361860215664,\n",
       "  0.15750211328268052,\n",
       "  0.15722823292016982,\n",
       "  0.15314140692353248,\n",
       "  0.15504020750522612,\n",
       "  0.15770116373896598,\n",
       "  0.15953208282589912,\n",
       "  0.1530196636915207,\n",
       "  0.15967961102724076,\n",
       "  0.15447232276201248,\n",
       "  0.16000344157218932,\n",
       "  0.16460682600736617,\n",
       "  0.17080754563212394,\n",
       "  0.17157484591007233,\n",
       "  0.16162924319505692,\n",
       "  0.15785446316003798,\n",
       "  0.15554877668619155,\n",
       "  0.15200715661048889,\n",
       "  0.15356676802039146,\n",
       "  0.15538457036018372,\n",
       "  0.15565541088581086,\n",
       "  0.14877040088176727,\n",
       "  0.14958079308271408,\n",
       "  0.15247316434979438,\n",
       "  0.1568813607096672,\n",
       "  0.15233157202601433,\n",
       "  0.1665321946144104,\n",
       "  0.16131826043128966,\n",
       "  0.15534656941890718,\n",
       "  0.1582641825079918,\n",
       "  0.15549207404255866,\n",
       "  0.1535196051001549,\n",
       "  0.1499178484082222,\n",
       "  0.15462518334388733,\n",
       "  0.1550741598010063,\n",
       "  0.14741565138101578,\n",
       "  0.15220943838357925,\n",
       "  0.14925444051623343,\n",
       "  0.1530251085758209,\n",
       "  0.15435673743486406,\n",
       "  0.14799462258815765,\n",
       "  0.15300911143422127,\n",
       "  0.15185223519802094,\n",
       "  0.15062568783760072,\n",
       "  0.15589418709278108,\n",
       "  0.15669014528393746,\n",
       "  0.1607419952750206,\n",
       "  0.14576443359255792,\n",
       "  0.15148792341351508,\n",
       "  0.16183848381042482,\n",
       "  0.15106030702590942,\n",
       "  0.15941887199878693,\n",
       "  0.15645220428705214,\n",
       "  0.15621497556567193,\n",
       "  0.15649900659918786,\n",
       "  0.15528019964694978,\n",
       "  0.15928701609373092,\n",
       "  0.14393967054784298,\n",
       "  0.15463877394795417,\n",
       "  0.1588717669248581,\n",
       "  0.1498977780342102,\n",
       "  0.1518259882926941,\n",
       "  0.1548446498811245,\n",
       "  0.1537522241473198,\n",
       "  0.15573244616389276,\n",
       "  0.1489703468978405,\n",
       "  0.15284933373332024,\n",
       "  0.14789451584219931,\n",
       "  0.14323151260614395,\n",
       "  0.15509339645504952],\n",
       " 'sparse_categorical_accuracy': [0.0475,\n",
       "  0.2675,\n",
       "  0.3675,\n",
       "  0.3775,\n",
       "  0.4575,\n",
       "  0.4425,\n",
       "  0.4525,\n",
       "  0.46,\n",
       "  0.4725,\n",
       "  0.47,\n",
       "  0.4775,\n",
       "  0.445,\n",
       "  0.4725,\n",
       "  0.485,\n",
       "  0.4775,\n",
       "  0.4725,\n",
       "  0.4775,\n",
       "  0.4825,\n",
       "  0.485,\n",
       "  0.4875,\n",
       "  0.4875,\n",
       "  0.4925,\n",
       "  0.48,\n",
       "  0.4875,\n",
       "  0.48,\n",
       "  0.485,\n",
       "  0.485,\n",
       "  0.47,\n",
       "  0.4925,\n",
       "  0.505,\n",
       "  0.51,\n",
       "  0.5,\n",
       "  0.495,\n",
       "  0.505,\n",
       "  0.5,\n",
       "  0.495,\n",
       "  0.51,\n",
       "  0.495,\n",
       "  0.485,\n",
       "  0.495,\n",
       "  0.5025,\n",
       "  0.5025,\n",
       "  0.4975,\n",
       "  0.5,\n",
       "  0.52,\n",
       "  0.5025,\n",
       "  0.4875,\n",
       "  0.48,\n",
       "  0.5025,\n",
       "  0.5025,\n",
       "  0.5075,\n",
       "  0.5175,\n",
       "  0.5025,\n",
       "  0.5125,\n",
       "  0.5125,\n",
       "  0.5175,\n",
       "  0.5,\n",
       "  0.5125,\n",
       "  0.495,\n",
       "  0.52,\n",
       "  0.53,\n",
       "  0.5125,\n",
       "  0.5125,\n",
       "  0.515,\n",
       "  0.4925,\n",
       "  0.54,\n",
       "  0.51,\n",
       "  0.505,\n",
       "  0.52,\n",
       "  0.5175,\n",
       "  0.53,\n",
       "  0.5125,\n",
       "  0.56,\n",
       "  0.535,\n",
       "  0.5375,\n",
       "  0.5425,\n",
       "  0.545,\n",
       "  0.535,\n",
       "  0.52,\n",
       "  0.5175,\n",
       "  0.5475,\n",
       "  0.5425,\n",
       "  0.5625,\n",
       "  0.5225,\n",
       "  0.5275,\n",
       "  0.5575,\n",
       "  0.54,\n",
       "  0.5525,\n",
       "  0.56,\n",
       "  0.535,\n",
       "  0.5225,\n",
       "  0.5575,\n",
       "  0.5325,\n",
       "  0.54,\n",
       "  0.5775,\n",
       "  0.5275,\n",
       "  0.5525,\n",
       "  0.5325,\n",
       "  0.5375,\n",
       "  0.5475,\n",
       "  0.55,\n",
       "  0.535,\n",
       "  0.5475,\n",
       "  0.5725,\n",
       "  0.555,\n",
       "  0.575,\n",
       "  0.545,\n",
       "  0.5675,\n",
       "  0.5575,\n",
       "  0.57,\n",
       "  0.5425,\n",
       "  0.525,\n",
       "  0.55,\n",
       "  0.5475,\n",
       "  0.545,\n",
       "  0.5675,\n",
       "  0.575,\n",
       "  0.565,\n",
       "  0.535,\n",
       "  0.57,\n",
       "  0.56,\n",
       "  0.5575,\n",
       "  0.56,\n",
       "  0.545,\n",
       "  0.5575,\n",
       "  0.5825,\n",
       "  0.57,\n",
       "  0.5475,\n",
       "  0.5575,\n",
       "  0.53,\n",
       "  0.5825,\n",
       "  0.575,\n",
       "  0.585,\n",
       "  0.5675,\n",
       "  0.57,\n",
       "  0.5825,\n",
       "  0.5525,\n",
       "  0.5825,\n",
       "  0.5575,\n",
       "  0.555,\n",
       "  0.5575,\n",
       "  0.5575,\n",
       "  0.5525,\n",
       "  0.535,\n",
       "  0.575,\n",
       "  0.565,\n",
       "  0.5725,\n",
       "  0.58,\n",
       "  0.5775,\n",
       "  0.565,\n",
       "  0.575,\n",
       "  0.5925,\n",
       "  0.575,\n",
       "  0.585,\n",
       "  0.5525,\n",
       "  0.5925,\n",
       "  0.545,\n",
       "  0.555,\n",
       "  0.575,\n",
       "  0.59,\n",
       "  0.595,\n",
       "  0.5675,\n",
       "  0.565,\n",
       "  0.5725,\n",
       "  0.565,\n",
       "  0.5825,\n",
       "  0.5725,\n",
       "  0.575,\n",
       "  0.5625,\n",
       "  0.5775,\n",
       "  0.59,\n",
       "  0.57,\n",
       "  0.59,\n",
       "  0.56,\n",
       "  0.5625,\n",
       "  0.5875,\n",
       "  0.5525,\n",
       "  0.595,\n",
       "  0.57,\n",
       "  0.55,\n",
       "  0.5975,\n",
       "  0.5775,\n",
       "  0.58,\n",
       "  0.575,\n",
       "  0.575,\n",
       "  0.565,\n",
       "  0.5675,\n",
       "  0.585,\n",
       "  0.5675,\n",
       "  0.54,\n",
       "  0.555,\n",
       "  0.585,\n",
       "  0.5625,\n",
       "  0.585,\n",
       "  0.5575,\n",
       "  0.5925,\n",
       "  0.5875,\n",
       "  0.6075,\n",
       "  0.615,\n",
       "  0.59],\n",
       " 'val_loss': [1.1751927733421326,\n",
       "  1.0923710465431213,\n",
       "  0.924738347530365,\n",
       "  0.5613811910152435,\n",
       "  0.6478409767150879,\n",
       "  0.6369260251522064,\n",
       "  0.6212317645549774,\n",
       "  0.6052037477493286,\n",
       "  0.5796709656715393,\n",
       "  0.598143607378006,\n",
       "  0.598272055387497,\n",
       "  0.6102160811424255,\n",
       "  0.5947391092777252,\n",
       "  0.6069948971271515,\n",
       "  0.6049732565879822,\n",
       "  0.6475706398487091,\n",
       "  0.66119185090065,\n",
       "  0.6659278273582458,\n",
       "  0.626566618680954,\n",
       "  0.6644061207771301,\n",
       "  0.6681255996227264,\n",
       "  0.725307285785675,\n",
       "  0.7307164072990417,\n",
       "  0.7377225160598755,\n",
       "  0.784554123878479,\n",
       "  0.8083875477313995,\n",
       "  0.8553726971149445,\n",
       "  0.8916190266609192,\n",
       "  0.9029423594474792,\n",
       "  0.9559751451015472,\n",
       "  0.8866543769836426,\n",
       "  0.8536485433578491,\n",
       "  0.9008293449878693,\n",
       "  0.9499433040618896,\n",
       "  0.9436938166618347,\n",
       "  0.9137219190597534,\n",
       "  0.8506151437759399,\n",
       "  0.932002454996109,\n",
       "  1.0132055580615997,\n",
       "  0.9872520864009857,\n",
       "  0.9542103707790375,\n",
       "  0.9761273264884949,\n",
       "  0.9960304498672485,\n",
       "  0.9764106571674347,\n",
       "  0.9447011649608612,\n",
       "  0.9599927663803101,\n",
       "  0.9984265863895416,\n",
       "  1.0450725555419922,\n",
       "  1.0494959652423859,\n",
       "  1.0561969876289368,\n",
       "  1.0550028681755066,\n",
       "  1.0568665564060211,\n",
       "  1.0801230072975159,\n",
       "  1.0982959270477295,\n",
       "  1.0634609162807465,\n",
       "  1.0829410254955292,\n",
       "  1.1139199137687683,\n",
       "  1.1018112301826477,\n",
       "  1.1002318859100342,\n",
       "  1.0940874814987183,\n",
       "  1.0806441903114319,\n",
       "  1.0770720541477203,\n",
       "  1.0890518426895142,\n",
       "  1.0725902318954468,\n",
       "  1.0231095850467682,\n",
       "  1.0301134288311005,\n",
       "  1.0279966592788696,\n",
       "  1.0784042477607727,\n",
       "  1.1113282442092896,\n",
       "  1.0985432863235474,\n",
       "  1.1002202033996582,\n",
       "  1.1022379994392395,\n",
       "  1.0975406765937805,\n",
       "  1.109641432762146,\n",
       "  1.0985335111618042,\n",
       "  1.1194077134132385,\n",
       "  1.090556561946869,\n",
       "  1.1072397828102112,\n",
       "  1.1096312403678894,\n",
       "  1.0725262463092804,\n",
       "  1.0377002954483032,\n",
       "  1.0819164216518402,\n",
       "  1.089034914970398,\n",
       "  1.102186679840088,\n",
       "  1.0941106677055359,\n",
       "  1.0626495480537415,\n",
       "  1.041825920343399,\n",
       "  1.0447919070720673,\n",
       "  1.0506280660629272,\n",
       "  1.0864560008049011,\n",
       "  1.088842749595642,\n",
       "  1.0779728293418884,\n",
       "  1.0999499559402466,\n",
       "  1.1124125123023987,\n",
       "  1.1137878894805908,\n",
       "  1.1108372807502747,\n",
       "  1.1134528517723083,\n",
       "  1.1381273865699768,\n",
       "  1.0981448888778687,\n",
       "  1.0553657412528992,\n",
       "  1.1084933280944824,\n",
       "  1.1358639597892761,\n",
       "  1.1005576848983765,\n",
       "  1.0818028450012207,\n",
       "  1.0944992899894714,\n",
       "  1.1068042516708374,\n",
       "  1.0914689898490906,\n",
       "  1.062261700630188,\n",
       "  1.0848917365074158,\n",
       "  1.1068952679634094,\n",
       "  1.1319363713264465,\n",
       "  1.1064897179603577,\n",
       "  1.1145901679992676,\n",
       "  1.1260727643966675,\n",
       "  1.132142961025238,\n",
       "  1.1149145364761353,\n",
       "  1.0749905109405518,\n",
       "  1.0426096022129059,\n",
       "  1.0635299384593964,\n",
       "  1.110097348690033,\n",
       "  1.1088125705718994,\n",
       "  1.1039037704467773,\n",
       "  1.1323145627975464,\n",
       "  1.1288849711418152,\n",
       "  1.1329034566879272,\n",
       "  1.110427439212799,\n",
       "  1.0992626547813416,\n",
       "  1.1263830065727234,\n",
       "  1.132886290550232,\n",
       "  1.124579906463623,\n",
       "  1.1188082098960876,\n",
       "  1.1324440240859985,\n",
       "  1.1202185153961182,\n",
       "  1.110105812549591,\n",
       "  1.1055733561515808,\n",
       "  1.1200470328330994,\n",
       "  1.1202861070632935,\n",
       "  1.116456687450409,\n",
       "  1.117877721786499,\n",
       "  1.1296711564064026,\n",
       "  1.117514967918396,\n",
       "  1.1100359559059143,\n",
       "  1.1128286123275757,\n",
       "  1.1231303215026855,\n",
       "  1.1360870599746704,\n",
       "  1.1403877139091492,\n",
       "  1.1353988647460938,\n",
       "  1.1203407049179077,\n",
       "  1.1205145716667175,\n",
       "  1.1218460202217102,\n",
       "  1.1298682689666748,\n",
       "  1.126688539981842,\n",
       "  1.1274994015693665,\n",
       "  1.1461530923843384,\n",
       "  1.1565083265304565,\n",
       "  1.131791353225708,\n",
       "  1.1249771118164062,\n",
       "  1.1554102897644043,\n",
       "  1.1586401462554932,\n",
       "  1.145134687423706,\n",
       "  1.1291212439537048,\n",
       "  1.1457723379135132,\n",
       "  1.1499775052070618,\n",
       "  1.146487832069397,\n",
       "  1.1310990452766418,\n",
       "  1.1163713932037354,\n",
       "  1.1095200181007385,\n",
       "  1.1292839646339417,\n",
       "  1.1466258764266968,\n",
       "  1.126227855682373,\n",
       "  1.1026822924613953,\n",
       "  1.1141242384910583,\n",
       "  1.1273208260536194,\n",
       "  1.1428991556167603,\n",
       "  1.138733983039856,\n",
       "  1.1147903203964233,\n",
       "  1.125252902507782,\n",
       "  1.1245139837265015,\n",
       "  1.1471514105796814,\n",
       "  1.1488208174705505,\n",
       "  1.1384382247924805,\n",
       "  1.098278522491455,\n",
       "  1.0966318249702454,\n",
       "  1.1497673988342285,\n",
       "  1.1490576267242432,\n",
       "  1.1496827602386475,\n",
       "  1.1429049968719482,\n",
       "  1.152096450328827,\n",
       "  1.1554028987884521,\n",
       "  1.1551677584648132,\n",
       "  1.1355319619178772,\n",
       "  1.1423404812812805,\n",
       "  1.1624907851219177,\n",
       "  1.1674110293388367,\n",
       "  1.1670573353767395,\n",
       "  1.1569663286209106,\n",
       "  1.1667996644973755,\n",
       "  1.1594215035438538,\n",
       "  1.1441800594329834,\n",
       "  1.1249533891677856],\n",
       " 'val_sparse_categorical_accuracy': [0.15625,\n",
       "  0.27083334,\n",
       "  0.5833333,\n",
       "  0.59375,\n",
       "  0.59375,\n",
       "  0.59375,\n",
       "  0.59375,\n",
       "  0.5625,\n",
       "  0.5625,\n",
       "  0.5208333,\n",
       "  0.41666666,\n",
       "  0.38541666,\n",
       "  0.34375,\n",
       "  0.32291666,\n",
       "  0.29166666,\n",
       "  0.22916667,\n",
       "  0.20833333,\n",
       "  0.23958333,\n",
       "  0.32291666,\n",
       "  0.30208334,\n",
       "  0.25,\n",
       "  0.23958333,\n",
       "  0.26041666,\n",
       "  0.28125,\n",
       "  0.32291666,\n",
       "  0.29166666,\n",
       "  0.30208334,\n",
       "  0.30208334,\n",
       "  0.30208334,\n",
       "  0.30208334,\n",
       "  0.3125,\n",
       "  0.3125,\n",
       "  0.30208334,\n",
       "  0.30208334,\n",
       "  0.30208334,\n",
       "  0.30208334,\n",
       "  0.29166666,\n",
       "  0.29166666,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.29166666,\n",
       "  0.29166666,\n",
       "  0.29166666,\n",
       "  0.29166666,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.26041666,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.28125,\n",
       "  0.28125,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.26041666,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334,\n",
       "  0.27083334]}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "favorite_color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6174,
     "status": "ok",
     "timestamp": 1584133947565,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "UA3-6HawzXN8",
    "outputId": "92af61bc-0b2c-407f-b4db-a14e18c40ed5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context output shape (None, None, 160)\n"
     ]
    }
   ],
   "source": [
    "inference_answer_model,inference_encoder,inference_decoder,inference_feasibility_model = create_models(embedding_matrix,\n",
    "                                                                                      num_unit_gru = 80,\n",
    "                                                                                      num_layers_gru = 2,\n",
    "                                                                                      ndim =100,\n",
    "                                                                                      num_episodes = 2,\n",
    "                                                                                      num_dense_layer_feasibility_units = 32,\n",
    "                                                                                      dropout_rate = 0.5,\n",
    "                                                                                      num_dense_layers_feasibility = 2,\n",
    "                                                                                      attentionType = 0, # 0 means Luong's 1 means BahdanauUnits\n",
    "                                                                                      BahdanauUnits = 64)\n",
    "\n",
    "inference_encoder.load_weights('tpu_encoder_model_2.h5')\n",
    "inference_decoder.load_weights('tpu_decoder_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4486,
     "status": "ok",
     "timestamp": 1584134255349,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "o_L0YvbuuMb-",
    "outputId": "2a1fc282-9e73-4137-ca7c-da9f47af25d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: in 2003 what well known u s secretary of state declared the situation in darfur as an act of genocide\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> colin powell </s>\n",
      "question: what shape was the sixaxis final model in\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what is a team called that is in the process of joining a league\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> a probational franchise </s>\n",
      "question: what is lord justice sedley s nationality\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what do field windings provide\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> flux </s>\n",
      "question: what period ranged from the 41st to the 29th century bc\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what would be needed to support metadata and obviate the need for tags\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> a standard container format </s>\n",
      "question: how much energy could a capacitor in a disposable camera release\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: where is raleigh durham international airport\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> interstate 40 between raleigh and durham </s>\n",
      "question: what does the one drop rule do\n",
      "Decoded sentence: </s> \n",
      "Actual answer: <s> discrimination against people who are not visibly european in ancestry </s>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(context_input_seq,question_input_seq,gpu_encoder_model,gpu_decoder_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 320,
     "status": "ok",
     "timestamp": 1584134222210,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "mhYNd8lEwGCf",
    "outputId": "0d657f8d-f745-4696-e6f2-44e54dc3b90c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> mesrop mashtots </s>'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[0].tolist() if i !=0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4842,
     "status": "ok",
     "timestamp": 1584122895488,
     "user": {
      "displayName": "Anup Jha",
      "photoUrl": "",
      "userId": "03117321996358762005"
     },
     "user_tz": 420
    },
    "id": "15sHa4lo5x9G",
    "outputId": "de0c7e92-fce0-4f3c-b8f9-9516d61ae237"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n",
      "-\n",
      "Decoded sentence: </s> \n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index+3000: seq_index +3000+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index+3000: seq_index+3000 + 1]\n",
    "    decoded_sentence = decode_sequence(context_input_seq,question_input_seq,inference_encoder,inference_decoder)\n",
    "    print('-')\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Experiment0': {'num_unit_gru': 64, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 16, 'dropout_rate': 0.4, 'num_dense_layers_feasibility': 1, 'attentionType': 0, 'BahdanauUnits': 32, 'learning_rate': 0.005}, 'Experiment1': {'num_unit_gru': 80, 'num_layers_gru': 1, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.6, 'num_dense_layers_feasibility': 2, 'attentionType': 0, 'BahdanauUnits': 32, 'learning_rate': 0.001}, 'Experiment2': {'num_unit_gru': 32, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 16, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 1, 'attentionType': 0, 'BahdanauUnits': 64, 'learning_rate': 0.005}, 'Experiment3': {'num_unit_gru': 100, 'num_layers_gru': 2, 'num_episodes': 1, 'num_dense_layer_feasibility_units': 16, 'dropout_rate': 0.4, 'num_dense_layers_feasibility': 1, 'attentionType': 1, 'BahdanauUnits': 32, 'learning_rate': 0.001}, 'Experiment4': {'num_unit_gru': 32, 'num_layers_gru': 1, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 16, 'dropout_rate': 0.4, 'num_dense_layers_feasibility': 2, 'attentionType': 0, 'BahdanauUnits': 64, 'learning_rate': 0.005}, 'Experiment5': {'num_unit_gru': 80, 'num_layers_gru': 1, 'num_episodes': 1, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 1, 'attentionType': 1, 'BahdanauUnits': 64, 'learning_rate': 0.005}}\n"
     ]
    }
   ],
   "source": [
    "#Define Experiments using random choice\n",
    "import random\n",
    "num_unit_gru_list = [32,64,80,100]\n",
    "num_layers_gru_list = [1,2]\n",
    "num_episodes_list = [1,2,3]\n",
    "num_dense_layer_feasibility_units_list = [16,32]\n",
    "dropout_rate_list = [0.4,0.5,0.6,0.7]\n",
    "num_dense_layers_feasibility_list = [1,2]\n",
    "attentionType_list = [0,1]\n",
    "BahdanauUnits_list = [32,64]\n",
    "learning_rate_list = [.005,.001]\n",
    "Experiments = {}\n",
    "for i in range(6):\n",
    "    experiment_name = 'Experiment'+str(i)\n",
    "    experiment_hyperparam_dic = {}\n",
    "    experiment_num_unit_gru = random.choice(num_unit_gru_list)\n",
    "    experiment_num_layers_gru = random.choice(num_layers_gru_list)\n",
    "    experiment_num_episodes = random.choice(num_episodes_list)\n",
    "    experiment_num_dense_layer_feasibility_units = random.choice(num_dense_layer_feasibility_units_list)\n",
    "    experiment_dropout_rate = random.choice(dropout_rate_list)\n",
    "    experiment_num_dense_layers_feasibility = random.choice(num_dense_layers_feasibility_list)\n",
    "    experiment_attentionType = random.choice(attentionType_list)\n",
    "    experiment_BahdanauUnits = random.choice(BahdanauUnits_list)\n",
    "    experiment_learning_rate = random.choice(learning_rate_list)\n",
    "    experiment_hyperparam_dic['num_unit_gru'] = experiment_num_unit_gru\n",
    "    experiment_hyperparam_dic['num_layers_gru'] = experiment_num_layers_gru\n",
    "    experiment_hyperparam_dic['num_episodes'] = experiment_num_episodes\n",
    "    experiment_hyperparam_dic['num_dense_layer_feasibility_units'] = experiment_num_dense_layer_feasibility_units\n",
    "    experiment_hyperparam_dic['dropout_rate'] = experiment_dropout_rate\n",
    "    experiment_hyperparam_dic['num_dense_layers_feasibility'] = experiment_num_dense_layers_feasibility\n",
    "    experiment_hyperparam_dic['attentionType'] = experiment_attentionType\n",
    "    experiment_hyperparam_dic['BahdanauUnits'] = experiment_BahdanauUnits\n",
    "    experiment_hyperparam_dic['learning_rate'] = experiment_learning_rate\n",
    "    Experiments[experiment_name] = experiment_hyperparam_dic\n",
    "    \n",
    "print(Experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experiments_Dic = {'Experiment0': {'num_unit_gru': 80,\n",
    "  'num_layers_gru': 1,\n",
    "  'num_episodes': 2,\n",
    "  'num_dense_layer_feasibility_units': 32,\n",
    "  'dropout_rate': 0.5,\n",
    "  'num_dense_layers_feasibility': 1,\n",
    "  'attentionType': 0,\n",
    "  'BahdanauUnits': 64,\n",
    "  'learning_rate': 0.001},\n",
    " 'Experiment1': {'num_unit_gru': 80,\n",
    "  'num_layers_gru': 1,\n",
    "  'num_episodes': 2,\n",
    "  'num_dense_layer_feasibility_units': 16,\n",
    "  'dropout_rate': 0.4,\n",
    "  'num_dense_layers_feasibility': 1,\n",
    "  'attentionType': 0,\n",
    "  'BahdanauUnits': 64,\n",
    "  'learning_rate': 0.005},\n",
    " 'Experiment2': {'num_unit_gru': 80,\n",
    "  'num_layers_gru': 2,\n",
    "  'num_episodes': 3,\n",
    "  'num_dense_layer_feasibility_units': 16,\n",
    "  'dropout_rate': 0.7,\n",
    "  'num_dense_layers_feasibility': 1,\n",
    "  'attentionType': 1,\n",
    "  'BahdanauUnits': 64,\n",
    "  'learning_rate': 0.005},\n",
    " 'Experiment3': {'num_unit_gru': 64,\n",
    "  'num_layers_gru': 2,\n",
    "  'num_episodes': 2,\n",
    "  'num_dense_layer_feasibility_units': 32,\n",
    "  'dropout_rate': 0.6,\n",
    "  'num_dense_layers_feasibility': 1,\n",
    "  'attentionType': 1,\n",
    "  'BahdanauUnits': 64,\n",
    "  'learning_rate': 0.005},\n",
    " 'Experiment4': {'num_unit_gru': 64,\n",
    "  'num_layers_gru': 2,\n",
    "  'num_episodes': 2,\n",
    "  'num_dense_layer_feasibility_units': 32,\n",
    "  'dropout_rate': 0.5,\n",
    "  'num_dense_layers_feasibility': 1,\n",
    "  'attentionType': 0,\n",
    "  'BahdanauUnits': 64,\n",
    "  'learning_rate': 0.001},\n",
    " 'Experiment5': {'num_unit_gru': 64,\n",
    "  'num_layers_gru': 2,\n",
    "  'num_episodes': 2,\n",
    "  'num_dense_layer_feasibility_units': 32,\n",
    "  'dropout_rate': 0.4,\n",
    "  'num_dense_layers_feasibility': 2,\n",
    "  'attentionType': 1,\n",
    "  'BahdanauUnits': 32,\n",
    "  'learning_rate': 0.005}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AJ1Hs43oiWba"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d3oe0zv0iRb0"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNBA4CfeT1duxjpcDFy4gDs",
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "W266_NLP_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
