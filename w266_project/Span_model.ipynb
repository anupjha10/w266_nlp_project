{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: tqdm in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "    \"\"\"\n",
    "      Load the data to a pandas dataframe.\n",
    "      Dataframe Columns:\n",
    "      title\n",
    "      para_index\n",
    "      context\n",
    "      q_index\n",
    "      q_id\n",
    "      q_isimpossible\n",
    "      q_question\n",
    "      q_anscount - number of answers\n",
    "      q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for pdata in data['data']:\n",
    "        for para in pdata['paragraphs']:\n",
    "            for q in para['qas']:\n",
    "                result.append({\n",
    "                                'title' : pdata['title'],\n",
    "                                'context' : para['context'],\n",
    "                                'q_id' : q['id'],\n",
    "                                'q_isimpossible' : q['is_impossible'],\n",
    "                                'q_question' : q['question'],\n",
    "                                'q_anscount' : len(q['answers']),\n",
    "                                'q_answers' : [a for a in q['answers']],\n",
    "                                'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "                                'context_lowercase': para['context'].lower(),\n",
    "                                'q_question_lowercase' : q['question'].lower(),\n",
    "                                'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']]\n",
    "                               })\n",
    "    return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "    \"\"\"\n",
    "    Download a file from google drive with the shared link\n",
    "    \"\"\" \n",
    "    data = load_json_file(filename)\n",
    "    return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 250\n",
    "question_maxlen = 20\n",
    "answer_maxlen = 15\n",
    "ndim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "            if i <= num_words:\n",
    "                vocab[word] = i\n",
    "        else:\n",
    "            vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def calc_answer_span(context,answer):\n",
    "    \n",
    "    ans_len = len(answer)\n",
    "    \n",
    "    if ans_len!=0 and answer[0] in context:\n",
    "        indices = [i for i, x in enumerate(context) if x == answer[0]]\n",
    "        try:\n",
    "            if(len(indices)>1):\n",
    "                start = [i for i in indices if (context[i:i+ans_len] == answer) ]\n",
    "                end = start[0] + ans_len - 1\n",
    "                return (start[0],end)\n",
    "            else:\n",
    "                start = context.index(answer[0])\n",
    "                end = start + ans_len - 1\n",
    "                return (start,end)\n",
    "        except:\n",
    "            return (-1,-1)\n",
    "    else:\n",
    "        return (-1,-1)\n",
    "\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    qid_original = train_lists[0]\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_span = [calc_answer_span(context_seq[i],answer_seq[i]) for i,x in enumerate(context_seq)]\n",
    "    answer_start_index = [item[0] for item in answer_span]\n",
    "    answer_end_index =  [item[1] for item in answer_span]\n",
    "    answer_start_seq = []\n",
    "    answer_end_seq = []\n",
    "    for i,x in enumerate(answer_start_index):\n",
    "        start = np.zeros(context_maxlen,dtype = \"int32\")\n",
    "        end   = np.zeros(context_maxlen,dtype = \"int32\")\n",
    "        #last space reserved for the question where there are no answers\n",
    "        if (answer_start_index[i] < context_maxlen-1):\n",
    "            start[answer_start_index[i]] = 1\n",
    "        if (answer_end_index[i] < context_maxlen-1):\n",
    "            end[answer_end_index[i]] = 1\n",
    "        answer_start_seq.append(start)\n",
    "        answer_end_seq.append(end)\n",
    "        \n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen-1,padding='post', truncating='post')\n",
    "    #Adding 0 to last position special for no answer questions\n",
    "    context_seq_padded = pad_sequences(context_seq_padded,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    answer_start_seq_padded = pad_sequences(answer_start_seq,context_maxlen,padding='post', truncating='post') \n",
    "    answer_end_seq_padded = pad_sequences(answer_end_seq,context_maxlen,padding='post', truncating='post')\n",
    "    #context_match_question = []\n",
    "    #for i,a in enumerate(context_seq_padded):\n",
    "    #    exact = [[1]if ecw in question_seq_padded[i] and ecw !=0 else [0] for ecw in a]\n",
    "    #    context_match_question.append(exact)\n",
    "    \n",
    "    #context_match_question_padded = np.array(context_match_question)\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    qid = [qid_original[i] for i in indices]\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible = answer_impossible[indices]\n",
    "    answer_start_seq_padded = answer_start_seq_padded[indices]\n",
    "    answer_end_seq_padded = answer_end_seq_padded[indices]\n",
    "    #context_match_question_padded = context_match_question_padded[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_qid = qid[:train_samples]\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible[:train_samples]\n",
    "    train_answer_start_seq_padded = answer_start_seq_padded[:train_samples]\n",
    "    train_answer_end_seq_padded = answer_end_seq_padded[:train_samples]\n",
    "    #train_context_match_question_padded = context_match_question_padded[:train_samples]\n",
    "    val_qid = qid[train_samples:end_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible[train_samples:end_samples]\n",
    "    val_answer_start_seq_padded = answer_start_seq_padded[train_samples:end_samples]\n",
    "    val_answer_end_seq_padded = answer_end_seq_padded[train_samples:end_samples]\n",
    "    #val_context_match_question_padded = context_match_question_padded[train_samples:end_samples]\n",
    "    return (train_qid,train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            train_answer_start_seq_padded,train_answer_end_seq_padded,\n",
    "            #train_context_match_question_padded,\n",
    "            val_qid,val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible,\n",
    "            val_answer_start_seq_padded,val_answer_end_seq_padded\n",
    "            #,val_context_match_question_padded\n",
    "           )\n",
    "\n",
    "train_qid,train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\\\n",
    "train_answer_start_seq_padded,train_answer_end_seq_padded,\\\n",
    "val_qid,val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible,\\\n",
    "val_answer_start_seq_padded,val_answer_end_seq_padded\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88702, 100)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Model with attention in every step of the answer decoder\n",
    "class BahdanauAttention_model2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention_model2, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Answer Module which is custom as we need to feed output of each time sequence with attention to next \n",
    "# time sequence\n",
    "class answer_module(tf.keras.Model):\n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      ndim,\n",
    "                      num_unit_gru,\n",
    "                      num_layers_gru,\n",
    "                      dropout_rate,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(answer_module, self).__init__()\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.WSTART = tf.keras.layers.Dense(2*self.num_unit_gru)\n",
    "        self.WEND = tf.keras.layers.Dense(2*self.num_unit_gru)        \n",
    "    \n",
    "    def call(self,\n",
    "             question,\n",
    "             context):\n",
    "        \n",
    "        context = tf.transpose(context,[0,2,1])\n",
    "        ################ start prediction logit ######################\n",
    "        start = self.WSTART(question)\n",
    "        hidden_start_time_axis = tf.expand_dims(start, 1)\n",
    "        \n",
    "        start_logit = tf.squeeze(tf.matmul(hidden_start_time_axis,context),axis=1)\n",
    "        start_logit = tf.math.exp(start_logit)\n",
    "            \n",
    "        ################ end prediction logit ######################\n",
    "        end = self.WEND(question)\n",
    "\n",
    "        hidden_end_time_axis = tf.expand_dims(end, 1)\n",
    "        \n",
    "        # squeeze remooves time slice we added before\n",
    "        # final shape = (batch_size,decoder_timesteps)\n",
    "        end_logit = tf.squeeze(tf.matmul(hidden_end_time_axis,context),axis=1)\n",
    "        end_logit = tf.math.exp(end_logit)\n",
    "        \n",
    "        \n",
    "        return start_logit,end_logit \n",
    "\n",
    "#Encoder Module which combines the context,question into episodic memory and emits context outputs and \n",
    "#question outputs\n",
    "class encoder_module(tf.keras.Model):    \n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      max_context_length,\n",
    "                      max_question_length,\n",
    "                      max_answer_length,\n",
    "                      num_unit_gru = 64,\n",
    "                      num_layers_gru = 2,\n",
    "                      ndim =100,\n",
    "                      num_episodes = 2,\n",
    "                      dropout_rate = 0.5,\n",
    "                      num_episodic_network_unit = 64,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(encoder_module, self).__init__()\n",
    "        #Context Module\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers_gru = num_layers_gru\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ndim = ndim\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_question_length = max_question_length\n",
    "        self.max_answer_length = max_answer_length\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_episodic_network_unit = num_episodic_network_unit\n",
    "        self.context_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                         self.ndim,\n",
    "                                                         mask_zero=True,\n",
    "                                                         weights =[self.embedding_matrix],\n",
    "                                                         trainable = False,\n",
    "                                                         name='Context_Embedding')\n",
    "        self.context_output_layers = []\n",
    "        self.context_batch_normalization_layers = []\n",
    "        for i in range(self.num_layers_gru):\n",
    "            self.context_output_layers.append(layers.Bidirectional(\n",
    "                                                layers.GRU(self.num_unit_gru,\n",
    "                                                           dropout=self.dropout_rate,\n",
    "                                                           recurrent_dropout= self.dropout_rate,\n",
    "                                                           recurrent_initializer='glorot_uniform',\n",
    "                                                           return_sequences=True,\n",
    "                                                           kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                              l1_regularizer_weight,\n",
    "                                                                              l2_regularizer_weight),\n",
    "                                                           bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                              l1_regularizer_weight,\n",
    "                                                                              l2_regularizer_weight)),\n",
    "                                                           merge_mode='concat',\n",
    "                                                           name='Context_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.context_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        #Question Module\n",
    "        self.question_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                          self.ndim,\n",
    "                                                          mask_zero=True,\n",
    "                                                          weights =[self.embedding_matrix],\n",
    "                                                          trainable = False,\n",
    "                                                          name='Question_Embedding')\n",
    "          \n",
    "        self.question_output_layers = []\n",
    "        self.question_batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "            self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                 layers.GRU(self.num_unit_gru,\n",
    "                                                            dropout=self.dropout_rate,\n",
    "                                                            recurrent_dropout= self.dropout_rate,\n",
    "                                                            recurrent_initializer='glorot_uniform',\n",
    "                                                            return_sequences=True,\n",
    "                                                            kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                l1_regularizer_weight,\n",
    "                                                                                l2_regularizer_weight),\n",
    "                                                            bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                l1_regularizer_weight,\n",
    "                                                                                l2_regularizer_weight)),\n",
    "                                                             merge_mode='concat',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.question_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "            \n",
    "        self.question_attention_layer = layers.Dense(1)\n",
    "        #Episodic Memory \n",
    "        #self.episodic_weight_layer = layers.Dense(self.num_unit_gru,use_bias=False)\n",
    "        #self.episodic_tanh_layer = layers.Dense(self.num_episodic_network_unit,activation='tanh')\n",
    "        #self.episodic_score_layer = layers.Dense(1)\n",
    "        #Self alignment\n",
    "        self.Align_W1 = tf.keras.layers.Dense(1,activation=\"relu\")\n",
    "        self.Align_W2 = tf.keras.layers.Dense(1,activation=\"relu\")\n",
    "    def call(self,context_input,question_input):\n",
    "        #context Module\n",
    "        context_embeddings = self.context_embeddings_layer(context_input)\n",
    "        #Question Module\n",
    "        question_embeddings = self.question_embeddings_layer(question_input)\n",
    "        #self alignment\n",
    "        context_relu = self.Align_W1(context_embeddings)\n",
    "        question_relu = self.Align_W2(question_embeddings)\n",
    "        self_align_logits = tf.matmul(context_relu,tf.transpose(question_relu,[0,2,1]))\n",
    "        self_align_scores = tf.nn.softmax(self_align_logits)\n",
    "        self_align_embeddings = tf.matmul(self_align_scores,question_embeddings)\n",
    "        context_concat_embeddings = tf.concat(values=[context_embeddings,self_align_embeddings],axis=-1)\n",
    "        \n",
    "        #print(\"context_embeddings.shape:\",context_embeddings.shape)\n",
    "        #print(\"question_embeddings.shape:\",question_embeddings.shape)\n",
    "        #print(\"context_relu.shape:\",context_relu.shape)\n",
    "        #print(\"question_relu.shape:\",question_relu.shape)\n",
    "        #print(\"self_align_logits.shape:\",self_align_logits.shape)\n",
    "        #print(\"self_align_scores.shape:\",self_align_scores.shape)\n",
    "        #print(\"self_align_embeddings.shape:\",self_align_embeddings.shape)\n",
    "        #print(\"context_concat_embeddings.shape:\",context_concat_embeddings.shape)\n",
    "        for i in range(len(self.context_output_layers)):\n",
    "            if i==0:\n",
    "                context_outputs = self.context_output_layers[i](context_concat_embeddings)\n",
    "            else:\n",
    "                context_outputs = self.context_output_layers[i](context_outputs)\n",
    "            context_outputs = self.context_batch_normalization_layers[i](context_outputs)\n",
    "\n",
    "        \n",
    "        for i in range(len(self.question_output_layers)):\n",
    "            if i==0:\n",
    "                question_outputs = self.question_output_layers[i](question_embeddings)\n",
    "            else:\n",
    "                question_outputs = self.question_output_layers[i](question_outputs)\n",
    "            question_outputs = self.question_batch_normalization_layers[i](question_outputs) \n",
    "        \n",
    "        #Calculate the self attention for question        \n",
    "        question_score = self.question_attention_layer(question_outputs)\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        question_attention_weights = tf.nn.softmax(question_score, axis=1)\n",
    "        question_outputs = question_attention_weights * question_outputs\n",
    "        question_outputs = tf.reduce_sum(question_outputs, axis=1)\n",
    "        \n",
    "        #Episodic Memory \n",
    "        #m = tf.identity(question_outputs)\n",
    "        #for i in range(self.num_episodes):\n",
    "        #    m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "        #                          tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "        #    q_increased = tf.tile(tf.keras.backend.expand_dims(question_outputs,1),\n",
    "        #                          tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "        #    c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "        #    c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "        #    c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "        #    c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "        #    c_dot_q = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(question_outputs),1), \n",
    "        #                        context_outputs,\n",
    "        #                        transpose_b=True)\n",
    "        #    c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "        #    c_dot_m = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(m),1), \n",
    "        #                        context_outputs,transpose_b=True)\n",
    "        #    c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "        #    z = tf.concat([context_outputs,\n",
    "        #                            m_increased,\n",
    "        #                            q_increased,\n",
    "        #                            c_mul_q,\n",
    "        #                            c_mul_m,\n",
    "        #                            c_minus_q,\n",
    "        #                            c_minus_m,\n",
    "        #                            c_dot_q,\n",
    "        #                            c_dot_m],axis=-1)\n",
    "        #    score = self.episodic_score_layer(self.episodic_tanh_layer(z))\n",
    "        #    attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        #    m_value = attention_weights * context_outputs\n",
    "        #    m = tf.reduce_sum(m_value, axis=1)\n",
    "        #concatenate episodic memory with question\n",
    "        #concatenated_tensor = tf.concat(values=[m,question_outputs],axis=1)\n",
    "        return (question_outputs,context_outputs)\n",
    "    \n",
    "                \n",
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64,\n",
    "                  l1_regularizer_weight = .01,\n",
    "                  l2_regularizer_weight = .01):\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        weight_layer = layers.Dense(query.shape[1],use_bias=False)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(weight_layer(query),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(weight_layer(m),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    \"\"\"\n",
    "    #encoder Model\n",
    "    encoder_model = encoder_module(embedding_matrix,\n",
    "                                   vocab_size,\n",
    "                                   max_context_length,\n",
    "                                   max_question_length,\n",
    "                                   max_answer_length,\n",
    "                                   num_unit_gru,\n",
    "                                   num_layers_gru,\n",
    "                                   ndim,\n",
    "                                   num_episodes,\n",
    "                                   dropout_rate,\n",
    "                                   num_episodic_network_unit,\n",
    "                                   l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                   l2_regularizer_weight = l2_regularizer_weight\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    #Model([context_input,question_input], [m,concatenated_tensor,question_outputs,context_outputs])\n",
    "    #answer_module\n",
    "    answer_model = answer_module(embedding_matrix,\n",
    "                                 vocab_size,\n",
    "                                 ndim,\n",
    "                                 num_unit_gru,\n",
    "                                 num_layers_gru,\n",
    "                                 dropout_rate,\n",
    "                                 l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                 l2_regularizer_weight = l2_regularizer_weight)\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    #feasibility_episodic_memory_input = Input(shape=(num_unit_gru,), name=\"FeasibilityEpisodicMemoryInput\")\n",
    "    feasibility_start_logits_input = Input(shape=(max_context_length,),name=\"FeasibilityStartLogitInput\")\n",
    "    feasibility_end_logits_input = Input(shape=(max_context_length),name=\"FeasibilityEndLogitInput\")\n",
    "    feasibility_context_input = Input(shape=(None,2*num_unit_gru,),name='feasibilityContext_Input')\n",
    "    feasibility_question_input = Input(shape=(2*num_unit_gru,),name='feasibilityQuestion_Input')\n",
    "    #create attention between Context and Question\n",
    "    #q_with_time_axis = tf.keras.backend.expand_dims(feasibility_question_input,1)\n",
    "    #attentionContextQuestion = layers.AdditiveAttention()([q_with_time_axis,\n",
    "    #                                                      feasibility_context_input])\n",
    "    #attentionContextQuestionReduced = tf.keras.backend.sum(attentionContextQuestion, axis=1)\n",
    "    #create Episodic memory \n",
    "    #Episodic Memory \n",
    "    episodic_weight_layer = layers.Dense(2*num_unit_gru,use_bias=False)\n",
    "    episodic_tanh_layer = layers.Dense(num_episodic_network_unit,activation='tanh')\n",
    "    episodic_score_layer = layers.Dense(1)\n",
    "    m = tf.identity(feasibility_question_input)\n",
    "    for i in range(num_episodes):\n",
    "        m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                              tf.constant([1,max_context_length,1],tf.int32))\n",
    "        q_increased = tf.tile(tf.keras.backend.expand_dims(feasibility_question_input,1),\n",
    "                              tf.constant([1,max_context_length,1],tf.int32))\n",
    "        c_mul_q = layers.multiply([feasibility_context_input,q_increased])\n",
    "        c_mul_m = layers.multiply([feasibility_context_input,m_increased])\n",
    "        c_minus_q =tf.keras.backend.abs(layers.subtract([feasibility_context_input,q_increased]))\n",
    "        c_minus_m = tf.keras.backend.abs(layers.subtract([feasibility_context_input,m_increased]))\n",
    "        c_dot_q = tf.matmul(tf.keras.backend.expand_dims(episodic_weight_layer(feasibility_question_input),1), \n",
    "                            feasibility_context_input,\n",
    "                            transpose_b=True)\n",
    "        c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "        c_dot_m = tf.matmul(tf.keras.backend.expand_dims(episodic_weight_layer(m),1), \n",
    "                            feasibility_context_input,transpose_b=True)\n",
    "        c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "        z = tf.concat([feasibility_context_input,\n",
    "                                m_increased,\n",
    "                                q_increased,\n",
    "                                c_mul_q,\n",
    "                                c_mul_m,\n",
    "                                c_minus_q,\n",
    "                                c_minus_m,\n",
    "                                c_dot_q,\n",
    "                                c_dot_m],axis=-1)\n",
    "        score = episodic_score_layer(episodic_tanh_layer(z))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        m_value = attention_weights * feasibility_context_input\n",
    "        m = tf.reduce_sum(m_value, axis=1)\n",
    "    \n",
    "    \n",
    "    feasibility_dense_input = tf.concat([m,\n",
    "                                         feasibility_start_logits_input,\n",
    "                                         feasibility_end_logits_input],\n",
    "                                        axis=-1)\n",
    "    for i in range(num_dense_layers_feasibility):        \n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(feasibility_dense_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',\n",
    "                                      name='feasibility_output',\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                    l1_regularizer_weight,\n",
    "                                                                    l2_regularizer_weight),\n",
    "                                      bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                    l1_regularizer_weight,\n",
    "                                                                    l2_regularizer_weight))(dropout_layer)\n",
    "    feasibility_model = Model([feasibility_question_input,\n",
    "                               feasibility_context_input,\n",
    "                               feasibility_start_logits_input,\n",
    "                               feasibility_end_logits_input],\n",
    "                               feasibility_output)\n",
    "    \n",
    "    return (answer_model,encoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sentence(context_input_seq,\n",
    "                     question_input_seq,\n",
    "                     encoder_model,\n",
    "                     answer_model):\n",
    "    decoded_sentence = \"\"\n",
    "    question_output,context_output = encoder_model(context_input_seq,question_input_seq) \n",
    "    \n",
    "    start_logits,end_logits = answer_model(question_output,context_output)\n",
    "    \n",
    "    #Do a outer matrix multiplication of the logits \n",
    "    # we need to get the start and end index with highest multiplication of start and end probs\n",
    "    outer = tf.matmul(tf.expand_dims(start_logits, axis=2),tf.expand_dims(end_logits, axis=0)) \n",
    "    outer = tf.linalg.band_part(outer, 0, answer_maxlen)\n",
    "    start_position = tf.argmax(tf.reduce_max(outer, axis=2),axis=1)    \n",
    "    end_position = tf.argmax(tf.reduce_max(outer, axis=1),axis=1)\n",
    "    #print(start_position.shape)\n",
    "    #print(end_position.shape)\n",
    "    #print(start_position)\n",
    "    #print(end_position)\n",
    "    \n",
    "    for i in range(start_position[0],end_position[0]+1):\n",
    "        sampled_token_index = context_input_seq[0,i]\n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \"\"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        if i == start_position[0]:\n",
    "            decoded_sentence += sampled_char\n",
    "        else:\n",
    "            decoded_sentence += \" \"+sampled_char\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.0001,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment2': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment4': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 128,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.001},\n",
       " 'Experiment5': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment6': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 1,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 80,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 64,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment1': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.0001,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment2': {'num_unit_gru': 100,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 64,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment3': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment4': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 48,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 128,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.001},\n",
    "                  'Experiment5': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment6': {'num_unit_gru': 64,\n",
    "                                  'num_layers_gru': 1,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 80,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01}\n",
    "                }\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "        \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                      l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                      l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        #encoder_model.compile(optimizer=adam_optim,\n",
    "        #                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        #encoder_model.summary()\n",
    "        #answer_model.compile(optimizer=adam_optim,\n",
    "        #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        #                        )\n",
    "        #\n",
    "        #answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        #feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' +\n",
    "                                                                     os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = \n",
    "                                                        num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                         l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                         l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            #encoder_model.compile(optimizer=adam_optim,\n",
    "            #                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "            #encoder_model.summary()\n",
    "            #\n",
    "            #answer_model.compile(optimizer=adam_optim,\n",
    "            #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            #                    )\n",
    "\n",
    "            #answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            #feasibility_model.summary()\n",
    "    #Train the Answer Model and Encoder Model\n",
    "    answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    answer_loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    train_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.CategoricalAccuracy()\n",
    "    @tf.function\n",
    "    def answer_loss_function(real_start, real_end,pred_start,pred_end):\n",
    "        \n",
    "        start_loss = answer_loss_object(real_start,pred_start)\n",
    "        end_loss = answer_loss_object(real_end,pred_end)\n",
    "        return tf.reduce_mean(start_loss+end_loss)\n",
    "\n",
    "    @tf.function\n",
    "    def answer_train_step(inp,ques,targ_start,targ_end,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        batch_loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            question_output,context_output = encoder_model(inp,ques)\n",
    "            start_logits,end_logits = answer_model(question_output,context_output)\n",
    "            loss += answer_loss_function(targ_start,targ_end,start_logits,end_logits)\n",
    "            batch_loss = loss\n",
    "            train_acc_metric(targ_start,start_logits)\n",
    "            train_acc_metric(targ_end,end_logits)\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def answer_val_step(inp,ques,targ_start,targ_end,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        question_output,context_output = encoder_model(inp,ques)\n",
    "        start_logits,end_logits = answer_model(question_output,context_output)\n",
    "        loss += answer_loss_function(targ_start,targ_end,start_logits,end_logits)\n",
    "        batch_loss = loss\n",
    "        val_acc_metric(targ_start,start_logits)\n",
    "        val_acc_metric(targ_end,end_logits)\n",
    "        \n",
    "        return batch_loss\n",
    "    #Create batches for training\n",
    "    \n",
    "    TRAIN_BUFFER_SIZE = train_context_padded_seq[:num_training_samples].shape[0]\n",
    "    VAL_BUFFER_SIZE = val_context_padded_seq[:num_validation_samples].shape[0]\n",
    "    steps_per_epoch = TRAIN_BUFFER_SIZE//batch_size\n",
    "    steps_per_epoch_val = VAL_BUFFER_SIZE//batch_size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_context_padded_seq[:num_training_samples],\n",
    "                                                        train_question_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_input_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_target_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_impossible[:num_training_samples],\n",
    "                                                        train_answer_start_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_end_seq_padded[:num_training_samples]\n",
    "                                                       ))\\\n",
    "                                   .shuffle(TRAIN_BUFFER_SIZE,reshuffle_each_iteration=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_context_padded_seq[:num_validation_samples],\n",
    "                                                      val_question_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_input_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_target_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_impossible[:num_validation_samples],\n",
    "                                                      val_answer_start_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_end_seq_padded[:num_validation_samples]\n",
    "                                                     ))\n",
    "    val_dataset = val_dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    #Run Epochs\n",
    "    \n",
    "    history_answer_model = {'loss':[],\n",
    "                            'categorical_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_categorical_accuracy':[]}\n",
    "\n",
    "    tqdm.write(\"\\nTraining the answer model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "        tqdm.write('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible,\n",
    "                     batch_train_answer_start_seq_padded,\n",
    "                     batch_train_answer_end_seq_padded)) in tqdm(enumerate(train_dataset.take(steps_per_epoch)),\n",
    "                                                             total = num_training_samples//batch_size,\n",
    "                                                             desc=\"[Training Answer]\"):\n",
    "            batch_loss = answer_train_step(batch_train_context_padded_seq,\n",
    "                                           batch_train_question_seq_padded,\n",
    "                                           batch_train_answer_start_seq_padded,\n",
    "                                           batch_train_answer_end_seq_padded,\n",
    "                                           encoder_model,\n",
    "                                           answer_model)\n",
    "            total_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = train_acc_metric.result()\n",
    "        tqdm.write('\\nEpoch {} - Train Loss {:.4f} Train Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                 epoch_training_loss, \n",
    "                                                                                 epoch_training_accuracy))\n",
    "        train_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        #print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible,\n",
    "                     batch_val_answer_start_seq_padded,\n",
    "                     batch_val_answer_end_seq_padded)) in tqdm(enumerate(val_dataset.take(steps_per_epoch_val)),\n",
    "                                                           total = num_validation_samples//batch_size,\n",
    "                                                           desc=\"[Validating Answer]\"):\n",
    "            batch_loss = answer_val_step(batch_val_context_padded_seq,\n",
    "                                         batch_val_question_seq_padded,\n",
    "                                         batch_val_answer_start_seq_padded,\n",
    "                                         batch_val_answer_end_seq_padded,\n",
    "                                         encoder_model,\n",
    "                                         answer_model)\n",
    "            total_val_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = val_acc_metric.result()\n",
    "        tqdm.write('\\nEpoch {} - Validation Loss {:.4f} Validation Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                           epoch_val_loss, \n",
    "                                                                                           epoch_val_accuracy))\n",
    "        val_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        #print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_answer_model['loss'].append(epoch_training_loss)\n",
    "        history_answer_model['categorical_accuracy'].append(epoch_training_accuracy)\n",
    "        history_answer_model['val_loss'].append(epoch_val_loss)\n",
    "        history_answer_model['val_categorical_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        #print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "        tqdm.write('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    answer_model.save_weights(ExperimentNo+'span_model_answer_model.h5')\n",
    "    encoder_model.save_weights(ExperimentNo+'span_model_encoder_model.h5')\n",
    "    with open(ExperimentNo+'span_model_'+'history_answer_model', 'wb') as file_history:\n",
    "        pickle.dump(history_answer_model, file_history)\n",
    "        \n",
    "    \n",
    "    #Train the Feasibility Model\n",
    "    \n",
    "    feasibility_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    feasibility_loss_object = tf.keras.losses.BinaryCrossentropy(reduction='none')\n",
    "    feasibility_train_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    feasibility_val_acc_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    @tf.function\n",
    "    def feasibility_loss_function(y_true,y_pred):\n",
    "        \n",
    "        loss = feasibility_loss_object(y_true,y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "\n",
    "    @tf.function\n",
    "    def feasibility_train_step(inp,ques,feasibility_true,encoder_model,answer_model,feasibility_model):\n",
    "        loss = 0\n",
    "        batch_loss = 0\n",
    "        question_output,context_output = encoder_model(inp,ques)\n",
    "        start_logits,end_logits = answer_model(question_output,context_output)        \n",
    "        with tf.GradientTape() as tape:            \n",
    "            feasibility_pred = feasibility_model([question_output,\n",
    "                                                  context_output,\n",
    "                                                  start_logits,\n",
    "                                                  end_logits])\n",
    "            loss += feasibility_loss_function(feasibility_true,feasibility_pred)\n",
    "            batch_loss = loss\n",
    "            feasibility_train_acc_metric(feasibility_true,feasibility_pred)\n",
    "            variables = feasibility_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            feasibility_optimizer.apply_gradients(zip(gradients, variables))\n",
    "        return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def feasibility_val_step(inp,ques,feasibility_true,encoder_model,answer_model,feasibility_model):\n",
    "        loss = 0\n",
    "        question_output,context_output = encoder_model(inp,ques)\n",
    "        start_logits,end_logits = answer_model(question_output,context_output)         \n",
    "        feasibility_pred = feasibility_model([question_output,\n",
    "                                              context_output,\n",
    "                                              start_logits,\n",
    "                                              end_logits])\n",
    "        loss += feasibility_loss_function(feasibility_true,feasibility_pred)\n",
    "        batch_loss = loss\n",
    "        feasibility_val_acc_metric(feasibility_true,feasibility_pred)\n",
    "        return batch_loss\n",
    "    #Run Epochs\n",
    "    \n",
    "    history_feasibility_model = {'loss':[],\n",
    "                            'binary_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_binary_accuracy':[]}\n",
    "\n",
    "    tqdm.write(\"\\nTraining the Feasibility model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "        tqdm.write('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible,\n",
    "                     batch_train_answer_start_seq_padded,\n",
    "                     batch_train_answer_end_seq_padded)) in tqdm(enumerate(train_dataset.take(steps_per_epoch)),\n",
    "                                                             total = num_training_samples//batch_size,\n",
    "                                                             desc=\"[Training Feasibility]\"):\n",
    "            batch_loss = feasibility_train_step(batch_train_context_padded_seq,\n",
    "                                                batch_train_question_seq_padded,\n",
    "                                                batch_train_answer_impossible,\n",
    "                                                encoder_model,\n",
    "                                                answer_model,\n",
    "                                                feasibility_model)\n",
    "            total_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = feasibility_train_acc_metric.result()\n",
    "        tqdm.write('\\nEpoch {} - Train Loss {:.4f} Train Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                 epoch_training_loss, \n",
    "                                                                                 epoch_training_accuracy))\n",
    "        feasibility_train_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        #print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible,\n",
    "                     batch_val_answer_start_seq_padded,\n",
    "                     batch_val_answer_end_seq_padded)) in tqdm(enumerate(val_dataset.take(steps_per_epoch_val)),\n",
    "                                                           total = num_validation_samples//batch_size,\n",
    "                                                           desc=\"[Validating Feasibility]\"):\n",
    "            batch_loss = feasibility_val_step(batch_val_context_padded_seq,\n",
    "                                              batch_val_question_seq_padded,\n",
    "                                              batch_val_answer_impossible,\n",
    "                                              encoder_model,\n",
    "                                              answer_model,\n",
    "                                              feasibility_model)\n",
    "            total_val_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = feasibility_val_acc_metric.result()\n",
    "        tqdm.write('\\nEpoch {} - Validation Loss {:.4f} Validation Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                           epoch_val_loss, \n",
    "                                                                                           epoch_val_accuracy))\n",
    "        feasibility_val_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        #print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_feasibility_model['loss'].append(epoch_training_loss)\n",
    "        history_feasibility_model['binary_accuracy'].append(epoch_training_accuracy)\n",
    "        history_feasibility_model['val_loss'].append(epoch_val_loss)\n",
    "        history_feasibility_model['val_binary_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        #print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "        tqdm.write('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    feasibility_model.save(ExperimentNo+'span_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'span_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model, file_history)\n",
    "    \n",
    "    \"\"\"\n",
    "        \n",
    "    train_question_output,\\\n",
    "    train_context_output = encoder_model(train_context_padded_seq[:num_training_samples],\n",
    "                                         train_question_seq_padded[:num_training_samples])\n",
    "    \n",
    "    train_start_logits,train_end_logits = answer_model(train_question_output,train_context_output)\n",
    "    \n",
    "    val_question_output,\\\n",
    "    val_context_output = encoder_model(val_context_padded_seq[:num_validation_samples],\n",
    "                                                           val_question_seq_padded[:num_validation_samples])\n",
    "    \n",
    "    val_start_logits,val_end_logits = answer_model(val_question_output,val_context_output)\n",
    "    \n",
    "    \n",
    "    print(\"training the feasibility model\")\n",
    "    history_feasibility_model = feasibility_model.fit([train_question_output,\n",
    "                                                       train_context_output,\n",
    "                                                       train_start_logits,\n",
    "                                                       train_end_logits],\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            ([val_question_output,\n",
    "                                                              val_context_output,\n",
    "                                                              val_start_logits,\n",
    "                                                              val_end_logits],\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "\n",
    "    feasibility_model.save(ExperimentNo+'span_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'span_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model.history, file_history)\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                          l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                          l2_regularizer_weight = l2_regularizer_weight )\n",
    "\n",
    "    # train on 1 row so that weights can be loaded \n",
    "    #Train the Answer Model and Encoder Model\n",
    "    inference_answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    inference_answer_loss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=True, \n",
    "                                                                           reduction='none')\n",
    "    @tf.function\n",
    "    def inference_answer_loss_function(real_start, real_end,pred_start,pred_end):\n",
    "        \n",
    "        start_loss = inference_answer_loss_object(real_start,pred_start)\n",
    "        end_loss = inference_answer_loss_object(real_end,pred_end)\n",
    "        return tf.reduce_mean(start_loss+end_loss)\n",
    "\n",
    "    @tf.function\n",
    "    def inference_answer_train_step(inp,ques,targ_start,targ_end,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            question_output,context_output = encoder_model(inp,ques)\n",
    "            start_logits,end_logits = answer_model(question_output,context_output)\n",
    "            loss += inference_answer_loss_function(targ_start,targ_end,start_logits,end_logits)\n",
    "            batch_loss = loss/inp.shape[0]\n",
    "            #train_acc_metric(targ_start,start_logits)\n",
    "            #train_acc_metric(targ_end,end_logits)\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            inference_answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    inference_answer_train_step(train_context_padded_seq[:1],\n",
    "                              train_question_seq_padded[:1],\n",
    "                              train_answer_start_seq_padded[:1],\n",
    "                              train_answer_end_seq_padded[:1],\n",
    "                              inference_encoder_model,\n",
    "                              inference_answer_model)\n",
    "        \n",
    "    inference_answer_model.load_weights(ExperimentNo+'span_model_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'span_model_encoder_model.h5')\n",
    "    inference_feasibility_model.load_weights(ExperimentNo+'span_model_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Training Answer]:   0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the answer model:\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|██████████| 320/320 [03:59<00:00,  1.34it/s]\n",
      "[Validating Answer]:   0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Train Loss 0.0587 Train Accuracy 0.2929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|██████████| 32/32 [00:07<00:00,  4.50it/s]\n",
      "[Training Answer]:   0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Validation Loss 0.0499 Validation Accuracy 0.3397\n",
      "Time taken for epoch 246.85613179206848 sec\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|██████████| 320/320 [04:00<00:00,  1.33it/s]\n",
      "[Validating Answer]:   3%|▎         | 1/32 [00:00<00:05,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Train Loss 0.0470 Train Accuracy 0.3368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|██████████| 32/32 [00:05<00:00,  5.66it/s]\n",
      "[Training Answer]:   0%|          | 0/320 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Validation Loss 0.0457 Validation Accuracy 0.3441\n",
      "Time taken for epoch 246.5573329925537 sec\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]:  19%|█▉        | 60/320 [00:44<03:25,  1.27it/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4d9b8e0fc5be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m                 \u001b[0mnum_validation_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 batch_size = 128)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-28e37e042187>\u001b[0m in \u001b[0;36mrun_experiments\u001b[0;34m(Experiment_Dic, Experiment_No, embedding_matrix, ndim, tpu_enabled, num_training_samples, num_validation_samples, num_epochs, batch_size)\u001b[0m\n\u001b[1;32m    200\u001b[0m                                            \u001b[0mbatch_train_answer_end_seq_padded\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                                            \u001b[0mencoder_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                                            answer_model)\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;31m#if batch % 50 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                Experiment_No=6,\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                ndim = ndim,\n",
    "                tpu_enabled=0,\n",
    "                num_training_samples=40960,\n",
    "                num_validation_samples = 4096,\n",
    "                num_epochs = 50,\n",
    "                batch_size = 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=6,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what does ban bossy encourage\n",
      "Predicted Answer: \n",
      "Actual answer: <s> leadership in girls </s>\n",
      "question: what area had high windows just below ground level\n",
      "Predicted Answer: \n",
      "Actual answer: <s> </s>\n",
      "question: what happens if a plant looses roots or its shoots\n",
      "Predicted Answer: \n",
      "Actual answer: <s> can often regrow it </s>\n",
      "question: what was typically worn after the loss of a loved one in the roman republic\n",
      "Predicted Answer: \n",
      "Actual answer: <s> the toga pulla </s>\n",
      "question: what instruments are used in armenian folk music\n",
      "Predicted Answer: \n",
      "Actual answer: <s> the duduk the dhol the zurna and the kanun </s>\n",
      "question: why does a litigant initiate a lawsuit under the civil rights act of 1801\n",
      "Predicted Answer: \n",
      "Actual answer: <s> </s>\n",
      "question: at what rate have glaciers travelled during surges\n",
      "Predicted Answer: \n",
      "Actual answer: <s> 90 m 300 ft per day </s>\n",
      "question: what totals a thirs of the glaciers length\n",
      "Predicted Answer: \n",
      "Actual answer: <s> </s>\n",
      "question: what color does liverpool fans wear\n",
      "Predicted Answer: \n",
      "Actual answer: <s> </s>\n",
      "question: what other industry is a large part of houston s economy\n",
      "Predicted Answer: \n",
      "Actual answer: <s> houston ship channel </s>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sentence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_answer_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "feasibilityQuestion_Input (Inpu [(None, 128)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Identity (TensorFlo [(None, 128)]        0           feasibilityQuestion_Input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims (TensorF [(None, 1, 128)]     0           tf_op_layer_Identity[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_1 (Tenso [(None, 1, 128)]     0           feasibilityQuestion_Input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 128)          16384       feasibilityQuestion_Input[0][0]  \n",
      "                                                                 tf_op_layer_Identity[0][0]       \n",
      "                                                                 feasibilityQuestion_Input[0][0]  \n",
      "                                                                 tf_op_layer_Sum[0][0]            \n",
      "                                                                 feasibilityQuestion_Input[0][0]  \n",
      "                                                                 tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "feasibilityContext_Input (Input [(None, None, 128)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile (TensorFlowOpL [(None, 250, 128)]   0           tf_op_layer_ExpandDims[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_1 (TensorFlowO [(None, 250, 128)]   0           tf_op_layer_ExpandDims_1[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_2 (Tenso [(None, 1, 128)]     0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_3 (Tenso [(None, 1, 128)]     0           dense_5[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract (Subtract)             (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_1 (Subtract)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul (TensorFlowO [(None, 1, None)]    0           tf_op_layer_ExpandDims_2[0][0]   \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_1 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_3[0][0]   \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs (TensorFlowOpLa [(None, 250, 128)]   0           subtract[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_1 (TensorFlowOp [(None, 250, 128)]   0           subtract_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute (Permute)               (None, None, 1)      0           tf_op_layer_MatMul[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "permute_1 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat (TensorFlowO [(None, 250, 898)]   0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile[0][0]           \n",
      "                                                                 tf_op_layer_Tile_1[0][0]         \n",
      "                                                                 multiply[0][0]                   \n",
      "                                                                 multiply_1[0][0]                 \n",
      "                                                                 tf_op_layer_Abs[0][0]            \n",
      "                                                                 tf_op_layer_Abs_1[0][0]          \n",
      "                                                                 permute[0][0]                    \n",
      "                                                                 permute_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 250, 192)     172608      tf_op_layer_concat[0][0]         \n",
      "                                                                 tf_op_layer_concat_1[0][0]       \n",
      "                                                                 tf_op_layer_concat_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 250, 1)       193         dense_6[0][0]                    \n",
      "                                                                 dense_6[1][0]                    \n",
      "                                                                 dense_6[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose (TensorFl [(None, 1, 250)]     0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax (TensorFlow [(None, 1, 250)]     0           tf_op_layer_transpose[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_1 (Tensor [(None, 250, 1)]     0           tf_op_layer_Softmax[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul (TensorFlowOpLa [(None, 250, 128)]   0           tf_op_layer_transpose_1[0][0]    \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum (TensorFlowOpLa [(None, 128)]        0           tf_op_layer_mul[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_4 (Tenso [(None, 1, 128)]     0           tf_op_layer_Sum[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_5 (Tenso [(None, 1, 128)]     0           feasibilityQuestion_Input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_2 (TensorFlowO [(None, 250, 128)]   0           tf_op_layer_ExpandDims_4[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_3 (TensorFlowO [(None, 250, 128)]   0           tf_op_layer_ExpandDims_5[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_6 (Tenso [(None, 1, 128)]     0           dense_5[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_7 (Tenso [(None, 1, 128)]     0           dense_5[3][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_2 (Subtract)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_3 (Subtract)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_2 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_6[0][0]   \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_3 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_7[0][0]   \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_3 (Multiply)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_2 (TensorFlowOp [(None, 250, 128)]   0           subtract_2[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_3 (TensorFlowOp [(None, 250, 128)]   0           subtract_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_2 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_2[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "permute_3 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_1 (TensorFlo [(None, 250, 898)]   0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_2[0][0]         \n",
      "                                                                 tf_op_layer_Tile_3[0][0]         \n",
      "                                                                 multiply_2[0][0]                 \n",
      "                                                                 multiply_3[0][0]                 \n",
      "                                                                 tf_op_layer_Abs_2[0][0]          \n",
      "                                                                 tf_op_layer_Abs_3[0][0]          \n",
      "                                                                 permute_2[0][0]                  \n",
      "                                                                 permute_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_2 (Tensor [(None, 1, 250)]     0           dense_7[1][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_1 (TensorFl [(None, 1, 250)]     0           tf_op_layer_transpose_2[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_3 (Tensor [(None, 250, 1)]     0           tf_op_layer_Softmax_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_1 (TensorFlowOp [(None, 250, 128)]   0           tf_op_layer_transpose_3[0][0]    \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_1 (TensorFlowOp [(None, 128)]        0           tf_op_layer_mul_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_8 (Tenso [(None, 1, 128)]     0           tf_op_layer_Sum_1[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_9 (Tenso [(None, 1, 128)]     0           feasibilityQuestion_Input[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_4 (TensorFlowO [(None, 250, 128)]   0           tf_op_layer_ExpandDims_8[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Tile_5 (TensorFlowO [(None, 250, 128)]   0           tf_op_layer_ExpandDims_9[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_10 (Tens [(None, 1, 128)]     0           dense_5[4][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_11 (Tens [(None, 1, 128)]     0           dense_5[5][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "subtract_4 (Subtract)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "subtract_5 (Subtract)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_4 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_10[0][0]  \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_MatMul_5 (TensorFlo [(None, 1, None)]    0           tf_op_layer_ExpandDims_11[0][0]  \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_4 (Multiply)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "multiply_5 (Multiply)           (None, 250, 128)     0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_4 (TensorFlowOp [(None, 250, 128)]   0           subtract_4[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Abs_5 (TensorFlowOp [(None, 250, 128)]   0           subtract_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "permute_4 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_4[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "permute_5 (Permute)             (None, None, 1)      0           tf_op_layer_MatMul_5[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_2 (TensorFlo [(None, 250, 898)]   0           feasibilityContext_Input[0][0]   \n",
      "                                                                 tf_op_layer_Tile_4[0][0]         \n",
      "                                                                 tf_op_layer_Tile_5[0][0]         \n",
      "                                                                 multiply_4[0][0]                 \n",
      "                                                                 multiply_5[0][0]                 \n",
      "                                                                 tf_op_layer_Abs_4[0][0]          \n",
      "                                                                 tf_op_layer_Abs_5[0][0]          \n",
      "                                                                 permute_4[0][0]                  \n",
      "                                                                 permute_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_4 (Tensor [(None, 1, 250)]     0           dense_7[2][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Softmax_2 (TensorFl [(None, 1, 250)]     0           tf_op_layer_transpose_4[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_transpose_5 (Tensor [(None, 250, 1)]     0           tf_op_layer_Softmax_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_mul_2 (TensorFlowOp [(None, 250, 128)]   0           tf_op_layer_transpose_5[0][0]    \n",
      "                                                                 feasibilityContext_Input[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Sum_2 (TensorFlowOp [(None, 128)]        0           tf_op_layer_mul_2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "FeasibilityStartLogitInput (Inp [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "FeasibilityEndLogitInput (Input [(None, 250)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_3 (TensorFlo [(None, 628)]        0           tf_op_layer_Sum_2[0][0]          \n",
      "                                                                 FeasibilityStartLogitInput[0][0] \n",
      "                                                                 FeasibilityEndLogitInput[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "feasibility_layer_0 (Dense)     (None, 32)           20128       tf_op_layer_concat_3[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32)           128         feasibility_layer_0[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "feasibility_drop_0 (Dropout)    (None, 32)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "feasibility_output (Dense)      (None, 1)            33          feasibility_drop_0[0][0]         \n",
      "==================================================================================================\n",
      "Total params: 209,474\n",
      "Trainable params: 209,410\n",
      "Non-trainable params: 64\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inference_feasibility_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_module\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "Context_Embedding (Embedding multiple                  8870200   \n",
      "_________________________________________________________________\n",
      "Context_Bid_Layer0 (Bidirect multiple                  102144    \n",
      "_________________________________________________________________\n",
      "Context_Bid_Layer1 (Bidirect multiple                  74496     \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo multiple                  512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "Question_Embedding (Embeddin multiple                  8870200   \n",
      "_________________________________________________________________\n",
      "Question_Bid_Layer0 (Bidirec multiple                  63744     \n",
      "_________________________________________________________________\n",
      "Question_Bid_Layer1 (Bidirec multiple                  74496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch multiple                  512       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  129       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              multiple                  101       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              multiple                  101       \n",
      "=================================================================\n",
      "Total params: 18,057,659\n",
      "Trainable params: 316,235\n",
      "Non-trainable params: 17,741,424\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inference_encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"answer_module\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              multiple                  16512     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  16512     \n",
      "=================================================================\n",
      "Total params: 33,024\n",
      "Trainable params: 33,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inference_answer_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
