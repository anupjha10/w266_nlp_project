{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\n",
      "Requirement already satisfied: tqdm in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (4.43.0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "!pip install tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "  \"\"\"\n",
    "  Load the data to a pandas dataframe.\n",
    "  Dataframe Columns:\n",
    "    title\n",
    "    para_index\n",
    "    context\n",
    "    q_index\n",
    "    q_id\n",
    "    q_isimpossible\n",
    "    q_question\n",
    "    q_anscount - number of answers\n",
    "    q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for pdata in data['data']:\n",
    "    for para in pdata['paragraphs']:\n",
    "      for q in para['qas']:\n",
    "        result.append({\n",
    "            'title' : pdata['title'],\n",
    "            'context' : para['context'],\n",
    "            'q_id' : q['id'],\n",
    "            'q_isimpossible' : q['is_impossible'],\n",
    "            'q_question' : q['question'],\n",
    "            'q_anscount' : len(q['answers']),\n",
    "            'q_answers' : [a for a in q['answers']],\n",
    "            'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "            'context_lowercase': para['context'].lower(),\n",
    "            'q_question_lowercase' : q['question'].lower(),\n",
    "            'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']],\n",
    "            \n",
    "        })\n",
    "\n",
    "  return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "  \"\"\"\n",
    "  Download a file from google drive with the shared link\n",
    "  \"\"\" \n",
    "  data = load_json_file(filename)\n",
    "  return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 214\n",
    "question_maxlen = 18\n",
    "answer_maxlen = 10\n",
    "ndim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 5002\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "          if i <= num_words:\n",
    "            vocab[word] = i\n",
    "        else:\n",
    "          vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists,num_words=5000)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible_shuffled = answer_impossible[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible_shuffled[:train_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible_shuffled[train_samples:end_samples]\n",
    "    return (train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible)\n",
    "\n",
    "train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,\\\n",
    "train_answer_impossible,\\\n",
    "val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,\\\n",
    "val_answer_impossible\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104192"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_question_seq_padded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26112"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_answer_seq_padded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.300d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 300))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5003, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Model with attention in every step of the answer decoder\n",
    "class BahdanauAttention_model2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention_model2, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Answer Module which is custom as we need to feed output of each time sequence with attention to next \n",
    "# time sequence\n",
    "class answer_module(tf.keras.Model):\n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      ndim,\n",
    "                      num_unit_gru,\n",
    "                      num_layers_gru,\n",
    "                      dropout_rate,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(answer_module, self).__init__()\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_layer = BahdanauAttention_model2(num_unit_gru)\n",
    "        #self.answer_input_layer = Input(shape=(None,),dtype='int32',name='Answer_Input')\n",
    "        self.answer_embedding_layer = layers.Embedding(vocab_size+1,\n",
    "                                                       ndim,\n",
    "                                                       mask_zero=True,\n",
    "                                                       weights =[embedding_matrix],\n",
    "                                                       trainable = False,\n",
    "                                                       name='Answer_Embedding')\n",
    "        self.answer_output_layers = []\n",
    "        self.batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "            self.answer_output_layers.append(layers.GRU(self.num_unit_gru,\n",
    "                                                        dropout=self.dropout_rate,\n",
    "                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                        return_sequences=True,\n",
    "                                                        return_state=True,\n",
    "                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                        name='Answer_GRU_Layer'+str(i),\n",
    "                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                                       )\n",
    "                                              )\n",
    "            self.batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        \n",
    "        self.answer_decoder_dense = layers.Dense(vocab_size+1,\n",
    "                                                 name='Answer_output',\n",
    "                                                 kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                 bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight))\n",
    "        \n",
    "    \n",
    "    def call(self,\n",
    "             target_input,\n",
    "             answer_hidden,\n",
    "             question,\n",
    "             context,\n",
    "             episodic_memory):\n",
    "        attention_output,attention_weights = self.attention_layer(answer_hidden,context)\n",
    "        answer_embeddings = self.answer_embedding_layer(target_input)\n",
    "        answer_concat = tf.concat([tf.expand_dims(attention_output, 1),\n",
    "                                   tf.expand_dims(question, 1),\n",
    "                                   answer_embeddings], axis=-1)\n",
    "        for i in range(len(self.answer_output_layers)):\n",
    "            if ((episodic_memory != None) and (i==0)):\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_concat,\n",
    "                                                                           initial_state=episodic_memory)\n",
    "            elif i==0:\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_concat)\n",
    "            else:\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_outputs)\n",
    "            \n",
    "            answer_outputs   = self.batch_normalization_layers[i](answer_outputs)\n",
    "        answer_outputs = tf.reshape(answer_outputs, (-1, answer_outputs.shape[2]))\n",
    "\n",
    "        answer_decoder_outputs = self.answer_decoder_dense(answer_outputs)\n",
    "        return answer_decoder_outputs,hidden_state,attention_weights\n",
    "\n",
    "#Encoder Module which combines the context,question into episodic memory and emits context outputs and \n",
    "#question outputs\n",
    "class encoder_module(tf.keras.Model):    \n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      max_context_length,\n",
    "                      max_question_length,\n",
    "                      max_answer_length,\n",
    "                      num_unit_gru = 64,\n",
    "                      num_layers_gru = 2,\n",
    "                      ndim =100,\n",
    "                      num_episodes = 2,\n",
    "                      dropout_rate = 0.5,\n",
    "                      num_episodic_network_unit = 64,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(encoder_module, self).__init__()\n",
    "        #Context Module\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers_gru = num_layers_gru\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ndim = ndim\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_question_length = max_question_length\n",
    "        self.max_answer_length = max_answer_length\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_episodic_network_unit = num_episodic_network_unit\n",
    "        self.context_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                         self.ndim,\n",
    "                                                         mask_zero=True,\n",
    "                                                         weights =[self.embedding_matrix],\n",
    "                                                         trainable = False,\n",
    "                                                         name='Context_Embedding')\n",
    "        self.context_output_layers = []\n",
    "        self.context_batch_normalization_layers = []\n",
    "        for i in range(self.num_layers_gru):\n",
    "            self.context_output_layers.append(layers.Bidirectional(layers.GRU(self.num_unit_gru,\n",
    "                                                                               dropout=self.dropout_rate,\n",
    "                                                                               recurrent_dropout= self.dropout_rate,\n",
    "                                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                                               return_sequences=True,\n",
    "                                                                               kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                               bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                                   merge_mode='sum',\n",
    "                                                                   name='Context_Bid_Layer'+str(i))\n",
    "                                         )\n",
    "            self.context_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        #Question Module\n",
    "        self.question_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                          self.ndim,\n",
    "                                                          mask_zero=True,\n",
    "                                                          weights =[self.embedding_matrix],\n",
    "                                                          trainable = False,\n",
    "                                                          name='Question_Embedding')\n",
    "          \n",
    "        self.question_output_layers = []\n",
    "        self.question_batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "\n",
    "            if i==0 and num_layers_gru >1:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=True,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                             merge_mode='sum',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            elif i==0 and num_layers_gru ==1:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=False,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                         bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                             merge_mode='sum',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                             )\n",
    "            elif i==(num_layers_gru-1):\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=False,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                              merge_mode='sum',\n",
    "                                                              name='Question_Bid_Layer'+str(i))\n",
    "                                             )\n",
    "            else:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=True,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                                        merge_mode='sum',\n",
    "                                                                        name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.question_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        #Episodic Memory \n",
    "        self.episodic_weight_layer = layers.Dense(self.num_unit_gru,use_bias=False)\n",
    "        self.episodic_tanh_layer = layers.Dense(self.num_episodic_network_unit,activation='tanh')\n",
    "        self.episodic_score_layer = layers.Dense(1)\n",
    "        \n",
    "    def call(self,context_input,question_input):\n",
    "        #context Module\n",
    "        context_embeddings = self.context_embeddings_layer(context_input)\n",
    "        for i in range(len(self.context_output_layers)):\n",
    "            if i==0:\n",
    "                context_outputs = self.context_output_layers[i](context_embeddings)\n",
    "            else:\n",
    "                context_outputs = self.context_output_layers[i](context_outputs)\n",
    "            context_outputs = self.context_batch_normalization_layers[i](context_outputs)\n",
    "\n",
    "        #Question Module\n",
    "        question_embeddings = self.question_embeddings_layer(question_input)\n",
    "        for i in range(len(self.question_output_layers)):\n",
    "            if i==0:\n",
    "                question_outputs = self.question_output_layers[i](question_embeddings)\n",
    "            else:\n",
    "                question_outputs = self.question_output_layers[i](question_outputs)\n",
    "            question_outputs = self.question_batch_normalization_layers[i](question_outputs) \n",
    "        #Episodic Memory \n",
    "        m = tf.identity(question_outputs)\n",
    "        for i in range(self.num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(question_outputs,1),\n",
    "                                  tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(question_outputs),1), \n",
    "                                context_outputs,\n",
    "                                transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(m),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = tf.concat([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = self.episodic_score_layer(self.episodic_tanh_layer(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        #concatenate episodic memory with question\n",
    "        concatenated_tensor = tf.concat(values=[m,question_outputs],axis=1)\n",
    "        return (m,concatenated_tensor,question_outputs,context_outputs)\n",
    "    \n",
    "                \n",
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64,\n",
    "                  l1_regularizer_weight = .01,\n",
    "                  l2_regularizer_weight = .01):\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        weight_layer = layers.Dense(query.shape[1],use_bias=False)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(weight_layer(query),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(weight_layer(m),1), context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    \"\"\"\n",
    "    #encoder Model\n",
    "    encoder_model = encoder_module(embedding_matrix,\n",
    "                                   vocab_size,\n",
    "                                   max_context_length,\n",
    "                                   max_question_length,\n",
    "                                   max_answer_length,\n",
    "                                   num_unit_gru,\n",
    "                                   num_layers_gru,\n",
    "                                   ndim,\n",
    "                                   num_episodes,\n",
    "                                   dropout_rate,\n",
    "                                   num_episodic_network_unit,\n",
    "                                   l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                   l2_regularizer_weight = l2_regularizer_weight\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    #Model([context_input,question_input], [m,concatenated_tensor,question_outputs,context_outputs])\n",
    "    #answer_module\n",
    "    answer_model = answer_module(embedding_matrix,\n",
    "                                 vocab_size,\n",
    "                                 ndim,\n",
    "                                 num_unit_gru,\n",
    "                                 num_layers_gru,\n",
    "                                 dropout_rate,\n",
    "                                 l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                 l2_regularizer_weight = l2_regularizer_weight)\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    feasibility_input = Input(shape=(2*num_unit_gru,), name=\"FeasibilityInput\")\n",
    "    feasibility_context_input = Input(shape=(None,num_unit_gru,),name='feasibilityContext_Input')\n",
    "    feasibility_question_input = Input(shape=(num_unit_gru,),name='feasibilityQuestion_Input')\n",
    "    for i in range(num_dense_layers_feasibility):\n",
    "        #create attention between Context and Question\n",
    "        q_with_time_axis = tf.keras.backend.expand_dims(feasibility_question_input,1)\n",
    "        attentionContextQuestion = layers.AdditiveAttention()([q_with_time_axis,\n",
    "                                                               feasibility_context_input])\n",
    "        attentionContextQuestionReduced = tf.keras.backend.sum(attentionContextQuestion, axis=1)\n",
    "        feasibility_dense_input = tf.concat([feasibility_input,\n",
    "                                             attentionContextQuestionReduced],\n",
    "                                            axis=-1)\n",
    "        \n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(feasibility_dense_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',\n",
    "                                      name='feasibility_output',\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                     l1_regularizer_weight,\n",
    "                                                                                    l2_regularizer_weight),\n",
    "                                      bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                   l1_regularizer_weight,\n",
    "                                                                                   l2_regularizer_weight))(dropout_layer)\n",
    "    feasibility_model = Model([feasibility_input,feasibility_context_input,feasibility_question_input],\n",
    "                              feasibility_output)\n",
    "    \n",
    "    return (answer_model,encoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sentence(context_input_seq,\n",
    "                     question_input_seq,\n",
    "                     encoder_model,\n",
    "                     answer_model):\n",
    "    decoded_sentence = ''\n",
    "    episodic_memory,\\\n",
    "    concatenated_tensor,\\\n",
    "    question_output,\\\n",
    "    context_output = encoder_model(context_input_seq,question_input_seq) \n",
    "    answer_hidden = question_output\n",
    "    dec_input = tf.expand_dims([vocab[\"<s>\"]], 0)\n",
    "\n",
    "    for t in range(answer_maxlen):\n",
    "        predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                     answer_hidden,\n",
    "                                                     question_output,\n",
    "                                                     context_output,\n",
    "                                                     episodic_memory)\n",
    "        sampled_token_index = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \" \"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "        if id_vocab[sampled_token_index] == '</s>':\n",
    "            return decoded_sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([sampled_token_index], 0)\n",
    "        episodic_memory = None\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.0001,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment2': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment4': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 128,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.001},\n",
       " 'Experiment5': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment6': {'num_unit_gru': 16,\n",
       "  'num_layers_gru': 1,\n",
       "  'num_episodes': 1,\n",
       "  'num_dense_layer_feasibility_units': 16,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 32,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.1}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 64,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment1': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.0001,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment2': {'num_unit_gru': 100,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 64,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment3': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment4': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 48,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 128,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.001},\n",
    "                  'Experiment5': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment6': {'num_unit_gru': 16,\n",
    "                                  'num_layers_gru': 1,\n",
    "                                  'num_episodes': 1,\n",
    "                                  'num_dense_layer_feasibility_units': 16,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 32,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.1}\n",
    "                }\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "        \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                      l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                      l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        #encoder_model.compile(optimizer=adam_optim,\n",
    "        #                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        #encoder_model.summary()\n",
    "        #answer_model.compile(optimizer=adam_optim,\n",
    "        #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        #                        )\n",
    "        #\n",
    "        #answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        #feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' +\n",
    "                                                                     os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = \n",
    "                                                        num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                         l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                         l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            #encoder_model.compile(optimizer=adam_optim,\n",
    "            #                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "            #encoder_model.summary()\n",
    "            #\n",
    "            #answer_model.compile(optimizer=adam_optim,\n",
    "            #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            #                    )\n",
    "\n",
    "            #answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            #feasibility_model.summary()\n",
    "    #Train the Answer Model and Encoder Model\n",
    "    answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    answer_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    @tf.function\n",
    "    def answer_loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = answer_loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def answer_train_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "            answer_hidden = question_output\n",
    "            dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                             answer_hidden,\n",
    "                                                             question_output,\n",
    "                                                             context_output,\n",
    "                                                             episodic_memory)\n",
    "                loss += answer_loss_function(targ[:, t], predictions)\n",
    "                train_acc_metric(targ[:, t],predictions)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                episodic_memory = None\n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def answer_val_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "        answer_hidden = question_output\n",
    "        dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                         answer_hidden,\n",
    "                                                         question_output,\n",
    "                                                         context_output,\n",
    "                                                         episodic_memory)\n",
    "            loss += answer_loss_function(targ[:, t], predictions)\n",
    "            val_acc_metric(targ[:, t],predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            episodic_memory = None\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        return batch_loss\n",
    "    #Create batches for training\n",
    "    \n",
    "    TRAIN_BUFFER_SIZE = train_context_padded_seq[:num_training_samples].shape[0]\n",
    "    VAL_BUFFER_SIZE = val_context_padded_seq[:num_validation_samples].shape[0]\n",
    "    steps_per_epoch = TRAIN_BUFFER_SIZE//batch_size\n",
    "    steps_per_epoch_val = VAL_BUFFER_SIZE//batch_size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_context_padded_seq[:num_training_samples],\n",
    "                                                        train_question_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_input_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_target_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_impossible[:num_training_samples]))\\\n",
    "                                   .shuffle(TRAIN_BUFFER_SIZE,reshuffle_each_iteration=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_context_padded_seq[:num_validation_samples],\n",
    "                                                      val_question_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_input_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_target_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_impossible[:num_validation_samples]))\n",
    "    val_dataset = val_dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    #Run Epochs\n",
    "    \n",
    "    history_answer_model = {'loss':[],\n",
    "                            'sparse_categorical_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_sparse_categorical_accuracy':[]}\n",
    "\n",
    "    tqdm.write(\"\\nTraining the answer model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "        tqdm.write('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible)) in tqdm(enumerate(train_dataset.take(steps_per_epoch)),\n",
    "                                                             total = num_training_samples//batch_size,\n",
    "                                                             desc=\"[Training Answer]\"):\n",
    "            batch_loss = answer_train_step(batch_train_context_padded_seq,\n",
    "                                           batch_train_question_seq_padded,\n",
    "                                           batch_train_answer_input_seq_padded,\n",
    "                                           encoder_model,\n",
    "                                           answer_model)\n",
    "            total_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = train_acc_metric.result()\n",
    "        tqdm.write('\\nEpoch {} - Train Loss {:.4f} Train Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                 epoch_training_loss, \n",
    "                                                                                 epoch_training_accuracy))\n",
    "        train_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        #print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible)) in tqdm(enumerate(val_dataset.take(steps_per_epoch_val)),\n",
    "                                                           total = num_validation_samples//batch_size,\n",
    "                                                           desc=\"[Validating Answer]\"):\n",
    "            batch_loss = answer_val_step(batch_val_context_padded_seq,\n",
    "                                         batch_val_question_seq_padded,\n",
    "                                         batch_val_answer_input_seq_padded,\n",
    "                                         encoder_model,\n",
    "                                         answer_model)\n",
    "            total_val_loss += batch_loss\n",
    "            #if batch % 50 == 0:\n",
    "            #    print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = val_acc_metric.result()\n",
    "        tqdm.write('\\nEpoch {} - Validation Loss {:.4f} Validation Accuracy {:.4f}'.format(epoch + 1,\n",
    "                                                                                           epoch_val_loss, \n",
    "                                                                                           epoch_val_accuracy))\n",
    "        val_acc_metric.reset_states()\n",
    "        #print('')\n",
    "        #print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        #print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_answer_model['loss'].append(epoch_training_loss)\n",
    "        history_answer_model['sparse_categorical_accuracy'].append(epoch_training_accuracy)\n",
    "        history_answer_model['val_loss'].append(epoch_val_loss)\n",
    "        history_answer_model['val_sparse_categorical_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        #print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "        tqdm.write('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    answer_model.save_weights(ExperimentNo+'attention_model_answer_model.h5')\n",
    "    encoder_model.save_weights(ExperimentNo+'attention_model_encoder_model.h5')\n",
    "    with open(ExperimentNo+'attention_model_'+'history_answer_model', 'wb') as file_history:\n",
    "        pickle.dump(history_answer_model, file_history)\n",
    "        \n",
    "    \n",
    "    #Train the Feasibility Model\n",
    "    _,encoder_prediction,encoder_prediction_question,\\\n",
    "    encoder_prediction_contexts = encoder_model(train_context_padded_seq[:num_training_samples],\n",
    "                                             train_question_seq_padded[:num_training_samples])\n",
    "    _,encoder_validation_prediction,encoder_validation_prediction_question,\\\n",
    "    encoder_validation_prediction_contexts = encoder_model(val_context_padded_seq[:num_validation_samples],\n",
    "                                                         val_question_seq_padded[:num_validation_samples])\n",
    "    print(\"training the feasibility model\")\n",
    "    history_feasibility_model = feasibility_model.fit([encoder_prediction,\n",
    "                                                       encoder_prediction_contexts,\n",
    "                                                       encoder_prediction_question],\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            ([encoder_validation_prediction,\n",
    "                                                              encoder_validation_prediction_contexts,\n",
    "                                                              encoder_validation_prediction_question],\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "\n",
    "    feasibility_model.save(ExperimentNo+'attention_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'attention_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model.history, file_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                          l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                          l2_regularizer_weight = l2_regularizer_weight )\n",
    "\n",
    "    # train on 1 row so that weights can be loaded \n",
    "    #Train the Answer Model and Encoder Model\n",
    "    inference_answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    inference_answer_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    @tf.function\n",
    "    def inference_answer_loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = inference_answer_loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def inference_answer_train_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "            answer_hidden = question_output\n",
    "            dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                             answer_hidden,\n",
    "                                                             question_output,\n",
    "                                                             context_output,\n",
    "                                                             episodic_memory)\n",
    "                loss += inference_answer_loss_function(targ[:, t], predictions)\n",
    "                #train_acc_metric(targ[:, t],predictions)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                episodic_memory = None\n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            inference_answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "    inference_answer_train_step(train_context_padded_seq[:1],\n",
    "                              train_question_seq_padded[:1],\n",
    "                              train_answer_input_seq_padded[:1],\n",
    "                              inference_encoder_model,\n",
    "                              inference_answer_model)\n",
    "        \n",
    "    inference_answer_model.load_weights(ExperimentNo+'attention_model_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'attention_model_encoder_model.h5')\n",
    "    #inference_feasibility_model.load_weights(ExperimentNo+'attention_model_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the answer model:\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:44<00:00,  2.85it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Train Loss 1.2731 Train Accuracy 0.0869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:05<00:00,  1.51it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1 - Validation Loss 1.2110 Validation Accuracy 0.0866\n",
      "Time taken for epoch 230.08457970619202 sec\n",
      "\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Train Loss 1.1138 Train Accuracy 0.0890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.57it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2 - Validation Loss 1.1813 Validation Accuracy 0.0941\n",
      "Time taken for epoch 211.02720737457275 sec\n",
      "\n",
      "Epoch 3/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 11.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 - Train Loss 1.0870 Train Accuracy 0.0973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.56it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3 - Validation Loss 1.1634 Validation Accuracy 0.0966\n",
      "Time taken for epoch 211.7971043586731 sec\n",
      "\n",
      "Epoch 4/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 - Train Loss 1.0656 Train Accuracy 0.1008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.58it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4 - Validation Loss 1.1533 Validation Accuracy 0.0987\n",
      "Time taken for epoch 212.10426115989685 sec\n",
      "\n",
      "Epoch 5/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 11.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 - Train Loss 1.0454 Train Accuracy 0.1035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.94it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5 - Validation Loss 1.1437 Validation Accuracy 0.0959\n",
      "Time taken for epoch 211.78785753250122 sec\n",
      "\n",
      "Epoch 6/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.03it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 - Train Loss 1.0258 Train Accuracy 0.1058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.40it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6 - Validation Loss 1.1325 Validation Accuracy 0.0962\n",
      "Time taken for epoch 211.78714632987976 sec\n",
      "\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 - Train Loss 1.0062 Train Accuracy 0.1074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.65it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7 - Validation Loss 1.1217 Validation Accuracy 0.0955\n",
      "Time taken for epoch 210.9927635192871 sec\n",
      "\n",
      "Epoch 8/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 - Train Loss 0.9870 Train Accuracy 0.1093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.49it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8 - Validation Loss 1.1169 Validation Accuracy 0.0969\n",
      "Time taken for epoch 211.55877017974854 sec\n",
      "\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.02it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 - Train Loss 0.9688 Train Accuracy 0.1106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.46it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9 - Validation Loss 1.1139 Validation Accuracy 0.0945\n",
      "Time taken for epoch 212.78715801239014 sec\n",
      "\n",
      "Epoch 10/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.03it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 11.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 - Train Loss 0.9515 Train Accuracy 0.1117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.35it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10 - Validation Loss 1.1108 Validation Accuracy 0.0962\n",
      "Time taken for epoch 211.6704225540161 sec\n",
      "\n",
      "Epoch 11/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 - Train Loss 0.9352 Train Accuracy 0.1127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.92it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11 - Validation Loss 1.1146 Validation Accuracy 0.0945\n",
      "Time taken for epoch 211.7664155960083 sec\n",
      "\n",
      "Epoch 12/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 - Train Loss 0.9202 Train Accuracy 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.99it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12 - Validation Loss 1.1168 Validation Accuracy 0.0952\n",
      "Time taken for epoch 212.0037133693695 sec\n",
      "\n",
      "Epoch 13/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 - Train Loss 0.9052 Train Accuracy 0.1147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.55it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 13 - Validation Loss 1.1274 Validation Accuracy 0.0934\n",
      "Time taken for epoch 211.96750950813293 sec\n",
      "\n",
      "Epoch 14/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 - Train Loss 0.8922 Train Accuracy 0.1156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 12.15it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14 - Validation Loss 1.1244 Validation Accuracy 0.0920\n",
      "Time taken for epoch 211.55646014213562 sec\n",
      "\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 - Train Loss 0.8799 Train Accuracy 0.1160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.69it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15 - Validation Loss 1.1356 Validation Accuracy 0.0881\n",
      "Time taken for epoch 211.26387119293213 sec\n",
      "\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 - Train Loss 0.8681 Train Accuracy 0.1170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.82it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 16 - Validation Loss 1.1446 Validation Accuracy 0.0916\n",
      "Time taken for epoch 212.15539026260376 sec\n",
      "\n",
      "Epoch 17/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 10.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 - Train Loss 0.8569 Train Accuracy 0.1176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.10it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 17 - Validation Loss 1.1496 Validation Accuracy 0.0906\n",
      "Time taken for epoch 211.44936728477478 sec\n",
      "\n",
      "Epoch 18/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:31<00:00,  3.03it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 - Train Loss 0.8466 Train Accuracy 0.1182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.13it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 18 - Validation Loss 1.1683 Validation Accuracy 0.0913\n",
      "Time taken for epoch 211.98343896865845 sec\n",
      "\n",
      "Epoch 19/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 - Train Loss 0.8364 Train Accuracy 0.1191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.80it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 19 - Validation Loss 1.1795 Validation Accuracy 0.0884\n",
      "Time taken for epoch 211.07750868797302 sec\n",
      "\n",
      "Epoch 20/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.05it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 - Train Loss 0.8265 Train Accuracy 0.1196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.89it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20 - Validation Loss 1.1804 Validation Accuracy 0.0906\n",
      "Time taken for epoch 210.943852186203 sec\n",
      "\n",
      "Epoch 21/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  8.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 - Train Loss 0.8184 Train Accuracy 0.1205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.42it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 21 - Validation Loss 1.1945 Validation Accuracy 0.0866\n",
      "Time taken for epoch 211.3654215335846 sec\n",
      "\n",
      "Epoch 22/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 - Train Loss 0.8093 Train Accuracy 0.1210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.93it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 22 - Validation Loss 1.2169 Validation Accuracy 0.0863\n",
      "Time taken for epoch 211.082825422287 sec\n",
      "\n",
      "Epoch 23/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 - Train Loss 0.8024 Train Accuracy 0.1213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.40it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23 - Validation Loss 1.2170 Validation Accuracy 0.0863\n",
      "Time taken for epoch 211.07095098495483 sec\n",
      "\n",
      "Epoch 24/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.05it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 10.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 - Train Loss 0.7933 Train Accuracy 0.1226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.07it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 24 - Validation Loss 1.2165 Validation Accuracy 0.0895\n",
      "Time taken for epoch 210.82362747192383 sec\n",
      "\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 - Train Loss 0.7862 Train Accuracy 0.1228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.99it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25 - Validation Loss 1.2403 Validation Accuracy 0.0881\n",
      "Time taken for epoch 211.5685167312622 sec\n",
      "\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.04it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 - Train Loss 0.7787 Train Accuracy 0.1236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.34it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 26 - Validation Loss 1.2508 Validation Accuracy 0.0845\n",
      "Time taken for epoch 211.3778896331787 sec\n",
      "\n",
      "Epoch 27/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:29<00:00,  3.05it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 - Train Loss 0.7727 Train Accuracy 0.1240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.07it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27 - Validation Loss 1.2663 Validation Accuracy 0.0891\n",
      "Time taken for epoch 210.6427037715912 sec\n",
      "\n",
      "Epoch 28/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:26<00:00,  3.09it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 11.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 - Train Loss 0.7664 Train Accuracy 0.1248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.81it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 28 - Validation Loss 1.2747 Validation Accuracy 0.0874\n",
      "Time taken for epoch 207.6512942314148 sec\n",
      "\n",
      "Epoch 29/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:26<00:00,  3.10it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 - Train Loss 0.7595 Train Accuracy 0.1253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.24it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29 - Validation Loss 1.2871 Validation Accuracy 0.0856\n",
      "Time taken for epoch 206.95768761634827 sec\n",
      "\n",
      "Epoch 30/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.11it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 - Train Loss 0.7531 Train Accuracy 0.1261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.32it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30 - Validation Loss 1.2868 Validation Accuracy 0.0866\n",
      "Time taken for epoch 206.60338973999023 sec\n",
      "\n",
      "Epoch 31/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.11it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 - Train Loss 0.7471 Train Accuracy 0.1266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.27it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 31 - Validation Loss 1.3051 Validation Accuracy 0.0881\n",
      "Time taken for epoch 206.52607131004333 sec\n",
      "\n",
      "Epoch 32/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 - Train Loss 0.7425 Train Accuracy 0.1268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.44it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 32 - Validation Loss 1.3260 Validation Accuracy 0.0845\n",
      "Time taken for epoch 206.18457865715027 sec\n",
      "\n",
      "Epoch 33/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 - Train Loss 0.7378 Train Accuracy 0.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.58it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 33 - Validation Loss 1.3367 Validation Accuracy 0.0838\n",
      "Time taken for epoch 205.887619972229 sec\n",
      "\n",
      "Epoch 34/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:29<00:00,  3.05it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 - Train Loss 0.7316 Train Accuracy 0.1279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.89it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34 - Validation Loss 1.3382 Validation Accuracy 0.0842\n",
      "Time taken for epoch 210.7873797416687 sec\n",
      "\n",
      "Epoch 35/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:32<00:00,  3.02it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 - Train Loss 0.7273 Train Accuracy 0.1284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.18it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 35 - Validation Loss 1.3481 Validation Accuracy 0.0842\n",
      "Time taken for epoch 212.88057017326355 sec\n",
      "\n",
      "Epoch 36/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:29<00:00,  3.06it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 - Train Loss 0.7221 Train Accuracy 0.1289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.00it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 36 - Validation Loss 1.3567 Validation Accuracy 0.0913\n",
      "Time taken for epoch 210.06727576255798 sec\n",
      "\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:29<00:00,  3.06it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 - Train Loss 0.7171 Train Accuracy 0.1293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.09it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 37 - Validation Loss 1.3625 Validation Accuracy 0.0827\n",
      "Time taken for epoch 209.91608428955078 sec\n",
      "\n",
      "Epoch 38/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:30<00:00,  3.05it/s]\n",
      "[Validating Answer]:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 - Train Loss 0.7135 Train Accuracy 0.1300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.13it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 38 - Validation Loss 1.3880 Validation Accuracy 0.0881\n",
      "Time taken for epoch 210.83158445358276 sec\n",
      "\n",
      "Epoch 39/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:29<00:00,  3.05it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 - Train Loss 0.7084 Train Accuracy 0.1307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.98it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 39 - Validation Loss 1.3965 Validation Accuracy 0.0842\n",
      "Time taken for epoch 210.33050155639648 sec\n",
      "\n",
      "Epoch 40/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:28<00:00,  3.07it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 - Train Loss 0.7054 Train Accuracy 0.1306\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.15it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 40 - Validation Loss 1.3883 Validation Accuracy 0.0884\n",
      "Time taken for epoch 209.0566761493683 sec\n",
      "\n",
      "Epoch 41/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 - Train Loss 0.7009 Train Accuracy 0.1313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.97it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41 - Validation Loss 1.4098 Validation Accuracy 0.0852\n",
      "Time taken for epoch 206.2682764530182 sec\n",
      "\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 - Train Loss 0.6968 Train Accuracy 0.1316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.63it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 42 - Validation Loss 1.4111 Validation Accuracy 0.0835\n",
      "Time taken for epoch 206.20537090301514 sec\n",
      "\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.11it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 - Train Loss 0.6942 Train Accuracy 0.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.25it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 43 - Validation Loss 1.4226 Validation Accuracy 0.0849\n",
      "Time taken for epoch 206.7157289981842 sec\n",
      "\n",
      "Epoch 44/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.11it/s]\n",
      "[Validating Answer]:  25%|       | 2/8 [00:00<00:00, 10.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 - Train Loss 0.6918 Train Accuracy 0.1319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.23it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44 - Validation Loss 1.4080 Validation Accuracy 0.0813\n",
      "Time taken for epoch 206.53044772148132 sec\n",
      "\n",
      "Epoch 45/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00, 10.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 - Train Loss 0.6858 Train Accuracy 0.1329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.32it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45 - Validation Loss 1.4462 Validation Accuracy 0.0813\n",
      "Time taken for epoch 206.18790459632874 sec\n",
      "\n",
      "Epoch 46/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 - Train Loss 0.6840 Train Accuracy 0.1327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.28it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 46 - Validation Loss 1.4473 Validation Accuracy 0.0845\n",
      "Time taken for epoch 206.22868943214417 sec\n",
      "\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 - Train Loss 0.6815 Train Accuracy 0.1332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.27it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47 - Validation Loss 1.4392 Validation Accuracy 0.0842\n",
      "Time taken for epoch 206.20155477523804 sec\n",
      "\n",
      "Epoch 48/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 - Train Loss 0.6763 Train Accuracy 0.1337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.40it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48 - Validation Loss 1.4500 Validation Accuracy 0.0849\n",
      "Time taken for epoch 206.02336311340332 sec\n",
      "\n",
      "Epoch 49/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:25<00:00,  3.12it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 - Train Loss 0.6738 Train Accuracy 0.1343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00,  9.84it/s]\n",
      "[Training Answer]:   0%|          | 0/640 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49 - Validation Loss 1.4423 Validation Accuracy 0.0842\n",
      "Time taken for epoch 206.22287845611572 sec\n",
      "\n",
      "Epoch 50/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Training Answer]: 100%|| 640/640 [03:27<00:00,  3.09it/s]\n",
      "[Validating Answer]:  12%|        | 1/8 [00:00<00:00,  9.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 - Train Loss 0.6719 Train Accuracy 0.1342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Validating Answer]: 100%|| 8/8 [00:00<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 50 - Validation Loss 1.4670 Validation Accuracy 0.0806\n",
      "Time taken for epoch 207.93482065200806 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                Experiment_No=6,\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                ndim = ndim,\n",
    "                tpu_enabled=0,\n",
    "                num_training_samples=20480,\n",
    "                num_validation_samples = 256,\n",
    "                num_epochs = 50,\n",
    "                batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=6,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what do <unk> <unk> motors not <unk> at <unk>\n",
      "Predicted Answer: <unk> transfer </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what type of agreement requires majority approval by both the house and the senate before or after a\n",
      "Predicted Answer: the <unk> </s> \n",
      "Actual answer: <s> <unk> executive agreements </s>\n",
      "question: which well known <unk> has used sanskrit <unk> in her music\n",
      "Predicted Answer: <unk> <unk> </s> \n",
      "Actual answer: <s> madonna </s>\n",
      "question: what types of <unk> are used for human consumption\n",
      "Predicted Answer: <unk> <unk> </s> \n",
      "Actual answer: <s> the <unk> derived birds </s>\n",
      "question: what is part of the <unk> church\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: which tennessee city has the largest african american population\n",
      "Predicted Answer: australian </s> \n",
      "Actual answer: <s> <unk> </s>\n",
      "question: which tree species has become a problem for galicia\n",
      "Predicted Answer: italy </s> \n",
      "Actual answer: <s> <unk> tree </s>\n",
      "question: what have canadian military units done since 1974\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what is the cia s main focus\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> overseas intelligence gathering </s>\n",
      "question: who did the ottomans lose to at the battle of <unk>\n",
      "Predicted Answer: the ming <unk> </s> \n",
      "Actual answer: <s> <unk> </s>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(30000,30010):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sentence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_answer_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
