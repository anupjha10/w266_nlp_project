{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "  \"\"\"\n",
    "  Load the data to a pandas dataframe.\n",
    "  Dataframe Columns:\n",
    "    title\n",
    "    para_index\n",
    "    context\n",
    "    q_index\n",
    "    q_id\n",
    "    q_isimpossible\n",
    "    q_question\n",
    "    q_anscount - number of answers\n",
    "    q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for pdata in data['data']:\n",
    "    for para in pdata['paragraphs']:\n",
    "      for q in para['qas']:\n",
    "        result.append({\n",
    "            'title' : pdata['title'],\n",
    "            'context' : para['context'],\n",
    "            'q_id' : q['id'],\n",
    "            'q_isimpossible' : q['is_impossible'],\n",
    "            'q_question' : q['question'],\n",
    "            'q_anscount' : len(q['answers']),\n",
    "            'q_answers' : [a for a in q['answers']],\n",
    "            'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "            'context_lowercase': para['context'].lower(),\n",
    "            'q_question_lowercase' : q['question'].lower(),\n",
    "            'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']],\n",
    "            \n",
    "        })\n",
    "\n",
    "  return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "  \"\"\"\n",
    "  Download a file from google drive with the shared link\n",
    "  \"\"\" \n",
    "  data = load_json_file(filename)\n",
    "  return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 214\n",
    "question_maxlen = 18\n",
    "answer_maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "          if i <= num_words:\n",
    "            vocab[word] = i\n",
    "        else:\n",
    "          vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible_shuffled = answer_impossible[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible_shuffled[:train_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible_shuffled[train_samples:end_samples]\n",
    "    return (train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible)\n",
    "\n",
    "train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,\\\n",
    "train_answer_impossible,\\\n",
    "val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,\\\n",
    "val_answer_impossible\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "ndim = 100\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Model with attention in every step of the answer decoder\n",
    "class BahdanauAttention_model2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention_model2, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Answer Module which is custom as we need to feed output of each time sequence with attention to next \n",
    "# time sequence\n",
    "class answer_module(tf.keras.Model):\n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      ndim,\n",
    "                      num_unit_gru,\n",
    "                      num_layers_gru,\n",
    "                      dropout_rate\n",
    "                      ):\n",
    "        super(answer_module, self).__init__()\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_layer = BahdanauAttention_model2(num_unit_gru)\n",
    "        #self.answer_input_layer = Input(shape=(None,),dtype='int32',name='Answer_Input')\n",
    "        self.answer_embedding_layer = layers.Embedding(vocab_size+1,\n",
    "                                                       ndim,\n",
    "                                                       mask_zero=True,\n",
    "                                                       weights =[embedding_matrix],\n",
    "                                                       trainable = False,\n",
    "                                                       name='Answer_Embedding')\n",
    "        self.answer_output_layers = []\n",
    "        self.batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "            self.answer_output_layers.append(layers.GRU(self.num_unit_gru,\n",
    "                                                        dropout=self.dropout_rate,\n",
    "                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                        return_sequences=True,\n",
    "                                                        return_state=True,\n",
    "                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                        name='Answer_GRU_Layer'+str(i)\n",
    "                                                       )\n",
    "                                              )\n",
    "            self.batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        \n",
    "        self.answer_decoder_dense = layers.Dense(vocab_size+1,name='Answer_output')\n",
    "        \n",
    "    \n",
    "    def call(self,\n",
    "             target_input,\n",
    "             answer_hidden,\n",
    "             question,\n",
    "             context,\n",
    "             episodic_memory):\n",
    "        attention_output,attention_weights = self.attention_layer(answer_hidden,context)\n",
    "        answer_embeddings = self.answer_embedding_layer(target_input)\n",
    "        answer_concat = tf.concat([tf.expand_dims(attention_output, 1),\n",
    "                                   tf.expand_dims(question, 1),\n",
    "                                   answer_embeddings], axis=-1)\n",
    "        for i in range(len(self.answer_output_layers)):\n",
    "            if ((episodic_memory != None) and (i==0)):\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_concat,\n",
    "                                                                           initial_state=episodic_memory)\n",
    "            elif i==0:\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_concat)\n",
    "            else:\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_outputs)\n",
    "            \n",
    "            answer_outputs   = self.batch_normalization_layers[i](answer_outputs)\n",
    "        answer_outputs = tf.reshape(answer_outputs, (-1, answer_outputs.shape[2]))\n",
    "\n",
    "        answer_decoder_outputs = self.answer_decoder_dense(answer_outputs)\n",
    "        return answer_decoder_outputs,hidden_state,attention_weights\n",
    "\n",
    "#Encoder Module which combines the context,question into episodic memory and emits context outputs and \n",
    "#question outputs\n",
    "class encoder_module(tf.keras.Model):    \n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      max_context_length,\n",
    "                      max_question_length,\n",
    "                      max_answer_length,\n",
    "                      num_unit_gru = 64,\n",
    "                      num_layers_gru = 2,\n",
    "                      ndim =100,\n",
    "                      num_episodes = 2,\n",
    "                      dropout_rate = 0.5,\n",
    "                      num_episodic_network_unit = 64\n",
    "                      ):\n",
    "        super(encoder_module, self).__init__()\n",
    "        #Context Module\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers_gru = num_layers_gru\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ndim = ndim\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_question_length = max_question_length\n",
    "        self.max_answer_length = max_answer_length\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_episodic_network_unit = num_episodic_network_unit\n",
    "        self.context_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                         self.ndim,\n",
    "                                                         mask_zero=True,\n",
    "                                                         weights =[self.embedding_matrix],\n",
    "                                                         trainable = False,\n",
    "                                                         name='Context_Embedding')\n",
    "        self.context_output_layers = []\n",
    "        self.context_batch_normalization_layers = []\n",
    "        for i in range(self.num_layers_gru):\n",
    "            self.context_output_layers.append(layers.Bidirectional(layers.GRU(self.num_unit_gru,\n",
    "                                                                               dropout=self.dropout_rate,\n",
    "                                                                               recurrent_dropout= self.dropout_rate,\n",
    "                                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                                               return_sequences=True),\n",
    "                                                                   merge_mode='sum',\n",
    "                                                                   name='Context_Bid_Layer'+str(i))\n",
    "                                         )\n",
    "            self.context_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        #Question Module\n",
    "        self.question_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                          self.ndim,\n",
    "                                                          mask_zero=True,\n",
    "                                                          weights =[self.embedding_matrix],\n",
    "                                                          trainable = False,\n",
    "                                                          name='Question_Embedding')\n",
    "          \n",
    "        self.question_output_layers = []\n",
    "        self.question_batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "\n",
    "            if i==0 and num_layers_gru >1:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=True),\n",
    "                                                             merge_mode='sum',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            elif i==0 and num_layers_gru ==1:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=False),\n",
    "                                                             merge_mode='sum',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                             )\n",
    "            elif i==(num_layers_gru-1):\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=False),\n",
    "                                                              merge_mode='sum',\n",
    "                                                              name='Question_Bid_Layer'+str(i))\n",
    "                                             )\n",
    "            else:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=True),\n",
    "                                                                        merge_mode='sum',\n",
    "                                                                        name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.question_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        #Episodic Memory \n",
    "        self.episodic_weight_layer = layers.Dense(self.num_unit_gru,use_bias=False)\n",
    "        self.episodic_tanh_layer = layers.Dense(self.num_episodic_network_unit,activation='tanh')\n",
    "        self.episodic_score_layer = layers.Dense(1)\n",
    "        \n",
    "    def call(self,context_input,question_input):\n",
    "        #context Module\n",
    "        context_embeddings = self.context_embeddings_layer(context_input)\n",
    "        for i in range(len(self.context_output_layers)):\n",
    "            if i==0:\n",
    "                context_outputs = self.context_output_layers[i](context_embeddings)\n",
    "            else:\n",
    "                context_outputs = self.context_output_layers[i](context_outputs)\n",
    "            context_outputs = self.context_batch_normalization_layers[i](context_outputs)\n",
    "\n",
    "        #Question Module\n",
    "        question_embeddings = self.question_embeddings_layer(question_input)\n",
    "        for i in range(len(self.question_output_layers)):\n",
    "            if i==0:\n",
    "                question_outputs = self.question_output_layers[i](question_embeddings)\n",
    "            else:\n",
    "                question_outputs = self.question_output_layers[i](question_outputs)\n",
    "            question_outputs = self.question_batch_normalization_layers[i](question_outputs) \n",
    "        #Episodic Memory \n",
    "        m = tf.identity(question_outputs)\n",
    "        for i in range(self.num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(question_outputs,1),\n",
    "                                  tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(question_outputs),1), \n",
    "                                context_outputs,\n",
    "                                transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(m),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = tf.concat([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = self.episodic_score_layer(self.episodic_tanh_layer(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        #concatenate episodic memory with question\n",
    "        concatenated_tensor = tf.concat(values=[m,question_outputs],axis=1)\n",
    "        return (m,concatenated_tensor,question_outputs,context_outputs)\n",
    "    \n",
    "                \n",
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64):\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        weight_layer = layers.Dense(query.shape[1],use_bias=False)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(weight_layer(query),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(weight_layer(m),1), context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    \"\"\"\n",
    "    #encoder Model\n",
    "    encoder_model = encoder_module(embedding_matrix,\n",
    "                                   vocab_size,\n",
    "                                   max_context_length,\n",
    "                                   max_question_length,\n",
    "                                   max_answer_length,\n",
    "                                   num_unit_gru,\n",
    "                                   num_layers_gru,\n",
    "                                   ndim,\n",
    "                                   num_episodes,\n",
    "                                   dropout_rate,\n",
    "                                   num_episodic_network_unit\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    #Model([context_input,question_input], [m,concatenated_tensor,question_outputs,context_outputs])\n",
    "    #answer_module\n",
    "    answer_model = answer_module(embedding_matrix,\n",
    "                                 vocab_size,\n",
    "                                 ndim,\n",
    "                                 num_unit_gru,\n",
    "                                 num_layers_gru,\n",
    "                                 dropout_rate)\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    feasibility_input = Input(shape=(2*num_unit_gru,), name=\"FeasibilityInput\")\n",
    "    feasibility_context_input = Input(shape=(None,num_unit_gru,),name='feasibilityContext_Input')\n",
    "    feasibility_question_input = Input(shape=(num_unit_gru,),name='feasibilityQuestion_Input')\n",
    "    for i in range(num_dense_layers_feasibility):\n",
    "        #create attention between Context and Question\n",
    "        q_with_time_axis = tf.keras.backend.expand_dims(feasibility_question_input,1)\n",
    "        attentionContextQuestion = layers.AdditiveAttention()([q_with_time_axis,\n",
    "                                                               feasibility_context_input])\n",
    "        attentionContextQuestionReduced = tf.keras.backend.sum(attentionContextQuestion, axis=1)\n",
    "        feasibility_dense_input = tf.concat([feasibility_input,\n",
    "                                             attentionContextQuestionReduced],\n",
    "                                            axis=-1)\n",
    "        \n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i))(feasibility_dense_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i))(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',name='feasibility_output')(dropout_layer)\n",
    "    feasibility_model = Model([feasibility_input,feasibility_context_input,feasibility_question_input],\n",
    "                              feasibility_output)\n",
    "    \n",
    "    return (answer_model,encoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sentence(context_input_seq,\n",
    "                     question_input_seq,\n",
    "                     encoder_model,\n",
    "                     answer_model):\n",
    "    decoded_sentence = ''\n",
    "    episodic_memory,\\\n",
    "    concatenated_tensor,\\\n",
    "    question_output,\\\n",
    "    context_output = encoder_model(context_input_seq,question_input_seq) \n",
    "    answer_hidden = question_output\n",
    "    dec_input = tf.expand_dims([vocab[\"<s>\"]], 0)\n",
    "\n",
    "    for t in range(answer_maxlen):\n",
    "        predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                     answer_hidden,\n",
    "                                                     question_output,\n",
    "                                                     context_output,\n",
    "                                                     episodic_memory)\n",
    "        sampled_token_index = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \" \"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "        if id_vocab[sampled_token_index] == '</s>':\n",
    "            return decoded_sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([sampled_token_index], 0)\n",
    "        episodic_memory = None\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.001},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment2': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment4': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 128,\n",
       "  'learning_rate': 0.005},\n",
       " 'Experiment5': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 64, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.6, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 192, 'learning_rate': 0.001}, 'Experiment1': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 64, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 3, 'num_episodic_network_unit': 192, 'learning_rate': 0.005}, 'Experiment2': {'num_unit_gru': 100, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 64, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 64, 'learning_rate': 0.005}, 'Experiment3': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 32, 'dropout_rate': 0.7, 'num_dense_layers_feasibility': 3, 'num_episodic_network_unit': 192, 'learning_rate': 0.005}, 'Experiment4': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 2, 'num_dense_layer_feasibility_units': 48, 'dropout_rate': 0.6, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 128, 'learning_rate': 0.005}, 'Experiment5': {'num_unit_gru': 80, 'num_layers_gru': 2, 'num_episodes': 3, 'num_dense_layer_feasibility_units': 64, 'dropout_rate': 0.5, 'num_dense_layers_feasibility': 1, 'num_episodic_network_unit': 192, 'learning_rate': 0.005}}\n",
    "\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "        \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        #encoder_model.compile(optimizer=adam_optim,\n",
    "        #                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        #encoder_model.summary()\n",
    "        #answer_model.compile(optimizer=adam_optim,\n",
    "        #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        #                        )\n",
    "        #\n",
    "        #answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        #feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' +\n",
    "                                                                     os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = \n",
    "                                                        num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            #encoder_model.compile(optimizer=adam_optim,\n",
    "            #                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "            #encoder_model.summary()\n",
    "            #\n",
    "            #answer_model.compile(optimizer=adam_optim,\n",
    "            #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            #                    )\n",
    "\n",
    "            #answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            #feasibility_model.summary()\n",
    "    #Train the Answer Model and Encoder Model\n",
    "    answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    answer_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    @tf.function\n",
    "    def answer_loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = answer_loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def answer_train_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "            answer_hidden = question_output\n",
    "            dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                             answer_hidden,\n",
    "                                                             question_output,\n",
    "                                                             context_output,\n",
    "                                                             episodic_memory)\n",
    "                loss += answer_loss_function(targ[:, t], predictions)\n",
    "                train_acc_metric(targ[:, t],predictions)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                episodic_memory = None\n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def answer_val_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "        answer_hidden = question_output\n",
    "        dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                         answer_hidden,\n",
    "                                                         question_output,\n",
    "                                                         context_output,\n",
    "                                                         episodic_memory)\n",
    "            loss += answer_loss_function(targ[:, t], predictions)\n",
    "            val_acc_metric(targ[:, t],predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            episodic_memory = None\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        return batch_loss\n",
    "    #Create batches for training\n",
    "    \n",
    "    TRAIN_BUFFER_SIZE = train_context_padded_seq[:num_training_samples].shape[0]\n",
    "    VAL_BUFFER_SIZE = val_context_padded_seq[:num_validation_samples].shape[0]\n",
    "    steps_per_epoch = TRAIN_BUFFER_SIZE//batch_size\n",
    "    steps_per_epoch_val = VAL_BUFFER_SIZE//batch_size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_context_padded_seq[:num_training_samples],\n",
    "                                                        train_question_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_input_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_target_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_impossible[:num_training_samples]))\\\n",
    "                                   .shuffle(TRAIN_BUFFER_SIZE)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_context_padded_seq[:num_validation_samples],\n",
    "                                                      val_question_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_input_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_target_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_impossible[:num_validation_samples]))\n",
    "    val_dataset = val_dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    #Run Epochs\n",
    "    \n",
    "    history_answer_model = {'loss':[],\n",
    "                            'sparse_categorical_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_sparse_categorical_accuracy':[]}\n",
    "\n",
    "    print(\"Training the answer model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "            batch_loss = answer_train_step(batch_train_context_padded_seq,\n",
    "                                           batch_train_question_seq_padded,\n",
    "                                           batch_train_answer_input_seq_padded,\n",
    "                                           encoder_model,\n",
    "                                           answer_model)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 50 == 0:\n",
    "                print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = train_acc_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        print('')\n",
    "        print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible)) in enumerate(val_dataset.take(steps_per_epoch_val)):\n",
    "            batch_loss = answer_val_step(batch_val_context_padded_seq,\n",
    "                                         batch_val_question_seq_padded,\n",
    "                                         batch_val_answer_input_seq_padded,\n",
    "                                         encoder_model,\n",
    "                                         answer_model)\n",
    "            total_val_loss += batch_loss\n",
    "            if batch % 50 == 0:\n",
    "                print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print('')\n",
    "        print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_answer_model['loss'].append(epoch_training_loss)\n",
    "        history_answer_model['sparse_categorical_accuracy'].append(epoch_training_accuracy)\n",
    "        history_answer_model['val_loss'].append(epoch_val_loss)\n",
    "        history_answer_model['val_sparse_categorical_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    \n",
    "    answer_model.save_weights(ExperimentNo+'attention_model_answer_model.h5')\n",
    "    encoder_model.save_weights(ExperimentNo+'attention_model_encoder_model.h5')\n",
    "    with open(ExperimentNo+'attention_model_'+'history_answer_model', 'wb') as file_history:\n",
    "        pickle.dump(history_answer_model, file_history)\n",
    "        \n",
    "    \n",
    "    #Train the Feasibility Model\n",
    "    _,encoder_prediction,encoder_prediction_question,\\\n",
    "    encoder_prediction_contexts = encoder_model(train_context_padded_seq[:num_training_samples],\n",
    "                                             train_question_seq_padded[:num_training_samples])\n",
    "    _,encoder_validation_prediction,encoder_validation_prediction_question,\\\n",
    "    encoder_validation_prediction_contexts = encoder_model(val_context_padded_seq[:num_validation_samples],\n",
    "                                                         val_question_seq_padded[:num_validation_samples])\n",
    "    print(\"training the feasibility model\")\n",
    "    history_feasibility_model = feasibility_model.fit([encoder_prediction,\n",
    "                                                       encoder_prediction_contexts,\n",
    "                                                       encoder_prediction_question],\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            ([encoder_validation_prediction,\n",
    "                                                              encoder_validation_prediction_contexts,\n",
    "                                                              encoder_validation_prediction_question],\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "\n",
    "    feasibility_model.save(ExperimentNo+'attention_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'attention_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model.history, file_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    \n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit)\n",
    "\n",
    "    # train on 1 row so that weights can be loaded \n",
    "    #Train the Answer Model and Encoder Model\n",
    "    inference_answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    inference_answer_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    @tf.function\n",
    "    def inference_answer_loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = inference_answer_loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def inference_answer_train_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "            answer_hidden = question_output\n",
    "            dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                             answer_hidden,\n",
    "                                                             question_output,\n",
    "                                                             context_output,\n",
    "                                                             episodic_memory)\n",
    "                loss += inference_answer_loss_function(targ[:, t], predictions)\n",
    "                #train_acc_metric(targ[:, t],predictions)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                episodic_memory = None\n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            inference_answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "    inference_answer_train_step(train_context_padded_seq[:1],\n",
    "                              train_question_seq_padded[:1],\n",
    "                              train_answer_input_seq_padded[:1],\n",
    "                              inference_encoder_model,\n",
    "                              inference_answer_model)\n",
    "        \n",
    "    inference_answer_model.load_weights(ExperimentNo+'attention_model_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'attention_model_encoder_model.h5')\n",
    "    inference_feasibility_model.load_weights(ExperimentNo+'attention_model_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Training the answer model:\n",
      "=t=t=t\n",
      "Epoch 1 Train Loss 2.1647 Epoch 1 Train Accuracy 0.0872\n",
      "=v\n",
      "Epoch 1 Validation Loss 1.7500 Epoch 1 Validation Accuracy 0.0864\n",
      "Time taken for epoch 156.9231436252594 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 2 Train Loss 1.5015 Epoch 2 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 2 Validation Loss 1.7731 Epoch 2 Validation Accuracy 0.0864\n",
      "Time taken for epoch 107.53140211105347 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 3 Train Loss 1.4363 Epoch 3 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 3 Validation Loss 1.8048 Epoch 3 Validation Accuracy 0.0864\n",
      "Time taken for epoch 107.27254605293274 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 4 Train Loss 1.4088 Epoch 4 Train Accuracy 0.0884\n",
      "=v\n",
      "Epoch 4 Validation Loss 1.8026 Epoch 4 Validation Accuracy 0.0864\n",
      "Time taken for epoch 107.33821153640747 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 5 Train Loss 1.3901 Epoch 5 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 5 Validation Loss 1.8139 Epoch 5 Validation Accuracy 0.0864\n",
      "Time taken for epoch 107.60775184631348 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 6 Train Loss 1.3715 Epoch 6 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 6 Validation Loss 1.8247 Epoch 6 Validation Accuracy 0.0864\n",
      "Time taken for epoch 107.0998899936676 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 7 Train Loss 1.3561 Epoch 7 Train Accuracy 0.0887\n",
      "=v\n",
      "Epoch 7 Validation Loss 1.8541 Epoch 7 Validation Accuracy 0.0886\n",
      "Time taken for epoch 106.78801369667053 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 8 Train Loss 1.3390 Epoch 8 Train Accuracy 0.0906\n",
      "=v\n",
      "Epoch 8 Validation Loss 1.8439 Epoch 8 Validation Accuracy 0.0886\n",
      "Time taken for epoch 106.36439633369446 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 9 Train Loss 1.3158 Epoch 9 Train Accuracy 0.0912\n",
      "=v\n",
      "Epoch 9 Validation Loss 1.8500 Epoch 9 Validation Accuracy 0.0890\n",
      "Time taken for epoch 106.95854043960571 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 10 Train Loss 1.2929 Epoch 10 Train Accuracy 0.0915\n",
      "=v\n",
      "Epoch 10 Validation Loss 1.8534 Epoch 10 Validation Accuracy 0.0883\n",
      "Time taken for epoch 111.97452235221863 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 11 Train Loss 1.2734 Epoch 11 Train Accuracy 0.0915\n",
      "=v\n",
      "Epoch 11 Validation Loss 1.8669 Epoch 11 Validation Accuracy 0.0883\n",
      "Time taken for epoch 110.08380246162415 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 12 Train Loss 1.2545 Epoch 12 Train Accuracy 0.0917\n",
      "=v\n",
      "Epoch 12 Validation Loss 1.9065 Epoch 12 Validation Accuracy 0.0856\n",
      "Time taken for epoch 108.67161178588867 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 13 Train Loss 1.2330 Epoch 13 Train Accuracy 0.0919\n",
      "=v\n",
      "Epoch 13 Validation Loss 1.9284 Epoch 13 Validation Accuracy 0.0883\n",
      "Time taken for epoch 106.32290697097778 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 14 Train Loss 1.2107 Epoch 14 Train Accuracy 0.0923\n",
      "=v\n",
      "Epoch 14 Validation Loss 1.9491 Epoch 14 Validation Accuracy 0.0860\n",
      "Time taken for epoch 106.00616002082825 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 15 Train Loss 1.1910 Epoch 15 Train Accuracy 0.0923\n",
      "=v\n",
      "Epoch 15 Validation Loss 1.9823 Epoch 15 Validation Accuracy 0.0845\n",
      "Time taken for epoch 106.12448692321777 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 16 Train Loss 1.1676 Epoch 16 Train Accuracy 0.0929\n",
      "=v\n",
      "Epoch 16 Validation Loss 2.0196 Epoch 16 Validation Accuracy 0.0875\n",
      "Time taken for epoch 105.98583030700684 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 17 Train Loss 1.1486 Epoch 17 Train Accuracy 0.0932\n",
      "=v\n",
      "Epoch 17 Validation Loss 2.0649 Epoch 17 Validation Accuracy 0.0864\n",
      "Time taken for epoch 105.9716386795044 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 18 Train Loss 1.1271 Epoch 18 Train Accuracy 0.0937\n",
      "=v\n",
      "Epoch 18 Validation Loss 2.1088 Epoch 18 Validation Accuracy 0.0864\n",
      "Time taken for epoch 106.57489514350891 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 19 Train Loss 1.1054 Epoch 19 Train Accuracy 0.0942\n",
      "=v\n",
      "Epoch 19 Validation Loss 2.1178 Epoch 19 Validation Accuracy 0.0848\n",
      "Time taken for epoch 106.1445837020874 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 20 Train Loss 1.0830 Epoch 20 Train Accuracy 0.0949\n",
      "=v\n",
      "Epoch 20 Validation Loss 2.1391 Epoch 20 Validation Accuracy 0.0799\n",
      "Time taken for epoch 105.68716287612915 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 21 Train Loss 1.0611 Epoch 21 Train Accuracy 0.0953\n",
      "=v\n",
      "Epoch 21 Validation Loss 2.1981 Epoch 21 Validation Accuracy 0.0830\n",
      "Time taken for epoch 106.1263747215271 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 22 Train Loss 1.0389 Epoch 22 Train Accuracy 0.0966\n",
      "=v\n",
      "Epoch 22 Validation Loss 2.1996 Epoch 22 Validation Accuracy 0.0792\n",
      "Time taken for epoch 106.15237331390381 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 23 Train Loss 1.0089 Epoch 23 Train Accuracy 0.0973\n",
      "=v\n",
      "Epoch 23 Validation Loss 2.2510 Epoch 23 Validation Accuracy 0.0780\n",
      "Time taken for epoch 106.05437064170837 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 24 Train Loss 0.9775 Epoch 24 Train Accuracy 0.0994\n",
      "=v\n",
      "Epoch 24 Validation Loss 2.2975 Epoch 24 Validation Accuracy 0.0758\n",
      "Time taken for epoch 105.91096520423889 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 25 Train Loss 0.9536 Epoch 25 Train Accuracy 0.1012\n",
      "=v\n",
      "Epoch 25 Validation Loss 2.3105 Epoch 25 Validation Accuracy 0.0670\n",
      "Time taken for epoch 106.42235922813416 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 26 Train Loss 0.9269 Epoch 26 Train Accuracy 0.1020\n",
      "=v\n",
      "Epoch 26 Validation Loss 2.4188 Epoch 26 Validation Accuracy 0.0746\n",
      "Time taken for epoch 105.96051669120789 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 27 Train Loss 0.9026 Epoch 27 Train Accuracy 0.1032\n",
      "=v\n",
      "Epoch 27 Validation Loss 2.4207 Epoch 27 Validation Accuracy 0.0716\n",
      "Time taken for epoch 106.92897844314575 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 28 Train Loss 0.8682 Epoch 28 Train Accuracy 0.1047\n",
      "=v\n",
      "Epoch 28 Validation Loss 2.4347 Epoch 28 Validation Accuracy 0.0670\n",
      "Time taken for epoch 106.51466751098633 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 29 Train Loss 0.8441 Epoch 29 Train Accuracy 0.1061\n",
      "=v\n",
      "Epoch 29 Validation Loss 2.5058 Epoch 29 Validation Accuracy 0.0636\n",
      "Time taken for epoch 106.48214530944824 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 30 Train Loss 0.8120 Epoch 30 Train Accuracy 0.1077\n",
      "=v\n",
      "Epoch 30 Validation Loss 2.5759 Epoch 30 Validation Accuracy 0.0614\n",
      "Time taken for epoch 106.31413435935974 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 31 Train Loss 0.7762 Epoch 31 Train Accuracy 0.1100\n",
      "=v\n",
      "Epoch 31 Validation Loss 2.6351 Epoch 31 Validation Accuracy 0.0614\n",
      "Time taken for epoch 107.3633086681366 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 32 Train Loss 0.7400 Epoch 32 Train Accuracy 0.1134\n",
      "=v\n",
      "Epoch 32 Validation Loss 2.7092 Epoch 32 Validation Accuracy 0.0644\n",
      "Time taken for epoch 106.55436491966248 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 33 Train Loss 0.7028 Epoch 33 Train Accuracy 0.1188\n",
      "=v\n",
      "Epoch 33 Validation Loss 2.7686 Epoch 33 Validation Accuracy 0.0663\n",
      "Time taken for epoch 107.36260414123535 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 34 Train Loss 0.6659 Epoch 34 Train Accuracy 0.1225\n",
      "=v\n",
      "Epoch 34 Validation Loss 2.7673 Epoch 34 Validation Accuracy 0.0602\n",
      "Time taken for epoch 106.67829918861389 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 35 Train Loss 0.6336 Epoch 35 Train Accuracy 0.1282\n",
      "=v\n",
      "Epoch 35 Validation Loss 2.8506 Epoch 35 Validation Accuracy 0.0598\n",
      "Time taken for epoch 106.82231783866882 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 36 Train Loss 0.5993 Epoch 36 Train Accuracy 0.1344\n",
      "=v\n",
      "Epoch 36 Validation Loss 2.8868 Epoch 36 Validation Accuracy 0.0564\n",
      "Time taken for epoch 107.41762089729309 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 37 Train Loss 0.5669 Epoch 37 Train Accuracy 0.1393\n",
      "=v\n",
      "Epoch 37 Validation Loss 2.9671 Epoch 37 Validation Accuracy 0.0591\n",
      "Time taken for epoch 105.44974660873413 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 38 Train Loss 0.5230 Epoch 38 Train Accuracy 0.1475\n",
      "=v\n",
      "Epoch 38 Validation Loss 3.0383 Epoch 38 Validation Accuracy 0.0530\n",
      "Time taken for epoch 105.50444555282593 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 39 Train Loss 0.4867 Epoch 39 Train Accuracy 0.1567\n",
      "=v\n",
      "Epoch 39 Validation Loss 3.1374 Epoch 39 Validation Accuracy 0.0576\n",
      "Time taken for epoch 104.54937386512756 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 40 Train Loss 0.4553 Epoch 40 Train Accuracy 0.1634\n",
      "=v\n",
      "Epoch 40 Validation Loss 3.2004 Epoch 40 Validation Accuracy 0.0534\n",
      "Time taken for epoch 107.4300320148468 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 41 Train Loss 0.4162 Epoch 41 Train Accuracy 0.1721\n",
      "=v\n",
      "Epoch 41 Validation Loss 3.2569 Epoch 41 Validation Accuracy 0.0515\n",
      "Time taken for epoch 108.97812938690186 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 42 Train Loss 0.3843 Epoch 42 Train Accuracy 0.1821\n",
      "=v\n",
      "Epoch 42 Validation Loss 3.3543 Epoch 42 Validation Accuracy 0.0538\n",
      "Time taken for epoch 115.73930597305298 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 43 Train Loss 0.3509 Epoch 43 Train Accuracy 0.1899\n",
      "=v\n",
      "Epoch 43 Validation Loss 3.4204 Epoch 43 Validation Accuracy 0.0545\n",
      "Time taken for epoch 105.87962460517883 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 44 Train Loss 0.3212 Epoch 44 Train Accuracy 0.1980\n",
      "=v\n",
      "Epoch 44 Validation Loss 3.4793 Epoch 44 Validation Accuracy 0.0538\n",
      "Time taken for epoch 110.79703712463379 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 45 Train Loss 0.2929 Epoch 45 Train Accuracy 0.2045\n",
      "=v\n",
      "Epoch 45 Validation Loss 3.5432 Epoch 45 Validation Accuracy 0.0561\n",
      "Time taken for epoch 114.01576542854309 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=t=t=t\n",
      "Epoch 46 Train Loss 0.2653 Epoch 46 Train Accuracy 0.2115\n",
      "=v\n",
      "Epoch 46 Validation Loss 3.6481 Epoch 46 Validation Accuracy 0.0542\n",
      "Time taken for epoch 119.85014057159424 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 47 Train Loss 0.2411 Epoch 47 Train Accuracy 0.2199\n",
      "=v\n",
      "Epoch 47 Validation Loss 3.6535 Epoch 47 Validation Accuracy 0.0515\n",
      "Time taken for epoch 108.56740665435791 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 48 Train Loss 0.2207 Epoch 48 Train Accuracy 0.2253\n",
      "=v\n",
      "Epoch 48 Validation Loss 3.7269 Epoch 48 Validation Accuracy 0.0515\n",
      "Time taken for epoch 112.07529807090759 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 49 Train Loss 0.2009 Epoch 49 Train Accuracy 0.2300\n",
      "=v\n",
      "Epoch 49 Validation Loss 3.8305 Epoch 49 Validation Accuracy 0.0542\n",
      "Time taken for epoch 112.83595490455627 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 50 Train Loss 0.1779 Epoch 50 Train Accuracy 0.2365\n",
      "=v\n",
      "Epoch 50 Validation Loss 3.8433 Epoch 50 Validation Accuracy 0.0485\n",
      "Time taken for epoch 117.70112442970276 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 51 Train Loss 0.1551 Epoch 51 Train Accuracy 0.2422\n",
      "=v\n",
      "Epoch 51 Validation Loss 3.9693 Epoch 51 Validation Accuracy 0.0545\n",
      "Time taken for epoch 115.27434921264648 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 52 Train Loss 0.1396 Epoch 52 Train Accuracy 0.2458\n",
      "=v\n",
      "Epoch 52 Validation Loss 4.0008 Epoch 52 Validation Accuracy 0.0545\n",
      "Time taken for epoch 113.57644367218018 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 53 Train Loss 0.1273 Epoch 53 Train Accuracy 0.2490\n",
      "=v\n",
      "Epoch 53 Validation Loss 4.0918 Epoch 53 Validation Accuracy 0.0557\n",
      "Time taken for epoch 119.21459460258484 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 54 Train Loss 0.1159 Epoch 54 Train Accuracy 0.2517\n",
      "=v\n",
      "Epoch 54 Validation Loss 4.1254 Epoch 54 Validation Accuracy 0.0527\n",
      "Time taken for epoch 108.18561816215515 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 55 Train Loss 0.1036 Epoch 55 Train Accuracy 0.2537\n",
      "=v\n",
      "Epoch 55 Validation Loss 4.1970 Epoch 55 Validation Accuracy 0.0545\n",
      "Time taken for epoch 106.88483810424805 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 56 Train Loss 0.0923 Epoch 56 Train Accuracy 0.2569\n",
      "=v\n",
      "Epoch 56 Validation Loss 4.2504 Epoch 56 Validation Accuracy 0.0538\n",
      "Time taken for epoch 106.90160369873047 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 57 Train Loss 0.0861 Epoch 57 Train Accuracy 0.2586\n",
      "=v\n",
      "Epoch 57 Validation Loss 4.1816 Epoch 57 Validation Accuracy 0.0451\n",
      "Time taken for epoch 106.49645113945007 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 58 Train Loss 0.0767 Epoch 58 Train Accuracy 0.2600\n",
      "=v\n",
      "Epoch 58 Validation Loss 4.3548 Epoch 58 Validation Accuracy 0.0496\n",
      "Time taken for epoch 106.50984025001526 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 59 Train Loss 0.0694 Epoch 59 Train Accuracy 0.2606\n",
      "=v\n",
      "Epoch 59 Validation Loss 4.3584 Epoch 59 Validation Accuracy 0.0496\n",
      "Time taken for epoch 107.76016092300415 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 60 Train Loss 0.0631 Epoch 60 Train Accuracy 0.2630\n",
      "=v\n",
      "Epoch 60 Validation Loss 4.3700 Epoch 60 Validation Accuracy 0.0489\n",
      "Time taken for epoch 106.31611251831055 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 61 Train Loss 0.0534 Epoch 61 Train Accuracy 0.2645\n",
      "=v\n",
      "Epoch 61 Validation Loss 4.4336 Epoch 61 Validation Accuracy 0.0489\n",
      "Time taken for epoch 106.08399653434753 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 62 Train Loss 0.0497 Epoch 62 Train Accuracy 0.2653\n",
      "=v\n",
      "Epoch 62 Validation Loss 4.5035 Epoch 62 Validation Accuracy 0.0492\n",
      "Time taken for epoch 106.16034436225891 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 63 Train Loss 0.0446 Epoch 63 Train Accuracy 0.2666\n",
      "=v\n",
      "Epoch 63 Validation Loss 4.5763 Epoch 63 Validation Accuracy 0.0504\n",
      "Time taken for epoch 106.0985369682312 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 64 Train Loss 0.0404 Epoch 64 Train Accuracy 0.2664\n",
      "=v\n",
      "Epoch 64 Validation Loss 4.6003 Epoch 64 Validation Accuracy 0.0462\n",
      "Time taken for epoch 106.28183650970459 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 65 Train Loss 0.0389 Epoch 65 Train Accuracy 0.2671\n",
      "=v\n",
      "Epoch 65 Validation Loss 4.6580 Epoch 65 Validation Accuracy 0.0504\n",
      "Time taken for epoch 105.92915201187134 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 66 Train Loss 0.0456 Epoch 66 Train Accuracy 0.2656\n",
      "=v\n",
      "Epoch 66 Validation Loss 4.6950 Epoch 66 Validation Accuracy 0.0492\n",
      "Time taken for epoch 103.71807432174683 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 67 Train Loss 0.0559 Epoch 67 Train Accuracy 0.2617\n",
      "=v\n",
      "Epoch 67 Validation Loss 4.6618 Epoch 67 Validation Accuracy 0.0489\n",
      "Time taken for epoch 103.62309646606445 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 68 Train Loss 0.0695 Epoch 68 Train Accuracy 0.2563\n",
      "=v\n",
      "Epoch 68 Validation Loss 4.8062 Epoch 68 Validation Accuracy 0.0511\n",
      "Time taken for epoch 103.63151025772095 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 69 Train Loss 0.0733 Epoch 69 Train Accuracy 0.2554\n",
      "=v\n",
      "Epoch 69 Validation Loss 4.6924 Epoch 69 Validation Accuracy 0.0496\n",
      "Time taken for epoch 103.56810164451599 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 70 Train Loss 0.0586 Epoch 70 Train Accuracy 0.2600\n",
      "=v\n",
      "Epoch 70 Validation Loss 4.7890 Epoch 70 Validation Accuracy 0.0489\n",
      "Time taken for epoch 103.70518207550049 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 71 Train Loss 0.0466 Epoch 71 Train Accuracy 0.2642\n",
      "=v\n",
      "Epoch 71 Validation Loss 4.7900 Epoch 71 Validation Accuracy 0.0489\n",
      "Time taken for epoch 103.69205904006958 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 72 Train Loss 0.0338 Epoch 72 Train Accuracy 0.2665\n",
      "=v\n",
      "Epoch 72 Validation Loss 4.8375 Epoch 72 Validation Accuracy 0.0462\n",
      "Time taken for epoch 103.83210325241089 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 73 Train Loss 0.0261 Epoch 73 Train Accuracy 0.2682\n",
      "=v\n",
      "Epoch 73 Validation Loss 4.8356 Epoch 73 Validation Accuracy 0.0496\n",
      "Time taken for epoch 103.66900420188904 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 74 Train Loss 0.0207 Epoch 74 Train Accuracy 0.2698\n",
      "=v\n",
      "Epoch 74 Validation Loss 4.9020 Epoch 74 Validation Accuracy 0.0511\n",
      "Time taken for epoch 103.5521559715271 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 75 Train Loss 0.0178 Epoch 75 Train Accuracy 0.2706\n",
      "=v\n",
      "Epoch 75 Validation Loss 4.8765 Epoch 75 Validation Accuracy 0.0500\n",
      "Time taken for epoch 103.50602173805237 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 76 Train Loss 0.0143 Epoch 76 Train Accuracy 0.2708\n",
      "=v\n",
      "Epoch 76 Validation Loss 4.9417 Epoch 76 Validation Accuracy 0.0492\n",
      "Time taken for epoch 103.57575798034668 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 77 Train Loss 0.0122 Epoch 77 Train Accuracy 0.2712\n",
      "=v\n",
      "Epoch 77 Validation Loss 4.9808 Epoch 77 Validation Accuracy 0.0519\n",
      "Time taken for epoch 103.68149280548096 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 78 Train Loss 0.0115 Epoch 78 Train Accuracy 0.2709\n",
      "=v\n",
      "Epoch 78 Validation Loss 4.9967 Epoch 78 Validation Accuracy 0.0496\n",
      "Time taken for epoch 103.85422396659851 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 79 Train Loss 0.0107 Epoch 79 Train Accuracy 0.2713\n",
      "=v\n",
      "Epoch 79 Validation Loss 5.0061 Epoch 79 Validation Accuracy 0.0496\n",
      "Time taken for epoch 103.69149994850159 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 80 Train Loss 0.0100 Epoch 80 Train Accuracy 0.2707\n",
      "=v\n",
      "Epoch 80 Validation Loss 5.0334 Epoch 80 Validation Accuracy 0.0508\n",
      "Time taken for epoch 103.57117652893066 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 81 Train Loss 0.0098 Epoch 81 Train Accuracy 0.2717\n",
      "=v\n",
      "Epoch 81 Validation Loss 5.0283 Epoch 81 Validation Accuracy 0.0458\n",
      "Time taken for epoch 104.01895546913147 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 82 Train Loss 0.0097 Epoch 82 Train Accuracy 0.2716\n",
      "=v\n",
      "Epoch 82 Validation Loss 5.0198 Epoch 82 Validation Accuracy 0.0481\n",
      "Time taken for epoch 103.59387278556824 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 83 Train Loss 0.0093 Epoch 83 Train Accuracy 0.2714\n",
      "=v\n",
      "Epoch 83 Validation Loss 5.0539 Epoch 83 Validation Accuracy 0.0481\n",
      "Time taken for epoch 103.68790793418884 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 84 Train Loss 0.0095 Epoch 84 Train Accuracy 0.2709\n",
      "=v\n",
      "Epoch 84 Validation Loss 5.0615 Epoch 84 Validation Accuracy 0.0489\n",
      "Time taken for epoch 103.69737958908081 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 85 Train Loss 0.0126 Epoch 85 Train Accuracy 0.2698\n",
      "=v\n",
      "Epoch 85 Validation Loss 5.1295 Epoch 85 Validation Accuracy 0.0530\n",
      "Time taken for epoch 103.8800458908081 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 86 Train Loss 0.0461 Epoch 86 Train Accuracy 0.2603\n",
      "=v\n",
      "Epoch 86 Validation Loss 5.1125 Epoch 86 Validation Accuracy 0.0485\n",
      "Time taken for epoch 103.54835987091064 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 87 Train Loss 0.1699 Epoch 87 Train Accuracy 0.2241\n",
      "=v\n",
      "Epoch 87 Validation Loss 5.0353 Epoch 87 Validation Accuracy 0.0462\n",
      "Time taken for epoch 103.6265287399292 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 88 Train Loss 0.1327 Epoch 88 Train Accuracy 0.2361\n",
      "=v\n",
      "Epoch 88 Validation Loss 4.9888 Epoch 88 Validation Accuracy 0.0458\n",
      "Time taken for epoch 103.61500859260559 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 89 Train Loss 0.0639 Epoch 89 Train Accuracy 0.2551\n",
      "=v\n",
      "Epoch 89 Validation Loss 4.9464 Epoch 89 Validation Accuracy 0.0417\n",
      "Time taken for epoch 103.42874145507812 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 90 Train Loss 0.0276 Epoch 90 Train Accuracy 0.2661\n",
      "=v\n",
      "Epoch 90 Validation Loss 4.9884 Epoch 90 Validation Accuracy 0.0481\n",
      "Time taken for epoch 103.38830733299255 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 91 Train Loss 0.0129 Epoch 91 Train Accuracy 0.2711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=v\n",
      "Epoch 91 Validation Loss 5.0388 Epoch 91 Validation Accuracy 0.0504\n",
      "Time taken for epoch 103.34833335876465 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 92 Train Loss 0.0087 Epoch 92 Train Accuracy 0.2718\n",
      "=v\n",
      "Epoch 92 Validation Loss 5.0319 Epoch 92 Validation Accuracy 0.0485\n",
      "Time taken for epoch 103.50383567810059 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 93 Train Loss 0.0075 Epoch 93 Train Accuracy 0.2714\n",
      "=v\n",
      "Epoch 93 Validation Loss 5.0662 Epoch 93 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.45824098587036 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 94 Train Loss 0.0067 Epoch 94 Train Accuracy 0.2719\n",
      "=v\n",
      "Epoch 94 Validation Loss 5.0666 Epoch 94 Validation Accuracy 0.0492\n",
      "Time taken for epoch 103.44252467155457 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 95 Train Loss 0.0063 Epoch 95 Train Accuracy 0.2720\n",
      "=v\n",
      "Epoch 95 Validation Loss 5.0783 Epoch 95 Validation Accuracy 0.0500\n",
      "Time taken for epoch 103.45304489135742 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 96 Train Loss 0.0059 Epoch 96 Train Accuracy 0.2723\n",
      "=v\n",
      "Epoch 96 Validation Loss 5.0887 Epoch 96 Validation Accuracy 0.0492\n",
      "Time taken for epoch 103.65786814689636 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 97 Train Loss 0.0058 Epoch 97 Train Accuracy 0.2715\n",
      "=v\n",
      "Epoch 97 Validation Loss 5.1120 Epoch 97 Validation Accuracy 0.0508\n",
      "Time taken for epoch 103.48131990432739 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 98 Train Loss 0.0053 Epoch 98 Train Accuracy 0.2719\n",
      "=v\n",
      "Epoch 98 Validation Loss 5.1352 Epoch 98 Validation Accuracy 0.0511\n",
      "Time taken for epoch 103.5479211807251 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 99 Train Loss 0.0051 Epoch 99 Train Accuracy 0.2721\n",
      "=v\n",
      "Epoch 99 Validation Loss 5.1447 Epoch 99 Validation Accuracy 0.0508\n",
      "Time taken for epoch 103.90136480331421 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 100 Train Loss 0.0051 Epoch 100 Train Accuracy 0.2718\n",
      "=v\n",
      "Epoch 100 Validation Loss 5.1454 Epoch 100 Validation Accuracy 0.0523\n",
      "Time taken for epoch 103.5137071609497 sec\n",
      "\n",
      "training the feasibility model\n",
      "Train on 2048 samples, validate on 256 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2048/2048 [==============================] - 1s 583us/sample - loss: 0.5836 - binary_accuracy: 0.7256 - val_loss: 0.8347 - val_binary_accuracy: 0.5938\n",
      "Epoch 2/100\n",
      "2048/2048 [==============================] - 0s 189us/sample - loss: 0.3092 - binary_accuracy: 0.8823 - val_loss: 0.9959 - val_binary_accuracy: 0.5938\n",
      "Epoch 3/100\n",
      "2048/2048 [==============================] - 0s 187us/sample - loss: 0.2460 - binary_accuracy: 0.9111 - val_loss: 1.1509 - val_binary_accuracy: 0.5742\n",
      "Epoch 4/100\n",
      "2048/2048 [==============================] - 0s 179us/sample - loss: 0.1997 - binary_accuracy: 0.9272 - val_loss: 1.2322 - val_binary_accuracy: 0.5781\n",
      "Epoch 5/100\n",
      "2048/2048 [==============================] - 0s 187us/sample - loss: 0.1860 - binary_accuracy: 0.9346 - val_loss: 1.3462 - val_binary_accuracy: 0.5742\n",
      "Epoch 6/100\n",
      "2048/2048 [==============================] - 0s 192us/sample - loss: 0.1685 - binary_accuracy: 0.9409 - val_loss: 1.3540 - val_binary_accuracy: 0.5664\n",
      "Epoch 7/100\n",
      "2048/2048 [==============================] - 0s 192us/sample - loss: 0.1608 - binary_accuracy: 0.9385 - val_loss: 1.5395 - val_binary_accuracy: 0.5703\n",
      "Epoch 8/100\n",
      "2048/2048 [==============================] - 0s 198us/sample - loss: 0.1615 - binary_accuracy: 0.9399 - val_loss: 1.5545 - val_binary_accuracy: 0.5898\n",
      "Epoch 9/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.1458 - binary_accuracy: 0.9497 - val_loss: 1.6291 - val_binary_accuracy: 0.5781\n",
      "Epoch 10/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1534 - binary_accuracy: 0.9453 - val_loss: 1.5880 - val_binary_accuracy: 0.5703\n",
      "Epoch 11/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1466 - binary_accuracy: 0.9458 - val_loss: 1.7065 - val_binary_accuracy: 0.5586\n",
      "Epoch 12/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1418 - binary_accuracy: 0.9429 - val_loss: 1.7822 - val_binary_accuracy: 0.5820\n",
      "Epoch 13/100\n",
      "2048/2048 [==============================] - 0s 198us/sample - loss: 0.1337 - binary_accuracy: 0.9507 - val_loss: 1.8057 - val_binary_accuracy: 0.5898\n",
      "Epoch 14/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1331 - binary_accuracy: 0.9531 - val_loss: 1.6953 - val_binary_accuracy: 0.5703\n",
      "Epoch 15/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1320 - binary_accuracy: 0.9507 - val_loss: 1.7076 - val_binary_accuracy: 0.5742\n",
      "Epoch 16/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.1236 - binary_accuracy: 0.9546 - val_loss: 1.8220 - val_binary_accuracy: 0.5820\n",
      "Epoch 17/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.1136 - binary_accuracy: 0.9604 - val_loss: 1.9928 - val_binary_accuracy: 0.6094\n",
      "Epoch 18/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.1139 - binary_accuracy: 0.9609 - val_loss: 1.7910 - val_binary_accuracy: 0.5859\n",
      "Epoch 19/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.1406 - binary_accuracy: 0.9458 - val_loss: 2.0107 - val_binary_accuracy: 0.5703\n",
      "Epoch 20/100\n",
      "2048/2048 [==============================] - 0s 198us/sample - loss: 0.1089 - binary_accuracy: 0.9590 - val_loss: 1.9058 - val_binary_accuracy: 0.5977\n",
      "Epoch 21/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1298 - binary_accuracy: 0.9565 - val_loss: 1.8867 - val_binary_accuracy: 0.5977\n",
      "Epoch 22/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1003 - binary_accuracy: 0.9639 - val_loss: 1.9545 - val_binary_accuracy: 0.5703\n",
      "Epoch 23/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1199 - binary_accuracy: 0.9619 - val_loss: 1.7441 - val_binary_accuracy: 0.5820\n",
      "Epoch 24/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1049 - binary_accuracy: 0.9624 - val_loss: 2.0308 - val_binary_accuracy: 0.5859\n",
      "Epoch 25/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.1122 - binary_accuracy: 0.9585 - val_loss: 1.8102 - val_binary_accuracy: 0.5703\n",
      "Epoch 26/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.1218 - binary_accuracy: 0.9521 - val_loss: 1.9219 - val_binary_accuracy: 0.5703\n",
      "Epoch 27/100\n",
      "2048/2048 [==============================] - 0s 197us/sample - loss: 0.1128 - binary_accuracy: 0.9604 - val_loss: 2.0063 - val_binary_accuracy: 0.5625\n",
      "Epoch 28/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.1053 - binary_accuracy: 0.9609 - val_loss: 1.9624 - val_binary_accuracy: 0.6016\n",
      "Epoch 29/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1208 - binary_accuracy: 0.9609 - val_loss: 2.1386 - val_binary_accuracy: 0.6133\n",
      "Epoch 30/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0968 - binary_accuracy: 0.9673 - val_loss: 2.3426 - val_binary_accuracy: 0.5938\n",
      "Epoch 31/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1036 - binary_accuracy: 0.9639 - val_loss: 1.9235 - val_binary_accuracy: 0.5820\n",
      "Epoch 32/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.1229 - binary_accuracy: 0.9600 - val_loss: 2.1445 - val_binary_accuracy: 0.5898\n",
      "Epoch 33/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1148 - binary_accuracy: 0.9565 - val_loss: 2.1827 - val_binary_accuracy: 0.5781\n",
      "Epoch 34/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.1081 - binary_accuracy: 0.9648 - val_loss: 2.0971 - val_binary_accuracy: 0.5742\n",
      "Epoch 35/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1040 - binary_accuracy: 0.9678 - val_loss: 2.1515 - val_binary_accuracy: 0.5781\n",
      "Epoch 36/100\n",
      "2048/2048 [==============================] - 0s 165us/sample - loss: 0.0977 - binary_accuracy: 0.9683 - val_loss: 2.0682 - val_binary_accuracy: 0.5742\n",
      "Epoch 37/100\n",
      "2048/2048 [==============================] - 0s 172us/sample - loss: 0.0923 - binary_accuracy: 0.9663 - val_loss: 2.1056 - val_binary_accuracy: 0.5742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "2048/2048 [==============================] - 0s 172us/sample - loss: 0.0868 - binary_accuracy: 0.9673 - val_loss: 2.2130 - val_binary_accuracy: 0.5781\n",
      "Epoch 39/100\n",
      "2048/2048 [==============================] - 0s 173us/sample - loss: 0.1024 - binary_accuracy: 0.9648 - val_loss: 2.1867 - val_binary_accuracy: 0.5742\n",
      "Epoch 40/100\n",
      "2048/2048 [==============================] - 0s 172us/sample - loss: 0.1042 - binary_accuracy: 0.9629 - val_loss: 2.1763 - val_binary_accuracy: 0.5703\n",
      "Epoch 41/100\n",
      "2048/2048 [==============================] - 0s 183us/sample - loss: 0.1092 - binary_accuracy: 0.9658 - val_loss: 2.1642 - val_binary_accuracy: 0.5742\n",
      "Epoch 42/100\n",
      "2048/2048 [==============================] - 0s 188us/sample - loss: 0.1050 - binary_accuracy: 0.9639 - val_loss: 1.9980 - val_binary_accuracy: 0.5859\n",
      "Epoch 43/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0938 - binary_accuracy: 0.9692 - val_loss: 2.3541 - val_binary_accuracy: 0.5859\n",
      "Epoch 44/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.1007 - binary_accuracy: 0.9634 - val_loss: 2.2547 - val_binary_accuracy: 0.5742\n",
      "Epoch 45/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1050 - binary_accuracy: 0.9639 - val_loss: 2.1841 - val_binary_accuracy: 0.5742\n",
      "Epoch 46/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0999 - binary_accuracy: 0.9639 - val_loss: 2.3346 - val_binary_accuracy: 0.5859\n",
      "Epoch 47/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1142 - binary_accuracy: 0.9580 - val_loss: 2.3924 - val_binary_accuracy: 0.5781\n",
      "Epoch 48/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0944 - binary_accuracy: 0.9648 - val_loss: 2.3561 - val_binary_accuracy: 0.5781\n",
      "Epoch 49/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0820 - binary_accuracy: 0.9741 - val_loss: 2.2571 - val_binary_accuracy: 0.5781\n",
      "Epoch 50/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1007 - binary_accuracy: 0.9658 - val_loss: 2.3682 - val_binary_accuracy: 0.5820\n",
      "Epoch 51/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0968 - binary_accuracy: 0.9629 - val_loss: 2.3538 - val_binary_accuracy: 0.5742\n",
      "Epoch 52/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1133 - binary_accuracy: 0.9644 - val_loss: 2.2668 - val_binary_accuracy: 0.5820\n",
      "Epoch 53/100\n",
      "2048/2048 [==============================] - 0s 196us/sample - loss: 0.0809 - binary_accuracy: 0.9707 - val_loss: 2.2388 - val_binary_accuracy: 0.5625\n",
      "Epoch 54/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1074 - binary_accuracy: 0.9668 - val_loss: 2.3358 - val_binary_accuracy: 0.5781\n",
      "Epoch 55/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.1076 - binary_accuracy: 0.9585 - val_loss: 2.2450 - val_binary_accuracy: 0.5742\n",
      "Epoch 56/100\n",
      "2048/2048 [==============================] - 0s 179us/sample - loss: 0.1166 - binary_accuracy: 0.9590 - val_loss: 2.5964 - val_binary_accuracy: 0.5625\n",
      "Epoch 57/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.0851 - binary_accuracy: 0.9746 - val_loss: 2.4169 - val_binary_accuracy: 0.5664\n",
      "Epoch 58/100\n",
      "2048/2048 [==============================] - 0s 185us/sample - loss: 0.0940 - binary_accuracy: 0.9702 - val_loss: 2.3761 - val_binary_accuracy: 0.5742\n",
      "Epoch 59/100\n",
      "2048/2048 [==============================] - 0s 193us/sample - loss: 0.0958 - binary_accuracy: 0.9683 - val_loss: 2.4801 - val_binary_accuracy: 0.5820\n",
      "Epoch 60/100\n",
      "2048/2048 [==============================] - 0s 193us/sample - loss: 0.1038 - binary_accuracy: 0.9688 - val_loss: 2.4202 - val_binary_accuracy: 0.5938\n",
      "Epoch 61/100\n",
      "2048/2048 [==============================] - 0s 196us/sample - loss: 0.0883 - binary_accuracy: 0.9673 - val_loss: 2.3503 - val_binary_accuracy: 0.5742\n",
      "Epoch 62/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.1033 - binary_accuracy: 0.9600 - val_loss: 2.4121 - val_binary_accuracy: 0.5664\n",
      "Epoch 63/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0946 - binary_accuracy: 0.9697 - val_loss: 2.3870 - val_binary_accuracy: 0.5820\n",
      "Epoch 64/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.1052 - binary_accuracy: 0.9634 - val_loss: 2.2005 - val_binary_accuracy: 0.5820\n",
      "Epoch 65/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0777 - binary_accuracy: 0.9727 - val_loss: 2.4341 - val_binary_accuracy: 0.6016\n",
      "Epoch 66/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0919 - binary_accuracy: 0.9658 - val_loss: 2.5042 - val_binary_accuracy: 0.5898\n",
      "Epoch 67/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0982 - binary_accuracy: 0.9648 - val_loss: 2.4574 - val_binary_accuracy: 0.5781\n",
      "Epoch 68/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0815 - binary_accuracy: 0.9712 - val_loss: 2.3663 - val_binary_accuracy: 0.5859\n",
      "Epoch 69/100\n",
      "2048/2048 [==============================] - 0s 206us/sample - loss: 0.0823 - binary_accuracy: 0.9741 - val_loss: 2.4390 - val_binary_accuracy: 0.5898\n",
      "Epoch 70/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0790 - binary_accuracy: 0.9722 - val_loss: 2.4668 - val_binary_accuracy: 0.5664\n",
      "Epoch 71/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.0761 - binary_accuracy: 0.9727 - val_loss: 2.6013 - val_binary_accuracy: 0.5586\n",
      "Epoch 72/100\n",
      "2048/2048 [==============================] - 0s 209us/sample - loss: 0.0853 - binary_accuracy: 0.9717 - val_loss: 2.4974 - val_binary_accuracy: 0.5742\n",
      "Epoch 73/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0945 - binary_accuracy: 0.9683 - val_loss: 2.5441 - val_binary_accuracy: 0.5664\n",
      "Epoch 74/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0909 - binary_accuracy: 0.9673 - val_loss: 2.4183 - val_binary_accuracy: 0.5664\n",
      "Epoch 75/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0798 - binary_accuracy: 0.9727 - val_loss: 2.5377 - val_binary_accuracy: 0.5781\n",
      "Epoch 76/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0832 - binary_accuracy: 0.9648 - val_loss: 2.5527 - val_binary_accuracy: 0.5781\n",
      "Epoch 77/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.0696 - binary_accuracy: 0.9766 - val_loss: 2.4922 - val_binary_accuracy: 0.6055\n",
      "Epoch 78/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0808 - binary_accuracy: 0.9712 - val_loss: 2.6287 - val_binary_accuracy: 0.5859\n",
      "Epoch 79/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0955 - binary_accuracy: 0.9771 - val_loss: 2.4516 - val_binary_accuracy: 0.5938\n",
      "Epoch 80/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.1171 - binary_accuracy: 0.9619 - val_loss: 2.3083 - val_binary_accuracy: 0.5859\n",
      "Epoch 81/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0894 - binary_accuracy: 0.9678 - val_loss: 2.5300 - val_binary_accuracy: 0.5859\n",
      "Epoch 82/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0863 - binary_accuracy: 0.9688 - val_loss: 2.4507 - val_binary_accuracy: 0.5742\n",
      "Epoch 83/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.0938 - binary_accuracy: 0.9722 - val_loss: 2.5110 - val_binary_accuracy: 0.5820\n",
      "Epoch 84/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0847 - binary_accuracy: 0.9717 - val_loss: 2.4318 - val_binary_accuracy: 0.5820\n",
      "Epoch 85/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0822 - binary_accuracy: 0.9731 - val_loss: 2.6550 - val_binary_accuracy: 0.5859\n",
      "Epoch 86/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.0724 - binary_accuracy: 0.9785 - val_loss: 2.6221 - val_binary_accuracy: 0.5898\n",
      "Epoch 87/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0721 - binary_accuracy: 0.9771 - val_loss: 2.5817 - val_binary_accuracy: 0.5703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.0913 - binary_accuracy: 0.9702 - val_loss: 2.4719 - val_binary_accuracy: 0.5820\n",
      "Epoch 89/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.0714 - binary_accuracy: 0.9746 - val_loss: 2.5451 - val_binary_accuracy: 0.5859\n",
      "Epoch 90/100\n",
      "2048/2048 [==============================] - 0s 198us/sample - loss: 0.0644 - binary_accuracy: 0.9805 - val_loss: 2.8888 - val_binary_accuracy: 0.5703\n",
      "Epoch 91/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0681 - binary_accuracy: 0.9756 - val_loss: 2.7614 - val_binary_accuracy: 0.5977\n",
      "Epoch 92/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0722 - binary_accuracy: 0.9736 - val_loss: 2.6432 - val_binary_accuracy: 0.5781\n",
      "Epoch 93/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.0832 - binary_accuracy: 0.9722 - val_loss: 2.7541 - val_binary_accuracy: 0.5781\n",
      "Epoch 94/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0617 - binary_accuracy: 0.9775 - val_loss: 2.7451 - val_binary_accuracy: 0.5859\n",
      "Epoch 95/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.0820 - binary_accuracy: 0.9722 - val_loss: 2.8161 - val_binary_accuracy: 0.5781\n",
      "Epoch 96/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0706 - binary_accuracy: 0.9756 - val_loss: 2.6594 - val_binary_accuracy: 0.5820\n",
      "Epoch 97/100\n",
      "2048/2048 [==============================] - 0s 197us/sample - loss: 0.0774 - binary_accuracy: 0.9746 - val_loss: 2.6596 - val_binary_accuracy: 0.5586\n",
      "Epoch 98/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.0814 - binary_accuracy: 0.9751 - val_loss: 2.7232 - val_binary_accuracy: 0.5742\n",
      "Epoch 99/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0744 - binary_accuracy: 0.9722 - val_loss: 2.7463 - val_binary_accuracy: 0.5781\n",
      "Epoch 100/100\n",
      "2048/2048 [==============================] - 0s 199us/sample - loss: 0.0743 - binary_accuracy: 0.9741 - val_loss: 2.9560 - val_binary_accuracy: 0.5781\n"
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                Experiment_No=0,\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                ndim = 100,\n",
    "                tpu_enabled=0,\n",
    "                num_training_samples=2048,\n",
    "                num_validation_samples = 256,\n",
    "                num_epochs = 100,\n",
    "                batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=0,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: what does ban bossy encourage\n",
      "Predicted Answer: leadership in girls </s> \n",
      "Actual answer: <s> leadership in girls </s>\n",
      "question: what area had high windows just below ground level\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what happens if a plant looses roots or its shoots\n",
      "Predicted Answer: can often regrow it </s> \n",
      "Actual answer: <s> can often regrow it </s>\n",
      "question: what was typically worn after the loss of a loved one in the roman republic\n",
      "Predicted Answer: the toga pulla </s> \n",
      "Actual answer: <s> the toga pulla </s>\n",
      "question: what instruments are used in armenian folk music\n",
      "Predicted Answer: the duduk the dhol the dhol the dhol the dhol \n",
      "Actual answer: <s> the duduk the dhol the zurna and the kanun </s>\n",
      "question: why does a litigant initiate a lawsuit under the civil rights act of 1801\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: at what rate have glaciers travelled during surges\n",
      "Predicted Answer: 90 m 300 ft per day </s> \n",
      "Actual answer: <s> 90 m 300 ft per day </s>\n",
      "question: what totals a thirs of the glaciers length\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what color does liverpool fans wear\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what other industry is a large part of houston s economy\n",
      "Predicted Answer: houston ship channel </s> \n",
      "Actual answer: <s> houston ship channel </s>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sentence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_answer_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
