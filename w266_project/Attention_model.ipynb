{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorflow_version 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.chdir('/content/gdrive/My Drive/W266-NLP/Project')\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wget in /home/anupj/anaconda3/envs/tensorflow_cpu_2/lib/python3.7/site-packages (3.2)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import json\n",
    "from pandas.io.json import json_normalize\n",
    "from collections import Counter\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "import nltk\n",
    "from functools import reduce\n",
    "!pip install wget\n",
    "# Load PyDrive and Google Auth related packages\n",
    "#!pip install -U -q PyDrive\n",
    "#from pydrive.auth import GoogleAuth\n",
    "#from pydrive.drive import GoogleDrive\n",
    "#from google.colab import auth\n",
    "#from oauth2client.client import GoogleCredentials\n",
    "# Authenticate and create the PyDrive client.\n",
    "#auth.authenticate_user()\n",
    "#gauth = GoogleAuth()\n",
    "#gauth.credentials = GoogleCredentials.get_application_default()\n",
    "#drive = GoogleDrive(gauth)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Attention\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Input\n",
    "from functools import reduce\n",
    "flatten = lambda data: reduce(lambda x, y: x + y, data)\n",
    "import glove_helper\n",
    "\n",
    "# Load the json data\n",
    "def load_json_file(name):\n",
    "  \"\"\"\n",
    "  Load the json file and return a json object\n",
    "  \"\"\"\n",
    "  with open(name,encoding='utf-8') as myfile:\n",
    "    data = json.load(myfile)\n",
    "    return data\n",
    "\n",
    "# Convert json data object to a pandas data frame\n",
    "def convert_to_pd(data):\n",
    "  \"\"\"\n",
    "  Load the data to a pandas dataframe.\n",
    "  Dataframe Columns:\n",
    "    title\n",
    "    para_index\n",
    "    context\n",
    "    q_index\n",
    "    q_id\n",
    "    q_isimpossible\n",
    "    q_question\n",
    "    q_anscount - number of answers\n",
    "    q_answers - a list of object e.g [{ text: '', answer_start: 123}, ...]\n",
    "  \"\"\"\n",
    "  result = []\n",
    "  for pdata in data['data']:\n",
    "    for para in pdata['paragraphs']:\n",
    "      for q in para['qas']:\n",
    "        result.append({\n",
    "            'title' : pdata['title'],\n",
    "            'context' : para['context'],\n",
    "            'q_id' : q['id'],\n",
    "            'q_isimpossible' : q['is_impossible'],\n",
    "            'q_question' : q['question'],\n",
    "            'q_anscount' : len(q['answers']),\n",
    "            'q_answers' : [a for a in q['answers']],\n",
    "            'q_answers_text': [a.get(\"text\") for a in q['answers']],\n",
    "            'context_lowercase': para['context'].lower(),\n",
    "            'q_question_lowercase' : q['question'].lower(),\n",
    "            'q_answers_text_lowercase': [a.get(\"text\").lower() for a in q['answers']],\n",
    "            \n",
    "        })\n",
    "\n",
    "  return pd.DataFrame.from_dict(result, orient='columns')\n",
    "\n",
    "# Load the file from shareable google drive link and return a pandas dataframe\n",
    "def loadDataFile(filename): \n",
    "  \"\"\"\n",
    "  Download a file from google drive with the shared link\n",
    "  \"\"\" \n",
    "  data = load_json_file(filename)\n",
    "  return convert_to_pd(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DONOT RUN THIS ON COLAB#\n",
    "#to make use of CPU and not GPU DONOT RUN THIS ON COLAB\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filename = 'train-v2.0.json'\n",
    "dev_filename = 'dev-v2.0.json'\n",
    "\n",
    "train_pd = loadDataFile(train_filename)\n",
    "dev_pd = loadDataFile(dev_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max context length: 653\n",
      "Max question length: 40\n",
      "Max answer length: 43\n"
     ]
    }
   ],
   "source": [
    "def get_c_q_a(dataset):\n",
    "    q_id_list = []\n",
    "    context_list =[]\n",
    "    questions_list = []\n",
    "    answers_list =[]\n",
    "    q_impossible_list =[]\n",
    "    for index,row in dataset.iterrows():\n",
    "        q_id_list.append(row.q_id)\n",
    "        context_list.append(row.context)\n",
    "        questions_list.append(row.q_question)\n",
    "        q_impossible_list.append(int(row.q_isimpossible))\n",
    "        if len(row.q_answers_text)>0 :\n",
    "            answers_list.append(row.q_answers_text[0])\n",
    "        else:\n",
    "            answers_list.append(\"\")\n",
    "    return [q_id_list,context_list,questions_list,q_impossible_list,answers_list]\n",
    "\n",
    "train_lists = get_c_q_a(train_pd)\n",
    "dev_lists = get_c_q_a(dev_pd)\n",
    "context_maxlen = max(map(len, (x.split() for x in train_lists[1])))\n",
    "question_maxlen = max(map(len, (x.split() for x in train_lists[2])))\n",
    "answer_maxlen = max(map(len, (x.split() for x in train_lists[4])))\n",
    "print(\"Max context length:\",context_maxlen)\n",
    "print(\"Max question length:\",question_maxlen)\n",
    "print(\"Max answer length:\",answer_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_maxlen = 214\n",
    "question_maxlen = 18\n",
    "answer_maxlen = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab Size: 88701\n",
      "validation num samples where answer impossible:  8730\n",
      "validation num samples where answer not impossible:  17382\n",
      "train num samples where answer impossible:  34761\n",
      "train num samples where answer not impossible:  69431\n"
     ]
    }
   ],
   "source": [
    "def tokenize_c_q_a(dataset,num_words=None):\n",
    "    tokenizer = Tokenizer(num_words,filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'+\"''\",oov_token='<unk>')\n",
    "    data = dataset[1]+dataset[2]+dataset[4]\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    vocab = {}\n",
    "    for word,i in tokenizer.word_index.items():\n",
    "        if num_words is not None:\n",
    "          if i <= num_words:\n",
    "            vocab[word] = i\n",
    "        else:\n",
    "          vocab[word] = i\n",
    "    #vocab = tokenizer.word_index\n",
    "    vocab['<s>'] = len(vocab)+1\n",
    "    vocab['</s>'] = len(vocab)+1\n",
    "    id_vocab = {value: key for key, value in vocab.items()}\n",
    "    return (tokenizer,vocab,id_vocab)\n",
    "\n",
    "tokenizer_obj,vocab,id_vocab = tokenize_c_q_a(train_lists)\n",
    "\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab Size:\",vocab_size)\n",
    "\n",
    "def vectorize_data(tokenizer_obj,train_lists):\n",
    "    context_seq = tokenizer_obj.texts_to_sequences(train_lists[1])\n",
    "    question_seq = tokenizer_obj.texts_to_sequences(train_lists[2])\n",
    "    answer_seq = tokenizer_obj.texts_to_sequences(train_lists[4])\n",
    "    answer_input_seq = [[vocab['<s>']]+i+[vocab['</s>']] for i in answer_seq]\n",
    "    answer_target_seq = [i+[vocab['</s>']] for i in answer_seq]\n",
    "    context_seq_padded = pad_sequences(context_seq,context_maxlen,padding='post', truncating='post')\n",
    "    question_seq_padded = pad_sequences(question_seq,question_maxlen,padding='post', truncating='post')\n",
    "    answer_seq_padded = pad_sequences(answer_seq,answer_maxlen,padding='post', truncating='post')\n",
    "    answer_input_seq_padded = pad_sequences(answer_input_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_target_seq_padded = pad_sequences(answer_target_seq,answer_maxlen+2,padding='post', truncating='post')\n",
    "    answer_impossible = np.array(train_lists[3])\n",
    "    indices = np.arange(context_seq_padded.shape[0])\n",
    "    np.random.seed(19)\n",
    "    np.random.shuffle(indices)\n",
    "    context_seq_padded = context_seq_padded[indices]\n",
    "    question_seq_padded = question_seq_padded[indices]\n",
    "    answer_seq_padded = answer_seq_padded[indices]\n",
    "    answer_input_seq_padded = answer_input_seq_padded[indices]\n",
    "    answer_target_seq_padded = answer_target_seq_padded[indices]\n",
    "    answer_impossible_shuffled = answer_impossible[indices]\n",
    "    train_samples = int(((context_seq_padded.shape[0]*.8)//128)*128)\n",
    "    end_samples = int((context_seq_padded.shape[0]//128)*128)\n",
    "    train_context_padded_seq = context_seq_padded[:train_samples]\n",
    "    train_question_seq_padded = question_seq_padded[:train_samples]\n",
    "    train_answer_seq_padded = answer_seq_padded[:train_samples]\n",
    "    train_answer_input_seq_padded = answer_input_seq_padded[:train_samples]\n",
    "    train_answer_target_seq_padded = answer_target_seq_padded[:train_samples]\n",
    "    train_answer_impossible = answer_impossible_shuffled[:train_samples]\n",
    "    val_context_padded_seq = context_seq_padded[train_samples:end_samples]\n",
    "    val_question_seq_padded = question_seq_padded[train_samples:end_samples]\n",
    "    val_answer_seq_padded = answer_seq_padded[train_samples:end_samples]\n",
    "    val_answer_input_seq_padded = answer_input_seq_padded[train_samples:end_samples]\n",
    "    val_answer_target_seq_padded = answer_target_seq_padded[train_samples:end_samples]\n",
    "    val_answer_impossible = answer_impossible_shuffled[train_samples:end_samples]\n",
    "    return (train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\n",
    "            train_answer_input_seq_padded,train_answer_target_seq_padded,train_answer_impossible,\n",
    "            val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\n",
    "            val_answer_input_seq_padded,val_answer_target_seq_padded,val_answer_impossible)\n",
    "\n",
    "train_context_padded_seq,train_question_seq_padded,train_answer_seq_padded,\\\n",
    "train_answer_input_seq_padded,train_answer_target_seq_padded,\\\n",
    "train_answer_impossible,\\\n",
    "val_context_padded_seq,val_question_seq_padded,val_answer_seq_padded,\\\n",
    "val_answer_input_seq_padded,val_answer_target_seq_padded,\\\n",
    "val_answer_impossible\\\n",
    "= vectorize_data(tokenizer_obj,train_lists)\n",
    "\n",
    "print(\"validation num samples where answer impossible: \",len(val_answer_seq_padded[val_answer_impossible==1]))\n",
    "print(\"validation num samples where answer not impossible: \",len(val_answer_seq_padded[val_answer_impossible==0]))\n",
    "print(\"train num samples where answer impossible: \",len(train_answer_seq_padded[train_answer_impossible==1]))\n",
    "print(\"train num samples where answer not impossible: \",len(train_answer_seq_padded[train_answer_impossible==0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104192"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_question_seq_padded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26112"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_answer_seq_padded.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading vectors from data/glove/glove.6B.zip\n",
      "Parsing file: data/glove/glove.6B.zip:glove.6B.100d.txt\n",
      "Found 400,000 words.\n",
      "Parsing vectors... Done! (W.shape = (400003, 100))\n"
     ]
    }
   ],
   "source": [
    "def create_embedding_matrix(word_index,vocab_size=50000,ndim=100):\n",
    "    hands = glove_helper.Hands(ndim)\n",
    "    embedding_matrix = np.zeros((vocab_size+1,ndim))\n",
    "    for word,i in word_index.items():\n",
    "        if i<=vocab_size:\n",
    "            embedding_vector = hands.get_vector(word,strict=False)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix\n",
    "ndim = 100\n",
    "embedding_matrix = create_embedding_matrix(vocab,vocab_size,ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#New Model with attention in every step of the answer decoder\n",
    "class BahdanauAttention_model2(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention_model2, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        self.V = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        # query hidden state shape == (batch_size, hidden size)\n",
    "        # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
    "        # values shape == (batch_size, max_len, hidden size)\n",
    "        # we are doing this to broadcast addition along the time axis to calculate the score\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "\n",
    "        # score shape == (batch_size, max_length, 1)\n",
    "        # we get 1 at the last axis because we are applying score to self.V\n",
    "        # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
    "        score = self.V(tf.nn.tanh(\n",
    "                                self.W1(query_with_time_axis) + self.W2(values)))\n",
    "\n",
    "        # attention_weights shape == (batch_size, max_length, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        # context_vector shape after sum == (batch_size, hidden_size)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# Answer Module which is custom as we need to feed output of each time sequence with attention to next \n",
    "# time sequence\n",
    "class answer_module(tf.keras.Model):\n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      ndim,\n",
    "                      num_unit_gru,\n",
    "                      num_layers_gru,\n",
    "                      dropout_rate,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(answer_module, self).__init__()\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.attention_layer = BahdanauAttention_model2(num_unit_gru)\n",
    "        #self.answer_input_layer = Input(shape=(None,),dtype='int32',name='Answer_Input')\n",
    "        self.answer_embedding_layer = layers.Embedding(vocab_size+1,\n",
    "                                                       ndim,\n",
    "                                                       mask_zero=True,\n",
    "                                                       weights =[embedding_matrix],\n",
    "                                                       trainable = False,\n",
    "                                                       name='Answer_Embedding')\n",
    "        self.answer_output_layers = []\n",
    "        self.batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "            self.answer_output_layers.append(layers.GRU(self.num_unit_gru,\n",
    "                                                        dropout=self.dropout_rate,\n",
    "                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                        return_sequences=True,\n",
    "                                                        return_state=True,\n",
    "                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                        name='Answer_GRU_Layer'+str(i),\n",
    "                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                                       )\n",
    "                                              )\n",
    "            self.batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        \n",
    "        self.answer_decoder_dense = layers.Dense(vocab_size+1,\n",
    "                                                 name='Answer_output',\n",
    "                                                 kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                 bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight))\n",
    "        \n",
    "    \n",
    "    def call(self,\n",
    "             target_input,\n",
    "             answer_hidden,\n",
    "             question,\n",
    "             context,\n",
    "             episodic_memory):\n",
    "        attention_output,attention_weights = self.attention_layer(answer_hidden,context)\n",
    "        answer_embeddings = self.answer_embedding_layer(target_input)\n",
    "        answer_concat = tf.concat([tf.expand_dims(attention_output, 1),\n",
    "                                   tf.expand_dims(question, 1),\n",
    "                                   answer_embeddings], axis=-1)\n",
    "        for i in range(len(self.answer_output_layers)):\n",
    "            if ((episodic_memory != None) and (i==0)):\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_concat,\n",
    "                                                                           initial_state=episodic_memory)\n",
    "            elif i==0:\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_concat)\n",
    "            else:\n",
    "                answer_outputs,hidden_state = self.answer_output_layers[i](answer_outputs)\n",
    "            \n",
    "            answer_outputs   = self.batch_normalization_layers[i](answer_outputs)\n",
    "        answer_outputs = tf.reshape(answer_outputs, (-1, answer_outputs.shape[2]))\n",
    "\n",
    "        answer_decoder_outputs = self.answer_decoder_dense(answer_outputs)\n",
    "        return answer_decoder_outputs,hidden_state,attention_weights\n",
    "\n",
    "#Encoder Module which combines the context,question into episodic memory and emits context outputs and \n",
    "#question outputs\n",
    "class encoder_module(tf.keras.Model):    \n",
    "    def __init__(self,embedding_matrix,\n",
    "                      vocab_size,\n",
    "                      max_context_length,\n",
    "                      max_question_length,\n",
    "                      max_answer_length,\n",
    "                      num_unit_gru = 64,\n",
    "                      num_layers_gru = 2,\n",
    "                      ndim =100,\n",
    "                      num_episodes = 2,\n",
    "                      dropout_rate = 0.5,\n",
    "                      num_episodic_network_unit = 64,\n",
    "                      l1_regularizer_weight = .01,\n",
    "                      l2_regularizer_weight = .01\n",
    "                      ):\n",
    "        super(encoder_module, self).__init__()\n",
    "        #Context Module\n",
    "        self.num_unit_gru = num_unit_gru\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_layers_gru = num_layers_gru\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.vocab_size = vocab_size\n",
    "        self.ndim = ndim\n",
    "        self.max_context_length = max_context_length\n",
    "        self.max_question_length = max_question_length\n",
    "        self.max_answer_length = max_answer_length\n",
    "        self.num_episodes = num_episodes\n",
    "        self.num_episodic_network_unit = num_episodic_network_unit\n",
    "        self.context_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                         self.ndim,\n",
    "                                                         mask_zero=True,\n",
    "                                                         weights =[self.embedding_matrix],\n",
    "                                                         trainable = False,\n",
    "                                                         name='Context_Embedding')\n",
    "        self.context_output_layers = []\n",
    "        self.context_batch_normalization_layers = []\n",
    "        for i in range(self.num_layers_gru):\n",
    "            self.context_output_layers.append(layers.Bidirectional(layers.GRU(self.num_unit_gru,\n",
    "                                                                               dropout=self.dropout_rate,\n",
    "                                                                               recurrent_dropout= self.dropout_rate,\n",
    "                                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                                               return_sequences=True,\n",
    "                                                                               kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                               bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                                   merge_mode='sum',\n",
    "                                                                   name='Context_Bid_Layer'+str(i))\n",
    "                                         )\n",
    "            self.context_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        \n",
    "        #Question Module\n",
    "        self.question_embeddings_layer = layers.Embedding(self.vocab_size+1,\n",
    "                                                          self.ndim,\n",
    "                                                          mask_zero=True,\n",
    "                                                          weights =[self.embedding_matrix],\n",
    "                                                          trainable = False,\n",
    "                                                          name='Question_Embedding')\n",
    "          \n",
    "        self.question_output_layers = []\n",
    "        self.question_batch_normalization_layers = []\n",
    "        for i in range(num_layers_gru):\n",
    "\n",
    "            if i==0 and num_layers_gru >1:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=True,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                             merge_mode='sum',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            elif i==0 and num_layers_gru ==1:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=False,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                         bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                             merge_mode='sum',\n",
    "                                                             name='Question_Bid_Layer'+str(i))\n",
    "                                             )\n",
    "            elif i==(num_layers_gru-1):\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=False,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                              merge_mode='sum',\n",
    "                                                              name='Question_Bid_Layer'+str(i))\n",
    "                                             )\n",
    "            else:\n",
    "                self.question_output_layers.append(layers.Bidirectional(\n",
    "                                                             layers.GRU(self.num_unit_gru,\n",
    "                                                                        dropout=self.dropout_rate,\n",
    "                                                                        recurrent_dropout= self.dropout_rate,\n",
    "                                                                        recurrent_initializer='glorot_uniform',\n",
    "                                                                        return_sequences=True,\n",
    "                                                                        kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)),\n",
    "                                                                        merge_mode='sum',\n",
    "                                                                        name='Question_Bid_Layer'+str(i))\n",
    "                                              )\n",
    "            self.question_batch_normalization_layers.append(layers.BatchNormalization())\n",
    "        #Episodic Memory \n",
    "        self.episodic_weight_layer = layers.Dense(self.num_unit_gru,use_bias=False)\n",
    "        self.episodic_tanh_layer = layers.Dense(self.num_episodic_network_unit,activation='tanh')\n",
    "        self.episodic_score_layer = layers.Dense(1)\n",
    "        \n",
    "    def call(self,context_input,question_input):\n",
    "        #context Module\n",
    "        context_embeddings = self.context_embeddings_layer(context_input)\n",
    "        for i in range(len(self.context_output_layers)):\n",
    "            if i==0:\n",
    "                context_outputs = self.context_output_layers[i](context_embeddings)\n",
    "            else:\n",
    "                context_outputs = self.context_output_layers[i](context_outputs)\n",
    "            context_outputs = self.context_batch_normalization_layers[i](context_outputs)\n",
    "\n",
    "        #Question Module\n",
    "        question_embeddings = self.question_embeddings_layer(question_input)\n",
    "        for i in range(len(self.question_output_layers)):\n",
    "            if i==0:\n",
    "                question_outputs = self.question_output_layers[i](question_embeddings)\n",
    "            else:\n",
    "                question_outputs = self.question_output_layers[i](question_outputs)\n",
    "            question_outputs = self.question_batch_normalization_layers[i](question_outputs) \n",
    "        #Episodic Memory \n",
    "        m = tf.identity(question_outputs)\n",
    "        for i in range(self.num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(question_outputs,1),\n",
    "                                  tf.constant([1,self.max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(question_outputs),1), \n",
    "                                context_outputs,\n",
    "                                transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(self.episodic_weight_layer(m),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = tf.concat([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = self.episodic_score_layer(self.episodic_tanh_layer(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        #concatenate episodic memory with question\n",
    "        concatenated_tensor = tf.concat(values=[m,question_outputs],axis=1)\n",
    "        return (m,concatenated_tensor,question_outputs,context_outputs)\n",
    "    \n",
    "                \n",
    "#Function to create the Models\n",
    "def create_models(embedding_matrix,\n",
    "                  max_context_length,\n",
    "                  max_question_length,\n",
    "                  max_answer_length,\n",
    "                  num_unit_gru = 64,\n",
    "                  num_layers_gru = 2,\n",
    "                  ndim =100,\n",
    "                  num_episodes = 2,\n",
    "                  num_dense_layer_feasibility_units = 16,\n",
    "                  dropout_rate = 0.5,\n",
    "                  num_dense_layers_feasibility = 1,\n",
    "                  num_episodic_network_unit = 64,\n",
    "                  l1_regularizer_weight = .01,\n",
    "                  l2_regularizer_weight = .01):\n",
    "    \"\"\"\n",
    "    \n",
    "    def create_episodic_memory(num_episodes,\n",
    "                               query,\n",
    "                               context_outputs,\n",
    "                               max_context_length,\n",
    "                               max_question_length,\n",
    "                               num_episodic_network_unit):\n",
    "        m = layers.Lambda(lambda x: x)(query)\n",
    "        weight_layer = layers.Dense(query.shape[1],use_bias=False)\n",
    "        for i in range(num_episodes):\n",
    "            m_increased = tf.tile(tf.keras.backend.expand_dims(m,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            q_increased = tf.tile(tf.keras.backend.expand_dims(query,1),\n",
    "                                  tf.constant([1,max_context_length,1],tf.int32))\n",
    "            c_mul_q = layers.multiply([context_outputs,q_increased])\n",
    "            c_mul_m = layers.multiply([context_outputs,m_increased])\n",
    "            c_minus_q =tf.keras.backend.abs(layers.subtract([context_outputs,q_increased]))\n",
    "            c_minus_m = tf.keras.backend.abs(layers.subtract([context_outputs,m_increased]))\n",
    "            c_dot_q = tf.matmul(tf.keras.backend.expand_dims(weight_layer(query),1), \n",
    "                                context_outputs,transpose_b=True)\n",
    "            c_dot_q = layers.Permute((2,1))(c_dot_q)\n",
    "            c_dot_m = tf.matmul(tf.keras.backend.expand_dims(weight_layer(m),1), context_outputs,transpose_b=True)\n",
    "            c_dot_m = layers.Permute((2,1))(c_dot_m)\n",
    "            z = layers.concatenate([context_outputs,\n",
    "                                    m_increased,\n",
    "                                    q_increased,\n",
    "                                    c_mul_q,\n",
    "                                    c_mul_m,\n",
    "                                    c_minus_q,\n",
    "                                    c_minus_m,\n",
    "                                    c_dot_q,\n",
    "                                    c_dot_m],axis=-1)\n",
    "            score = layers.Dense(1)(layers.Dense(num_episodic_network_unit,activation='tanh')(z))\n",
    "            attention_weights = tf.nn.softmax(score, axis=1)\n",
    "            m_value = attention_weights * context_outputs\n",
    "            m = tf.reduce_sum(m_value, axis=1)\n",
    "        return m\n",
    "    \n",
    "    \n",
    "    #Input Module\n",
    "    context_input = Input(shape=(None,),dtype='int32',name='Context_Input')\n",
    "    context_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                          ndim,\n",
    "                                          mask_zero=True,\n",
    "                                          name='Context_Embedding')(context_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        context_outputs_layers = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                                 dropout=dropout_rate,\n",
    "                                                                 recurrent_dropout= dropout_rate,\n",
    "                                                                 recurrent_initializer='glorot_uniform',\n",
    "                                                                 return_sequences=True),\n",
    "                                                      merge_mode='sum',\n",
    "                                                      name='Context_Bid_Layer'+str(i))\n",
    "        if i==0:\n",
    "            context_outputs = context_outputs_layers(context_embeddings)\n",
    "        else:\n",
    "            context_outputs = context_outputs_layers(context_outputs)\n",
    "        context_outputs = layers.BatchNormalization()(context_outputs)\n",
    "    print(\"Context output shape\",context_outputs.shape)\n",
    "    #Question Module\n",
    "    question_input = Input(shape=(None,),dtype='int32',name='Question_Input')\n",
    "    question_embeddings = layers.Embedding(vocab_size+1,\n",
    "                                           ndim,\n",
    "                                           mask_zero=True,\n",
    "                                           name='Question_Embedding')(question_input)\n",
    "\n",
    "    for i in range(num_layers_gru):\n",
    "        if i==0 and num_layers_gru >1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==0 and num_layers_gru ==1:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_embeddings)\n",
    "        elif i==(num_layers_gru-1):\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=False),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        else:\n",
    "            question_outputs = layers.Bidirectional(layers.GRU(num_unit_gru,\n",
    "                                                               dropout=dropout_rate,\n",
    "                                                               recurrent_dropout= dropout_rate,\n",
    "                                                               recurrent_initializer='glorot_uniform',\n",
    "                                                               return_sequences=True),\n",
    "                                                    merge_mode='sum',\n",
    "                                                    name='Question_Bid_Layer'+str(i))(question_outputs)\n",
    "        question_outputs = layers.BatchNormalization()(question_outputs)\n",
    "    #Episodic Memory Module\n",
    "    m=create_episodic_memory(num_episodes,\n",
    "                             question_outputs,\n",
    "                             context_outputs,\n",
    "                             max_context_length,\n",
    "                             max_question_length,\n",
    "                             num_episodic_network_unit)\n",
    "\n",
    "    concatenated_tensor = layers.concatenate(inputs=[m,question_outputs],\n",
    "                                             name='Concatenation_Memory_Question',axis=1)\n",
    "    \"\"\"\n",
    "    #encoder Model\n",
    "    encoder_model = encoder_module(embedding_matrix,\n",
    "                                   vocab_size,\n",
    "                                   max_context_length,\n",
    "                                   max_question_length,\n",
    "                                   max_answer_length,\n",
    "                                   num_unit_gru,\n",
    "                                   num_layers_gru,\n",
    "                                   ndim,\n",
    "                                   num_episodes,\n",
    "                                   dropout_rate,\n",
    "                                   num_episodic_network_unit,\n",
    "                                   l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                   l2_regularizer_weight = l2_regularizer_weight\n",
    "                                 )\n",
    "    \n",
    "    \n",
    "    #Model([context_input,question_input], [m,concatenated_tensor,question_outputs,context_outputs])\n",
    "    #answer_module\n",
    "    answer_model = answer_module(embedding_matrix,\n",
    "                                 vocab_size,\n",
    "                                 ndim,\n",
    "                                 num_unit_gru,\n",
    "                                 num_layers_gru,\n",
    "                                 dropout_rate,\n",
    "                                 l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                 l2_regularizer_weight = l2_regularizer_weight)\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Question_Embedding\").trainable = False\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").set_weights([embedding_matrix])\n",
    "    #encoder_model.get_layer(\"Context_Embedding\").trainable = False\n",
    "    \n",
    "    #feasibility module\n",
    "    feasibility_input = Input(shape=(2*num_unit_gru,), name=\"FeasibilityInput\")\n",
    "    feasibility_context_input = Input(shape=(None,num_unit_gru,),name='feasibilityContext_Input')\n",
    "    feasibility_question_input = Input(shape=(num_unit_gru,),name='feasibilityQuestion_Input')\n",
    "    for i in range(num_dense_layers_feasibility):\n",
    "        #create attention between Context and Question\n",
    "        q_with_time_axis = tf.keras.backend.expand_dims(feasibility_question_input,1)\n",
    "        attentionContextQuestion = layers.AdditiveAttention()([q_with_time_axis,\n",
    "                                                               feasibility_context_input])\n",
    "        attentionContextQuestionReduced = tf.keras.backend.sum(attentionContextQuestion, axis=1)\n",
    "        feasibility_dense_input = tf.concat([feasibility_input,\n",
    "                                             attentionContextQuestionReduced],\n",
    "                                            axis=-1)\n",
    "        \n",
    "        if i==0:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(feasibility_dense_input)\n",
    "        else:\n",
    "            dense_layer = layers.Dense(num_dense_layer_feasibility_units,\n",
    "                                       activation='relu',\n",
    "                                       name='feasibility_layer_'+str(i),\n",
    "                                       kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight),\n",
    "                                        bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                      l1_regularizer_weight,\n",
    "                                                                                      l2_regularizer_weight)\n",
    "                                      )(dense_layer)\n",
    "        dense_layer = layers.BatchNormalization()(dense_layer)\n",
    "        dropout_layer = layers.Dropout(dropout_rate,name='feasibility_drop_'+str(i))(dense_layer)\n",
    "\n",
    "    feasibility_output = layers.Dense(1,activation='sigmoid',\n",
    "                                      name='feasibility_output',\n",
    "                                      kernel_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                     l1_regularizer_weight,\n",
    "                                                                                    l2_regularizer_weight),\n",
    "                                      bias_regularizer=tf.keras.regularizers.l1_l2(\n",
    "                                                                                   l1_regularizer_weight,\n",
    "                                                                                   l2_regularizer_weight))(dropout_layer)\n",
    "    feasibility_model = Model([feasibility_input,feasibility_context_input,feasibility_question_input],\n",
    "                              feasibility_output)\n",
    "    \n",
    "    return (answer_model,encoder_model,feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to get sentences from the predicted answers\n",
    "def decode_sentence(context_input_seq,\n",
    "                     question_input_seq,\n",
    "                     encoder_model,\n",
    "                     answer_model):\n",
    "    decoded_sentence = ''\n",
    "    episodic_memory,\\\n",
    "    concatenated_tensor,\\\n",
    "    question_output,\\\n",
    "    context_output = encoder_model(context_input_seq,question_input_seq) \n",
    "    answer_hidden = question_output\n",
    "    dec_input = tf.expand_dims([vocab[\"<s>\"]], 0)\n",
    "\n",
    "    for t in range(answer_maxlen):\n",
    "        predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                     answer_hidden,\n",
    "                                                     question_output,\n",
    "                                                     context_output,\n",
    "                                                     episodic_memory)\n",
    "        sampled_token_index = tf.argmax(predictions[0]).numpy()\n",
    "        \n",
    "        if sampled_token_index == 0:\n",
    "            sampled_char = \" \"\n",
    "        else:\n",
    "            sampled_char = id_vocab[sampled_token_index]\n",
    "        decoded_sentence += sampled_char + \" \"\n",
    "        if id_vocab[sampled_token_index] == '</s>':\n",
    "            return decoded_sentence\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([sampled_token_index], 0)\n",
    "        episodic_memory = None\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Experiment0': {'num_unit_gru': 64,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.001,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment1': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.0001,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment2': {'num_unit_gru': 100,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 64,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.01},\n",
       " 'Experiment3': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 32,\n",
       "  'dropout_rate': 0.7,\n",
       "  'num_dense_layers_feasibility': 3,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.01,\n",
       "  'l2_regularizer_weight': 0.0001},\n",
       " 'Experiment4': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 2,\n",
       "  'num_dense_layer_feasibility_units': 48,\n",
       "  'dropout_rate': 0.6,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 128,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.001},\n",
       " 'Experiment5': {'num_unit_gru': 80,\n",
       "  'num_layers_gru': 2,\n",
       "  'num_episodes': 3,\n",
       "  'num_dense_layer_feasibility_units': 64,\n",
       "  'dropout_rate': 0.5,\n",
       "  'num_dense_layers_feasibility': 1,\n",
       "  'num_episodic_network_unit': 192,\n",
       "  'learning_rate': 0.005,\n",
       "  'l1_regularizer_weight': 0.001,\n",
       "  'l2_regularizer_weight': 0.0001}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Experiment_Dic = {'Experiment0': {'num_unit_gru': 64,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.001,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment1': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.0001,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment2': {'num_unit_gru': 100,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 64,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.01},\n",
    "                  'Experiment3': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 32,\n",
    "                                  'dropout_rate': 0.7,\n",
    "                                  'num_dense_layers_feasibility': 3,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.01,\n",
    "                                  'l2_regularizer_weight':0.0001},\n",
    "                  'Experiment4': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 2,\n",
    "                                  'num_dense_layer_feasibility_units': 48,\n",
    "                                  'dropout_rate': 0.6,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 128,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.001},\n",
    "                  'Experiment5': {'num_unit_gru': 80,\n",
    "                                  'num_layers_gru': 2,\n",
    "                                  'num_episodes': 3,\n",
    "                                  'num_dense_layer_feasibility_units': 64,\n",
    "                                  'dropout_rate': 0.5,\n",
    "                                  'num_dense_layers_feasibility': 1,\n",
    "                                  'num_episodic_network_unit': 192,\n",
    "                                  'learning_rate': 0.005,\n",
    "                                  'l1_regularizer_weight':0.001,\n",
    "                                  'l2_regularizer_weight':0.0001}\n",
    "                }\n",
    "Experiment_Dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(Experiment_Dic,\n",
    "                    Experiment_No,\n",
    "                    embedding_matrix,\n",
    "                    ndim = 100,\n",
    "                    tpu_enabled=0,\n",
    "                    num_training_samples=1024,\n",
    "                    num_validation_samples = 256,\n",
    "                    num_epochs = 50,\n",
    "                    batch_size = 10):\n",
    "    num_training_samples = int((num_training_samples//128)*128)\n",
    "    num_validation_samples = int((num_validation_samples//128)*128)\n",
    "    #get the experiment details\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "        \n",
    "    if tpu_enabled==0:\n",
    "        #When GPU ENABLED\n",
    "        answer_model,\\\n",
    "        encoder_model,\\\n",
    "        feasibility_model = create_models(\n",
    "                                      embedding_matrix = embedding_matrix,\n",
    "                                      max_context_length = context_maxlen,\n",
    "                                      max_question_length = question_maxlen,\n",
    "                                      max_answer_length = answer_maxlen,\n",
    "                                      num_unit_gru = num_unit_gru,\n",
    "                                      num_layers_gru = num_layers_gru,\n",
    "                                      ndim =ndim,\n",
    "                                      num_episodes = num_episodes,\n",
    "                                      num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                      dropout_rate = dropout_rate,\n",
    "                                      num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                      num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                      l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                      l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "        adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "        #encoder_model.compile(optimizer=adam_optim,\n",
    "        #                      loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                      metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "        #encoder_model.summary()\n",
    "        #answer_model.compile(optimizer=adam_optim,\n",
    "        #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "        #                        )\n",
    "        #\n",
    "        #answer_model.summary()\n",
    "        feasibility_model.compile(optimizer=adam_optim,\n",
    "                                   loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                   metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                   )\n",
    "        #feasibility_model.summary()\n",
    "    else: \n",
    "        resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='grpc://' +\n",
    "                                                                     os.environ['COLAB_TPU_ADDR'])\n",
    "        tf.config.experimental_connect_to_cluster(resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(resolver)\n",
    "        strategy = tf.distribute.experimental.TPUStrategy(resolver)\n",
    "        batch_size = 128*8\n",
    "        with strategy.scope():\n",
    "            answer_model,\\\n",
    "            encoder_model,\\\n",
    "            feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = \n",
    "                                                        num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                         l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                         l2_regularizer_weight = l2_regularizer_weight)\n",
    "\n",
    "            adam_optim = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "            #encoder_model.compile(optimizer=adam_optim,\n",
    "            #                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                  metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "            #encoder_model.summary()\n",
    "            #\n",
    "            #answer_model.compile(optimizer=adam_optim,\n",
    "            #                           loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "            #                           metrics=[tf.keras.metrics.SparseCategoricalAccuracy()]\n",
    "            #                    )\n",
    "\n",
    "            #answer_model.summary()\n",
    "            feasibility_model.compile(optimizer=adam_optim,\n",
    "                                       loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "                                       metrics=[tf.keras.metrics.BinaryAccuracy()]\n",
    "                                       )\n",
    "            #feasibility_model.summary()\n",
    "    #Train the Answer Model and Encoder Model\n",
    "    answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    answer_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    train_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    val_acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "    @tf.function\n",
    "    def answer_loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = answer_loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def answer_train_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "            answer_hidden = question_output\n",
    "            dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                             answer_hidden,\n",
    "                                                             question_output,\n",
    "                                                             context_output,\n",
    "                                                             episodic_memory)\n",
    "                loss += answer_loss_function(targ[:, t], predictions)\n",
    "                train_acc_metric(targ[:, t],predictions)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                episodic_memory = None\n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "    @tf.function\n",
    "    def answer_val_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "        answer_hidden = question_output\n",
    "        dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                         answer_hidden,\n",
    "                                                         question_output,\n",
    "                                                         context_output,\n",
    "                                                         episodic_memory)\n",
    "            loss += answer_loss_function(targ[:, t], predictions)\n",
    "            val_acc_metric(targ[:, t],predictions)\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "            episodic_memory = None\n",
    "        batch_loss = (loss / int(targ.shape[1]))\n",
    "        return batch_loss\n",
    "    #Create batches for training\n",
    "    \n",
    "    TRAIN_BUFFER_SIZE = train_context_padded_seq[:num_training_samples].shape[0]\n",
    "    VAL_BUFFER_SIZE = val_context_padded_seq[:num_validation_samples].shape[0]\n",
    "    steps_per_epoch = TRAIN_BUFFER_SIZE//batch_size\n",
    "    steps_per_epoch_val = VAL_BUFFER_SIZE//batch_size\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((train_context_padded_seq[:num_training_samples],\n",
    "                                                        train_question_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_input_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_target_seq_padded[:num_training_samples],\n",
    "                                                        train_answer_impossible[:num_training_samples]))\\\n",
    "                                   .shuffle(TRAIN_BUFFER_SIZE,reshuffle_each_iteration=True)\n",
    "    train_dataset = train_dataset.batch(batch_size, drop_remainder=True)\n",
    "\n",
    "    val_dataset = tf.data.Dataset.from_tensor_slices((val_context_padded_seq[:num_validation_samples],\n",
    "                                                      val_question_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_input_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_target_seq_padded[:num_validation_samples],\n",
    "                                                      val_answer_impossible[:num_validation_samples]))\n",
    "    val_dataset = val_dataset.batch(batch_size,drop_remainder=True)\n",
    "    \n",
    "    #Run Epochs\n",
    "    \n",
    "    history_answer_model = {'loss':[],\n",
    "                            'sparse_categorical_accuracy':[],\n",
    "                            'val_loss':[],\n",
    "                            'val_sparse_categorical_accuracy':[]}\n",
    "\n",
    "    print(\"Training the answer model:\")\n",
    "    for epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        total_loss = 0\n",
    "        total_val_loss = 0\n",
    "\n",
    "        for (batch, (batch_train_context_padded_seq,\n",
    "                     batch_train_question_seq_padded,\n",
    "                     batch_train_answer_seq_padded,\n",
    "                     batch_train_answer_input_seq_padded,\n",
    "                     batch_train_answer_target_seq_padded,\n",
    "                     batch_train_answer_impossible)) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "            batch_loss = answer_train_step(batch_train_context_padded_seq,\n",
    "                                           batch_train_question_seq_padded,\n",
    "                                           batch_train_answer_input_seq_padded,\n",
    "                                           encoder_model,\n",
    "                                           answer_model)\n",
    "            total_loss += batch_loss\n",
    "            if batch % 50 == 0:\n",
    "                print('=t',end='')\n",
    "\n",
    "        epoch_training_loss = total_loss / steps_per_epoch\n",
    "        epoch_training_accuracy = train_acc_metric.result()\n",
    "        train_acc_metric.reset_states()\n",
    "        print('')\n",
    "        print('Epoch {} Train Loss {:.4f}'.format(epoch + 1,epoch_training_loss),end=' ')\n",
    "        print('Epoch {} Train Accuracy {:.4f}'.format(epoch + 1,epoch_training_accuracy))\n",
    "        \n",
    "        \n",
    "        for (batch, (batch_val_context_padded_seq,\n",
    "                     batch_val_question_seq_padded,\n",
    "                     batch_val_answer_seq_padded,\n",
    "                     batch_val_answer_input_seq_padded,\n",
    "                     batch_val_answer_target_seq_padded,\n",
    "                     batch_val_answer_impossible)) in enumerate(val_dataset.take(steps_per_epoch_val)):\n",
    "            batch_loss = answer_val_step(batch_val_context_padded_seq,\n",
    "                                         batch_val_question_seq_padded,\n",
    "                                         batch_val_answer_input_seq_padded,\n",
    "                                         encoder_model,\n",
    "                                         answer_model)\n",
    "            total_val_loss += batch_loss\n",
    "            if batch % 50 == 0:\n",
    "                print('=v',end='')\n",
    "\n",
    "        epoch_val_loss = total_val_loss / steps_per_epoch_val\n",
    "        epoch_val_accuracy = val_acc_metric.result()\n",
    "        val_acc_metric.reset_states()\n",
    "        print('')\n",
    "        print('Epoch {} Validation Loss {:.4f}'.format(epoch + 1,epoch_val_loss),end=' ')\n",
    "        print('Epoch {} Validation Accuracy {:.4f}'.format(epoch + 1,epoch_val_accuracy))\n",
    "        \n",
    "        \n",
    "        history_answer_model['loss'].append(epoch_training_loss)\n",
    "        history_answer_model['sparse_categorical_accuracy'].append(epoch_training_accuracy)\n",
    "        history_answer_model['val_loss'].append(epoch_val_loss)\n",
    "        history_answer_model['val_sparse_categorical_accuracy'].append(epoch_val_accuracy)\n",
    "        \n",
    "        print('Time taken for epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "    \n",
    "    answer_model.save_weights(ExperimentNo+'attention_model_answer_model.h5')\n",
    "    encoder_model.save_weights(ExperimentNo+'attention_model_encoder_model.h5')\n",
    "    with open(ExperimentNo+'attention_model_'+'history_answer_model', 'wb') as file_history:\n",
    "        pickle.dump(history_answer_model, file_history)\n",
    "        \n",
    "    \n",
    "    #Train the Feasibility Model\n",
    "    _,encoder_prediction,encoder_prediction_question,\\\n",
    "    encoder_prediction_contexts = encoder_model(train_context_padded_seq[:num_training_samples],\n",
    "                                             train_question_seq_padded[:num_training_samples])\n",
    "    _,encoder_validation_prediction,encoder_validation_prediction_question,\\\n",
    "    encoder_validation_prediction_contexts = encoder_model(val_context_padded_seq[:num_validation_samples],\n",
    "                                                         val_question_seq_padded[:num_validation_samples])\n",
    "    print(\"training the feasibility model\")\n",
    "    history_feasibility_model = feasibility_model.fit([encoder_prediction,\n",
    "                                                       encoder_prediction_contexts,\n",
    "                                                       encoder_prediction_question],\n",
    "                                                      train_answer_impossible[:num_training_samples],\n",
    "                                                      epochs=num_epochs,\n",
    "                                                      batch_size=batch_size,\n",
    "                                                      validation_data = \n",
    "                                                            ([encoder_validation_prediction,\n",
    "                                                              encoder_validation_prediction_contexts,\n",
    "                                                              encoder_validation_prediction_question],\n",
    "                                                             val_answer_impossible[:num_validation_samples])\n",
    "                                                      )\n",
    "\n",
    "\n",
    "\n",
    "    feasibility_model.save(ExperimentNo+'attention_model_feasibility_model.h5')\n",
    "    with open(ExperimentNo+'attention_model_'+'history_feasibility_model', 'wb') as file_history:\n",
    "        pickle.dump(history_feasibility_model.history, file_history)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model(Experiment_Dic,\n",
    "                           Experiment_No,\n",
    "                           embedding_matrix,\n",
    "                           ndim = 100):\n",
    "    ExperimentNo = 'Experiment'+str(Experiment_No)\n",
    "    num_unit_gru = Experiment_Dic[ExperimentNo]['num_unit_gru']\n",
    "    num_layers_gru = Experiment_Dic[ExperimentNo]['num_layers_gru']\n",
    "    num_episodes = Experiment_Dic[ExperimentNo]['num_episodes']\n",
    "    num_dense_layer_feasibility_units = Experiment_Dic[ExperimentNo]['num_dense_layer_feasibility_units']\n",
    "    dropout_rate = Experiment_Dic[ExperimentNo]['dropout_rate']\n",
    "    num_dense_layers_feasibility = Experiment_Dic[ExperimentNo]['num_dense_layers_feasibility']\n",
    "    num_episodic_network_unit = Experiment_Dic[ExperimentNo]['num_episodic_network_unit']\n",
    "    learning_rate = Experiment_Dic[ExperimentNo]['learning_rate']\n",
    "    l1_regularizer_weight = Experiment_Dic[ExperimentNo]['l1_regularizer_weight']\n",
    "    l2_regularizer_weight = Experiment_Dic[ExperimentNo]['l2_regularizer_weight']\n",
    "    inference_answer_model,\\\n",
    "    inference_encoder_model,\\\n",
    "    inference_feasibility_model = create_models(\n",
    "                                          embedding_matrix = embedding_matrix,\n",
    "                                          max_context_length = context_maxlen,\n",
    "                                          max_question_length = question_maxlen,\n",
    "                                          max_answer_length = answer_maxlen,\n",
    "                                          num_unit_gru = num_unit_gru,\n",
    "                                          num_layers_gru = num_layers_gru,\n",
    "                                          ndim =ndim,\n",
    "                                          num_episodes = num_episodes,\n",
    "                                          num_dense_layer_feasibility_units = num_dense_layer_feasibility_units,\n",
    "                                          dropout_rate = dropout_rate,\n",
    "                                          num_dense_layers_feasibility = num_dense_layers_feasibility,\n",
    "                                          num_episodic_network_unit = num_episodic_network_unit,\n",
    "                                          l1_regularizer_weight = l1_regularizer_weight,\n",
    "                                          l2_regularizer_weight = l2_regularizer_weight )\n",
    "\n",
    "    # train on 1 row so that weights can be loaded \n",
    "    #Train the Answer Model and Encoder Model\n",
    "    inference_answer_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    inference_answer_loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "    @tf.function\n",
    "    def inference_answer_loss_function(real, pred):\n",
    "        mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "        loss_ = inference_answer_loss_object(real, pred)\n",
    "        mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "        loss_ *= mask\n",
    "        return tf.reduce_mean(loss_)\n",
    "\n",
    "    @tf.function\n",
    "    def inference_answer_train_step(inp,ques,targ,encoder_model,answer_model):\n",
    "        loss = 0\n",
    "        with tf.GradientTape() as tape:\n",
    "            episodic_memory,concatenated_tensor,question_output,context_output = encoder_model(inp,ques)\n",
    "            answer_hidden = question_output\n",
    "            dec_input = tf.expand_dims(targ[:,0], 1)\n",
    "            for t in range(1, targ.shape[1]):\n",
    "                predictions, answer_hidden, _ = answer_model(dec_input,\n",
    "                                                             answer_hidden,\n",
    "                                                             question_output,\n",
    "                                                             context_output,\n",
    "                                                             episodic_memory)\n",
    "                loss += inference_answer_loss_function(targ[:, t], predictions)\n",
    "                #train_acc_metric(targ[:, t],predictions)\n",
    "                dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "                episodic_memory = None\n",
    "            batch_loss = (loss / int(targ.shape[1]))\n",
    "            variables = encoder_model.trainable_variables + answer_model.trainable_variables\n",
    "            gradients = tape.gradient(loss, variables)\n",
    "            inference_answer_optimizer.apply_gradients(zip(gradients, variables))\n",
    "            return batch_loss\n",
    "\n",
    "    inference_answer_train_step(train_context_padded_seq[:1],\n",
    "                              train_question_seq_padded[:1],\n",
    "                              train_answer_input_seq_padded[:1],\n",
    "                              inference_encoder_model,\n",
    "                              inference_answer_model)\n",
    "        \n",
    "    inference_answer_model.load_weights(ExperimentNo+'attention_model_answer_model.h5')\n",
    "    inference_encoder_model.load_weights(ExperimentNo+'attention_model_encoder_model.h5')\n",
    "    inference_feasibility_model.load_weights(ExperimentNo+'attention_model_feasibility_model.h5')\n",
    "    return (inference_answer_model,inference_encoder_model,inference_feasibility_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Training the answer model:\n",
      "=t=t=t\n",
      "Epoch 1 Train Loss 2.1799 Epoch 1 Train Accuracy 0.0868\n",
      "=v\n",
      "Epoch 1 Validation Loss 1.7453 Epoch 1 Validation Accuracy 0.0864\n",
      "Time taken for epoch 152.1944534778595 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 2 Train Loss 1.5025 Epoch 2 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 2 Validation Loss 1.7725 Epoch 2 Validation Accuracy 0.0864\n",
      "Time taken for epoch 106.2917423248291 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 3 Train Loss 1.4343 Epoch 3 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 3 Validation Loss 1.8065 Epoch 3 Validation Accuracy 0.0864\n",
      "Time taken for epoch 105.81519865989685 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 4 Train Loss 1.4088 Epoch 4 Train Accuracy 0.0884\n",
      "=v\n",
      "Epoch 4 Validation Loss 1.8318 Epoch 4 Validation Accuracy 0.0864\n",
      "Time taken for epoch 105.52970695495605 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 5 Train Loss 1.4045 Epoch 5 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 5 Validation Loss 1.8568 Epoch 5 Validation Accuracy 0.0864\n",
      "Time taken for epoch 105.77680325508118 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 6 Train Loss 1.3975 Epoch 6 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 6 Validation Loss 1.8551 Epoch 6 Validation Accuracy 0.0864\n",
      "Time taken for epoch 105.93714952468872 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 7 Train Loss 1.3882 Epoch 7 Train Accuracy 0.0883\n",
      "=v\n",
      "Epoch 7 Validation Loss 1.8442 Epoch 7 Validation Accuracy 0.0864\n",
      "Time taken for epoch 105.94253087043762 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 8 Train Loss 1.3714 Epoch 8 Train Accuracy 0.0884\n",
      "=v\n",
      "Epoch 8 Validation Loss 1.8404 Epoch 8 Validation Accuracy 0.0864\n",
      "Time taken for epoch 106.04293823242188 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 9 Train Loss 1.3489 Epoch 9 Train Accuracy 0.0884\n",
      "=v\n",
      "Epoch 9 Validation Loss 1.8467 Epoch 9 Validation Accuracy 0.0864\n",
      "Time taken for epoch 106.05816054344177 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 10 Train Loss 1.3337 Epoch 10 Train Accuracy 0.0908\n",
      "=v\n",
      "Epoch 10 Validation Loss 1.8382 Epoch 10 Validation Accuracy 0.0886\n",
      "Time taken for epoch 105.89810824394226 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 11 Train Loss 1.3087 Epoch 11 Train Accuracy 0.0915\n",
      "=v\n",
      "Epoch 11 Validation Loss 1.8519 Epoch 11 Validation Accuracy 0.0883\n",
      "Time taken for epoch 106.59669589996338 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 12 Train Loss 1.2776 Epoch 12 Train Accuracy 0.0917\n",
      "=v\n",
      "Epoch 12 Validation Loss 1.8688 Epoch 12 Validation Accuracy 0.0890\n",
      "Time taken for epoch 106.03541278839111 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 13 Train Loss 1.2498 Epoch 13 Train Accuracy 0.0926\n",
      "=v\n",
      "Epoch 13 Validation Loss 1.8987 Epoch 13 Validation Accuracy 0.0852\n",
      "Time taken for epoch 105.62478590011597 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 14 Train Loss 1.2237 Epoch 14 Train Accuracy 0.0926\n",
      "=v\n",
      "Epoch 14 Validation Loss 1.9287 Epoch 14 Validation Accuracy 0.0860\n",
      "Time taken for epoch 105.477219581604 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 15 Train Loss 1.1928 Epoch 15 Train Accuracy 0.0938\n",
      "=v\n",
      "Epoch 15 Validation Loss 1.9678 Epoch 15 Validation Accuracy 0.0792\n",
      "Time taken for epoch 105.81868720054626 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 16 Train Loss 1.1660 Epoch 16 Train Accuracy 0.0939\n",
      "=v\n",
      "Epoch 16 Validation Loss 1.9851 Epoch 16 Validation Accuracy 0.0792\n",
      "Time taken for epoch 105.25989174842834 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 17 Train Loss 1.1417 Epoch 17 Train Accuracy 0.0944\n",
      "=v\n",
      "Epoch 17 Validation Loss 2.0424 Epoch 17 Validation Accuracy 0.0795\n",
      "Time taken for epoch 105.22481918334961 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 18 Train Loss 1.1133 Epoch 18 Train Accuracy 0.0950\n",
      "=v\n",
      "Epoch 18 Validation Loss 2.0807 Epoch 18 Validation Accuracy 0.0769\n",
      "Time taken for epoch 104.28775763511658 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 19 Train Loss 1.0911 Epoch 19 Train Accuracy 0.0958\n",
      "=v\n",
      "Epoch 19 Validation Loss 2.1164 Epoch 19 Validation Accuracy 0.0720\n",
      "Time taken for epoch 103.07828211784363 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 20 Train Loss 1.0650 Epoch 20 Train Accuracy 0.0965\n",
      "=v\n",
      "Epoch 20 Validation Loss 2.1293 Epoch 20 Validation Accuracy 0.0758\n",
      "Time taken for epoch 103.33616971969604 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 21 Train Loss 1.0379 Epoch 21 Train Accuracy 0.0973\n",
      "=v\n",
      "Epoch 21 Validation Loss 2.1883 Epoch 21 Validation Accuracy 0.0758\n",
      "Time taken for epoch 103.39539217948914 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 22 Train Loss 1.0109 Epoch 22 Train Accuracy 0.0990\n",
      "=v\n",
      "Epoch 22 Validation Loss 2.2267 Epoch 22 Validation Accuracy 0.0735\n",
      "Time taken for epoch 103.19059562683105 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 23 Train Loss 0.9811 Epoch 23 Train Accuracy 0.0991\n",
      "=v\n",
      "Epoch 23 Validation Loss 2.2768 Epoch 23 Validation Accuracy 0.0716\n",
      "Time taken for epoch 103.2354645729065 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 24 Train Loss 0.9596 Epoch 24 Train Accuracy 0.1000\n",
      "=v\n",
      "Epoch 24 Validation Loss 2.3354 Epoch 24 Validation Accuracy 0.0678\n",
      "Time taken for epoch 103.11624479293823 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 25 Train Loss 0.9320 Epoch 25 Train Accuracy 0.1012\n",
      "=v\n",
      "Epoch 25 Validation Loss 2.3649 Epoch 25 Validation Accuracy 0.0708\n",
      "Time taken for epoch 102.98879909515381 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 26 Train Loss 0.8946 Epoch 26 Train Accuracy 0.1032\n",
      "=v\n",
      "Epoch 26 Validation Loss 2.4609 Epoch 26 Validation Accuracy 0.0667\n",
      "Time taken for epoch 102.98580741882324 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 27 Train Loss 0.8700 Epoch 27 Train Accuracy 0.1045\n",
      "=v\n",
      "Epoch 27 Validation Loss 2.4605 Epoch 27 Validation Accuracy 0.0629\n",
      "Time taken for epoch 105.06894731521606 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 28 Train Loss 0.8396 Epoch 28 Train Accuracy 0.1073\n",
      "=v\n",
      "Epoch 28 Validation Loss 2.4998 Epoch 28 Validation Accuracy 0.0636\n",
      "Time taken for epoch 105.5635461807251 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 29 Train Loss 0.8025 Epoch 29 Train Accuracy 0.1111\n",
      "=v\n",
      "Epoch 29 Validation Loss 2.5703 Epoch 29 Validation Accuracy 0.0652\n",
      "Time taken for epoch 105.76106309890747 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 30 Train Loss 0.7678 Epoch 30 Train Accuracy 0.1158\n",
      "=v\n",
      "Epoch 30 Validation Loss 2.6330 Epoch 30 Validation Accuracy 0.0610\n",
      "Time taken for epoch 105.3331081867218 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 31 Train Loss 0.7335 Epoch 31 Train Accuracy 0.1193\n",
      "=v\n",
      "Epoch 31 Validation Loss 2.6281 Epoch 31 Validation Accuracy 0.0576\n",
      "Time taken for epoch 104.3480293750763 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 32 Train Loss 0.6948 Epoch 32 Train Accuracy 0.1265\n",
      "=v\n",
      "Epoch 32 Validation Loss 2.7189 Epoch 32 Validation Accuracy 0.0602\n",
      "Time taken for epoch 103.41675043106079 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 33 Train Loss 0.6539 Epoch 33 Train Accuracy 0.1333\n",
      "=v\n",
      "Epoch 33 Validation Loss 2.8009 Epoch 33 Validation Accuracy 0.0580\n",
      "Time taken for epoch 103.36784839630127 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 34 Train Loss 0.6123 Epoch 34 Train Accuracy 0.1394\n",
      "=v\n",
      "Epoch 34 Validation Loss 2.8625 Epoch 34 Validation Accuracy 0.0610\n",
      "Time taken for epoch 103.20774340629578 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 35 Train Loss 0.5715 Epoch 35 Train Accuracy 0.1462\n",
      "=v\n",
      "Epoch 35 Validation Loss 2.9323 Epoch 35 Validation Accuracy 0.0557\n",
      "Time taken for epoch 103.37985253334045 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 36 Train Loss 0.5326 Epoch 36 Train Accuracy 0.1541\n",
      "=v\n",
      "Epoch 36 Validation Loss 3.0279 Epoch 36 Validation Accuracy 0.0580\n",
      "Time taken for epoch 103.33089280128479 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 37 Train Loss 0.4924 Epoch 37 Train Accuracy 0.1620\n",
      "=v\n",
      "Epoch 37 Validation Loss 3.0201 Epoch 37 Validation Accuracy 0.0485\n",
      "Time taken for epoch 103.11843633651733 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 38 Train Loss 0.4587 Epoch 38 Train Accuracy 0.1666\n",
      "=v\n",
      "Epoch 38 Validation Loss 3.1237 Epoch 38 Validation Accuracy 0.0508\n",
      "Time taken for epoch 103.1702401638031 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 39 Train Loss 0.4303 Epoch 39 Train Accuracy 0.1730\n",
      "=v\n",
      "Epoch 39 Validation Loss 3.2216 Epoch 39 Validation Accuracy 0.0564\n",
      "Time taken for epoch 103.26294255256653 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 40 Train Loss 0.3956 Epoch 40 Train Accuracy 0.1805\n",
      "=v\n",
      "Epoch 40 Validation Loss 3.2747 Epoch 40 Validation Accuracy 0.0530\n",
      "Time taken for epoch 103.33424615859985 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 41 Train Loss 0.3532 Epoch 41 Train Accuracy 0.1890\n",
      "=v\n",
      "Epoch 41 Validation Loss 3.2939 Epoch 41 Validation Accuracy 0.0504\n",
      "Time taken for epoch 103.12749242782593 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 42 Train Loss 0.3218 Epoch 42 Train Accuracy 0.1973\n",
      "=v\n",
      "Epoch 42 Validation Loss 3.3942 Epoch 42 Validation Accuracy 0.0508\n",
      "Time taken for epoch 103.12345504760742 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 43 Train Loss 0.2974 Epoch 43 Train Accuracy 0.2030\n",
      "=v\n",
      "Epoch 43 Validation Loss 3.4372 Epoch 43 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.14779472351074 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 44 Train Loss 0.2686 Epoch 44 Train Accuracy 0.2098\n",
      "=v\n",
      "Epoch 44 Validation Loss 3.5117 Epoch 44 Validation Accuracy 0.0527\n",
      "Time taken for epoch 103.22664070129395 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 45 Train Loss 0.2391 Epoch 45 Train Accuracy 0.2196\n",
      "=v\n",
      "Epoch 45 Validation Loss 3.5340 Epoch 45 Validation Accuracy 0.0485\n",
      "Time taken for epoch 103.19701528549194 sec\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=t=t=t\n",
      "Epoch 46 Train Loss 0.2159 Epoch 46 Train Accuracy 0.2248\n",
      "=v\n",
      "Epoch 46 Validation Loss 3.6441 Epoch 46 Validation Accuracy 0.0485\n",
      "Time taken for epoch 103.24605536460876 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 47 Train Loss 0.1910 Epoch 47 Train Accuracy 0.2324\n",
      "=v\n",
      "Epoch 47 Validation Loss 3.6930 Epoch 47 Validation Accuracy 0.0519\n",
      "Time taken for epoch 103.09227156639099 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 48 Train Loss 0.1679 Epoch 48 Train Accuracy 0.2382\n",
      "=v\n",
      "Epoch 48 Validation Loss 3.7383 Epoch 48 Validation Accuracy 0.0496\n",
      "Time taken for epoch 103.0259485244751 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 49 Train Loss 0.1506 Epoch 49 Train Accuracy 0.2427\n",
      "=v\n",
      "Epoch 49 Validation Loss 3.7814 Epoch 49 Validation Accuracy 0.0492\n",
      "Time taken for epoch 102.75559735298157 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 50 Train Loss 0.1338 Epoch 50 Train Accuracy 0.2481\n",
      "=v\n",
      "Epoch 50 Validation Loss 3.8533 Epoch 50 Validation Accuracy 0.0496\n",
      "Time taken for epoch 102.92084646224976 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 51 Train Loss 0.1197 Epoch 51 Train Accuracy 0.2505\n",
      "=v\n",
      "Epoch 51 Validation Loss 3.9349 Epoch 51 Validation Accuracy 0.0489\n",
      "Time taken for epoch 102.89709162712097 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 52 Train Loss 0.1078 Epoch 52 Train Accuracy 0.2540\n",
      "=v\n",
      "Epoch 52 Validation Loss 4.0010 Epoch 52 Validation Accuracy 0.0492\n",
      "Time taken for epoch 102.89453649520874 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 53 Train Loss 0.1034 Epoch 53 Train Accuracy 0.2536\n",
      "=v\n",
      "Epoch 53 Validation Loss 4.0201 Epoch 53 Validation Accuracy 0.0511\n",
      "Time taken for epoch 102.82325053215027 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 54 Train Loss 0.0854 Epoch 54 Train Accuracy 0.2584\n",
      "=v\n",
      "Epoch 54 Validation Loss 4.0577 Epoch 54 Validation Accuracy 0.0466\n",
      "Time taken for epoch 102.9643542766571 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 55 Train Loss 0.0708 Epoch 55 Train Accuracy 0.2617\n",
      "=v\n",
      "Epoch 55 Validation Loss 4.1031 Epoch 55 Validation Accuracy 0.0485\n",
      "Time taken for epoch 102.83013224601746 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 56 Train Loss 0.0648 Epoch 56 Train Accuracy 0.2641\n",
      "=v\n",
      "Epoch 56 Validation Loss 4.1373 Epoch 56 Validation Accuracy 0.0489\n",
      "Time taken for epoch 102.81697845458984 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 57 Train Loss 0.0616 Epoch 57 Train Accuracy 0.2634\n",
      "=v\n",
      "Epoch 57 Validation Loss 4.1639 Epoch 57 Validation Accuracy 0.0473\n",
      "Time taken for epoch 102.79270696640015 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 58 Train Loss 0.0583 Epoch 58 Train Accuracy 0.2643\n",
      "=v\n",
      "Epoch 58 Validation Loss 4.2346 Epoch 58 Validation Accuracy 0.0466\n",
      "Time taken for epoch 102.84474992752075 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 59 Train Loss 0.0527 Epoch 59 Train Accuracy 0.2650\n",
      "=v\n",
      "Epoch 59 Validation Loss 4.2917 Epoch 59 Validation Accuracy 0.0466\n",
      "Time taken for epoch 102.97741866111755 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 60 Train Loss 0.0505 Epoch 60 Train Accuracy 0.2653\n",
      "=v\n",
      "Epoch 60 Validation Loss 4.3180 Epoch 60 Validation Accuracy 0.0462\n",
      "Time taken for epoch 102.89227318763733 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 61 Train Loss 0.0418 Epoch 61 Train Accuracy 0.2674\n",
      "=v\n",
      "Epoch 61 Validation Loss 4.3207 Epoch 61 Validation Accuracy 0.0455\n",
      "Time taken for epoch 103.0911750793457 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 62 Train Loss 0.0398 Epoch 62 Train Accuracy 0.2668\n",
      "=v\n",
      "Epoch 62 Validation Loss 4.3567 Epoch 62 Validation Accuracy 0.0473\n",
      "Time taken for epoch 103.02925252914429 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 63 Train Loss 0.0424 Epoch 63 Train Accuracy 0.2659\n",
      "=v\n",
      "Epoch 63 Validation Loss 4.3966 Epoch 63 Validation Accuracy 0.0443\n",
      "Time taken for epoch 103.09166264533997 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 64 Train Loss 0.0796 Epoch 64 Train Accuracy 0.2556\n",
      "=v\n",
      "Epoch 64 Validation Loss 4.4593 Epoch 64 Validation Accuracy 0.0485\n",
      "Time taken for epoch 103.52838277816772 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 65 Train Loss 0.0633 Epoch 65 Train Accuracy 0.2606\n",
      "=v\n",
      "Epoch 65 Validation Loss 4.4297 Epoch 65 Validation Accuracy 0.0458\n",
      "Time taken for epoch 103.06807732582092 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 66 Train Loss 0.0418 Epoch 66 Train Accuracy 0.2665\n",
      "=v\n",
      "Epoch 66 Validation Loss 4.5180 Epoch 66 Validation Accuracy 0.0447\n",
      "Time taken for epoch 103.2359311580658 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 67 Train Loss 0.0307 Epoch 67 Train Accuracy 0.2688\n",
      "=v\n",
      "Epoch 67 Validation Loss 4.5291 Epoch 67 Validation Accuracy 0.0466\n",
      "Time taken for epoch 103.04545593261719 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 68 Train Loss 0.0211 Epoch 68 Train Accuracy 0.2700\n",
      "=v\n",
      "Epoch 68 Validation Loss 4.5576 Epoch 68 Validation Accuracy 0.0489\n",
      "Time taken for epoch 102.89966106414795 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 69 Train Loss 0.0168 Epoch 69 Train Accuracy 0.2702\n",
      "=v\n",
      "Epoch 69 Validation Loss 4.5891 Epoch 69 Validation Accuracy 0.0458\n",
      "Time taken for epoch 104.07280445098877 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 70 Train Loss 0.0150 Epoch 70 Train Accuracy 0.2709\n",
      "=v\n",
      "Epoch 70 Validation Loss 4.5891 Epoch 70 Validation Accuracy 0.0481\n",
      "Time taken for epoch 105.60262107849121 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 71 Train Loss 0.0142 Epoch 71 Train Accuracy 0.2706\n",
      "=v\n",
      "Epoch 71 Validation Loss 4.6393 Epoch 71 Validation Accuracy 0.0473\n",
      "Time taken for epoch 105.02737379074097 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 72 Train Loss 0.0130 Epoch 72 Train Accuracy 0.2705\n",
      "=v\n",
      "Epoch 72 Validation Loss 4.6512 Epoch 72 Validation Accuracy 0.0485\n",
      "Time taken for epoch 105.35923838615417 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 73 Train Loss 0.0115 Epoch 73 Train Accuracy 0.2709\n",
      "=v\n",
      "Epoch 73 Validation Loss 4.6893 Epoch 73 Validation Accuracy 0.0481\n",
      "Time taken for epoch 106.08207654953003 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 74 Train Loss 0.0108 Epoch 74 Train Accuracy 0.2713\n",
      "=v\n",
      "Epoch 74 Validation Loss 4.7044 Epoch 74 Validation Accuracy 0.0489\n",
      "Time taken for epoch 105.3914008140564 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 75 Train Loss 0.0101 Epoch 75 Train Accuracy 0.2707\n",
      "=v\n",
      "Epoch 75 Validation Loss 4.7270 Epoch 75 Validation Accuracy 0.0470\n",
      "Time taken for epoch 105.65188789367676 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 76 Train Loss 0.0098 Epoch 76 Train Accuracy 0.2711\n",
      "=v\n",
      "Epoch 76 Validation Loss 4.7723 Epoch 76 Validation Accuracy 0.0481\n",
      "Time taken for epoch 105.45998549461365 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 77 Train Loss 0.0093 Epoch 77 Train Accuracy 0.2715\n",
      "=v\n",
      "Epoch 77 Validation Loss 4.7486 Epoch 77 Validation Accuracy 0.0473\n",
      "Time taken for epoch 105.39767861366272 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 78 Train Loss 0.0089 Epoch 78 Train Accuracy 0.2715\n",
      "=v\n",
      "Epoch 78 Validation Loss 4.8051 Epoch 78 Validation Accuracy 0.0466\n",
      "Time taken for epoch 105.13732481002808 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 79 Train Loss 0.0122 Epoch 79 Train Accuracy 0.2701\n",
      "=v\n",
      "Epoch 79 Validation Loss 4.8650 Epoch 79 Validation Accuracy 0.0504\n",
      "Time taken for epoch 105.15347671508789 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 80 Train Loss 0.0327 Epoch 80 Train Accuracy 0.2651\n",
      "=v\n",
      "Epoch 80 Validation Loss 4.7229 Epoch 80 Validation Accuracy 0.0409\n",
      "Time taken for epoch 104.94293785095215 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 81 Train Loss 0.1210 Epoch 81 Train Accuracy 0.2365\n",
      "=v\n",
      "Epoch 81 Validation Loss 4.8353 Epoch 81 Validation Accuracy 0.0511\n",
      "Time taken for epoch 103.24630498886108 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 82 Train Loss 0.1481 Epoch 82 Train Accuracy 0.2298\n",
      "=v\n",
      "Epoch 82 Validation Loss 4.6932 Epoch 82 Validation Accuracy 0.0473\n",
      "Time taken for epoch 102.81933569908142 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 83 Train Loss 0.0787 Epoch 83 Train Accuracy 0.2525\n",
      "=v\n",
      "Epoch 83 Validation Loss 4.6824 Epoch 83 Validation Accuracy 0.0451\n",
      "Time taken for epoch 102.91336250305176 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 84 Train Loss 0.0307 Epoch 84 Train Accuracy 0.2661\n",
      "=v\n",
      "Epoch 84 Validation Loss 4.7699 Epoch 84 Validation Accuracy 0.0489\n",
      "Time taken for epoch 103.14559602737427 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 85 Train Loss 0.0140 Epoch 85 Train Accuracy 0.2705\n",
      "=v\n",
      "Epoch 85 Validation Loss 4.7539 Epoch 85 Validation Accuracy 0.0470\n",
      "Time taken for epoch 102.97571277618408 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 86 Train Loss 0.0090 Epoch 86 Train Accuracy 0.2713\n",
      "=v\n",
      "Epoch 86 Validation Loss 4.7632 Epoch 86 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.10431718826294 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 87 Train Loss 0.0077 Epoch 87 Train Accuracy 0.2717\n",
      "=v\n",
      "Epoch 87 Validation Loss 4.7852 Epoch 87 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.14502811431885 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 88 Train Loss 0.0072 Epoch 88 Train Accuracy 0.2718\n",
      "=v\n",
      "Epoch 88 Validation Loss 4.7955 Epoch 88 Validation Accuracy 0.0470\n",
      "Time taken for epoch 103.02501058578491 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 89 Train Loss 0.0068 Epoch 89 Train Accuracy 0.2714\n",
      "=v\n",
      "Epoch 89 Validation Loss 4.8153 Epoch 89 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.02701616287231 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 90 Train Loss 0.0066 Epoch 90 Train Accuracy 0.2716\n",
      "=v\n",
      "Epoch 90 Validation Loss 4.8091 Epoch 90 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.08349418640137 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 91 Train Loss 0.0061 Epoch 91 Train Accuracy 0.2713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=v\n",
      "Epoch 91 Validation Loss 4.8326 Epoch 91 Validation Accuracy 0.0477\n",
      "Time taken for epoch 103.09772396087646 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 92 Train Loss 0.0060 Epoch 92 Train Accuracy 0.2716\n",
      "=v\n",
      "Epoch 92 Validation Loss 4.8299 Epoch 92 Validation Accuracy 0.0473\n",
      "Time taken for epoch 102.88365936279297 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 93 Train Loss 0.0059 Epoch 93 Train Accuracy 0.2720\n",
      "=v\n",
      "Epoch 93 Validation Loss 4.8679 Epoch 93 Validation Accuracy 0.0492\n",
      "Time taken for epoch 102.93502712249756 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 94 Train Loss 0.0058 Epoch 94 Train Accuracy 0.2721\n",
      "=v\n",
      "Epoch 94 Validation Loss 4.8538 Epoch 94 Validation Accuracy 0.0481\n",
      "Time taken for epoch 102.74119758605957 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 95 Train Loss 0.0052 Epoch 95 Train Accuracy 0.2715\n",
      "=v\n",
      "Epoch 95 Validation Loss 4.9184 Epoch 95 Validation Accuracy 0.0485\n",
      "Time taken for epoch 102.59345841407776 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 96 Train Loss 0.0051 Epoch 96 Train Accuracy 0.2721\n",
      "=v\n",
      "Epoch 96 Validation Loss 4.8750 Epoch 96 Validation Accuracy 0.0477\n",
      "Time taken for epoch 102.39017796516418 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 97 Train Loss 0.0048 Epoch 97 Train Accuracy 0.2713\n",
      "=v\n",
      "Epoch 97 Validation Loss 4.9125 Epoch 97 Validation Accuracy 0.0481\n",
      "Time taken for epoch 102.45773959159851 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 98 Train Loss 0.0046 Epoch 98 Train Accuracy 0.2719\n",
      "=v\n",
      "Epoch 98 Validation Loss 4.8861 Epoch 98 Validation Accuracy 0.0466\n",
      "Time taken for epoch 102.63481450080872 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 99 Train Loss 0.0047 Epoch 99 Train Accuracy 0.2719\n",
      "=v\n",
      "Epoch 99 Validation Loss 4.9292 Epoch 99 Validation Accuracy 0.0477\n",
      "Time taken for epoch 102.38023352622986 sec\n",
      "\n",
      "=t=t=t\n",
      "Epoch 100 Train Loss 0.0046 Epoch 100 Train Accuracy 0.2721\n",
      "=v\n",
      "Epoch 100 Validation Loss 4.9281 Epoch 100 Validation Accuracy 0.0470\n",
      "Time taken for epoch 102.2952344417572 sec\n",
      "\n",
      "training the feasibility model\n",
      "Train on 2048 samples, validate on 256 samples\n",
      "Epoch 1/100\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "2048/2048 [==============================] - 1s 566us/sample - loss: 4.6524 - binary_accuracy: 0.7314 - val_loss: 3.6994 - val_binary_accuracy: 0.5742\n",
      "Epoch 2/100\n",
      "2048/2048 [==============================] - 0s 177us/sample - loss: 2.2777 - binary_accuracy: 0.8896 - val_loss: 2.3939 - val_binary_accuracy: 0.5664\n",
      "Epoch 3/100\n",
      "2048/2048 [==============================] - 0s 191us/sample - loss: 1.2901 - binary_accuracy: 0.9219 - val_loss: 1.8873 - val_binary_accuracy: 0.5703\n",
      "Epoch 4/100\n",
      "2048/2048 [==============================] - 0s 197us/sample - loss: 0.8419 - binary_accuracy: 0.9429 - val_loss: 1.6753 - val_binary_accuracy: 0.5742\n",
      "Epoch 5/100\n",
      "2048/2048 [==============================] - 0s 195us/sample - loss: 0.6093 - binary_accuracy: 0.9517 - val_loss: 1.5610 - val_binary_accuracy: 0.5547\n",
      "Epoch 6/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.5075 - binary_accuracy: 0.9434 - val_loss: 1.5280 - val_binary_accuracy: 0.5195\n",
      "Epoch 7/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.4341 - binary_accuracy: 0.9507 - val_loss: 1.5655 - val_binary_accuracy: 0.5781\n",
      "Epoch 8/100\n",
      "2048/2048 [==============================] - 0s 208us/sample - loss: 0.3943 - binary_accuracy: 0.9536 - val_loss: 1.8082 - val_binary_accuracy: 0.5820\n",
      "Epoch 9/100\n",
      "2048/2048 [==============================] - 0s 207us/sample - loss: 0.4173 - binary_accuracy: 0.9453 - val_loss: 1.6373 - val_binary_accuracy: 0.5625\n",
      "Epoch 10/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.4177 - binary_accuracy: 0.9404 - val_loss: 1.8929 - val_binary_accuracy: 0.5820\n",
      "Epoch 11/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3956 - binary_accuracy: 0.9395 - val_loss: 1.5735 - val_binary_accuracy: 0.5547\n",
      "Epoch 12/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3894 - binary_accuracy: 0.9507 - val_loss: 1.9193 - val_binary_accuracy: 0.6016\n",
      "Epoch 13/100\n",
      "2048/2048 [==============================] - 0s 206us/sample - loss: 0.3938 - binary_accuracy: 0.9419 - val_loss: 1.6160 - val_binary_accuracy: 0.5430\n",
      "Epoch 14/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3652 - binary_accuracy: 0.9448 - val_loss: 1.8043 - val_binary_accuracy: 0.5430\n",
      "Epoch 15/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.3422 - binary_accuracy: 0.9521 - val_loss: 1.9221 - val_binary_accuracy: 0.5859\n",
      "Epoch 16/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3290 - binary_accuracy: 0.9478 - val_loss: 1.6319 - val_binary_accuracy: 0.5938\n",
      "Epoch 17/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3569 - binary_accuracy: 0.9565 - val_loss: 1.8060 - val_binary_accuracy: 0.5898\n",
      "Epoch 18/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3444 - binary_accuracy: 0.9482 - val_loss: 1.7518 - val_binary_accuracy: 0.5547\n",
      "Epoch 19/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3311 - binary_accuracy: 0.9502 - val_loss: 1.7637 - val_binary_accuracy: 0.5625\n",
      "Epoch 20/100\n",
      "2048/2048 [==============================] - 0s 206us/sample - loss: 0.3127 - binary_accuracy: 0.9590 - val_loss: 1.9109 - val_binary_accuracy: 0.5703\n",
      "Epoch 21/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3145 - binary_accuracy: 0.9595 - val_loss: 1.7518 - val_binary_accuracy: 0.5820\n",
      "Epoch 22/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3321 - binary_accuracy: 0.9492 - val_loss: 1.8954 - val_binary_accuracy: 0.5742\n",
      "Epoch 23/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.2918 - binary_accuracy: 0.9600 - val_loss: 2.0607 - val_binary_accuracy: 0.5703\n",
      "Epoch 24/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3083 - binary_accuracy: 0.9556 - val_loss: 1.7673 - val_binary_accuracy: 0.5508\n",
      "Epoch 25/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3031 - binary_accuracy: 0.9580 - val_loss: 1.8709 - val_binary_accuracy: 0.5938\n",
      "Epoch 26/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3184 - binary_accuracy: 0.9561 - val_loss: 1.9106 - val_binary_accuracy: 0.6094\n",
      "Epoch 27/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3556 - binary_accuracy: 0.9414 - val_loss: 1.9306 - val_binary_accuracy: 0.5352\n",
      "Epoch 28/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3090 - binary_accuracy: 0.9619 - val_loss: 1.9725 - val_binary_accuracy: 0.5781\n",
      "Epoch 29/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3653 - binary_accuracy: 0.9482 - val_loss: 1.8414 - val_binary_accuracy: 0.5781\n",
      "Epoch 30/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3437 - binary_accuracy: 0.9482 - val_loss: 1.6309 - val_binary_accuracy: 0.5703\n",
      "Epoch 31/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3270 - binary_accuracy: 0.9487 - val_loss: 1.6892 - val_binary_accuracy: 0.5898\n",
      "Epoch 32/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3325 - binary_accuracy: 0.9463 - val_loss: 1.9129 - val_binary_accuracy: 0.5625\n",
      "Epoch 33/100\n",
      "2048/2048 [==============================] - 0s 206us/sample - loss: 0.3458 - binary_accuracy: 0.9404 - val_loss: 1.6939 - val_binary_accuracy: 0.5391\n",
      "Epoch 34/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3366 - binary_accuracy: 0.9458 - val_loss: 1.6889 - val_binary_accuracy: 0.5742\n",
      "Epoch 35/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3604 - binary_accuracy: 0.9351 - val_loss: 2.0739 - val_binary_accuracy: 0.5820\n",
      "Epoch 36/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3386 - binary_accuracy: 0.9468 - val_loss: 1.9078 - val_binary_accuracy: 0.5625\n",
      "Epoch 37/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3165 - binary_accuracy: 0.9521 - val_loss: 1.7595 - val_binary_accuracy: 0.5586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.2977 - binary_accuracy: 0.9580 - val_loss: 1.8524 - val_binary_accuracy: 0.5781\n",
      "Epoch 39/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3185 - binary_accuracy: 0.9565 - val_loss: 2.1392 - val_binary_accuracy: 0.5820\n",
      "Epoch 40/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.2840 - binary_accuracy: 0.9609 - val_loss: 1.6128 - val_binary_accuracy: 0.5781\n",
      "Epoch 41/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3315 - binary_accuracy: 0.9478 - val_loss: 2.0332 - val_binary_accuracy: 0.5977\n",
      "Epoch 42/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3556 - binary_accuracy: 0.9448 - val_loss: 2.0025 - val_binary_accuracy: 0.5820\n",
      "Epoch 43/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3308 - binary_accuracy: 0.9473 - val_loss: 1.7422 - val_binary_accuracy: 0.5703\n",
      "Epoch 44/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3264 - binary_accuracy: 0.9595 - val_loss: 2.1429 - val_binary_accuracy: 0.5820\n",
      "Epoch 45/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.3150 - binary_accuracy: 0.9463 - val_loss: 1.9565 - val_binary_accuracy: 0.5469\n",
      "Epoch 46/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3269 - binary_accuracy: 0.9575 - val_loss: 2.2303 - val_binary_accuracy: 0.5742\n",
      "Epoch 47/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3262 - binary_accuracy: 0.9507 - val_loss: 1.8616 - val_binary_accuracy: 0.5859\n",
      "Epoch 48/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3263 - binary_accuracy: 0.9521 - val_loss: 2.0930 - val_binary_accuracy: 0.5859\n",
      "Epoch 49/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.3549 - binary_accuracy: 0.9448 - val_loss: 1.7176 - val_binary_accuracy: 0.5781\n",
      "Epoch 50/100\n",
      "2048/2048 [==============================] - 0s 176us/sample - loss: 0.3136 - binary_accuracy: 0.9487 - val_loss: 2.0994 - val_binary_accuracy: 0.5938\n",
      "Epoch 51/100\n",
      "2048/2048 [==============================] - 0s 176us/sample - loss: 0.2987 - binary_accuracy: 0.9590 - val_loss: 1.7587 - val_binary_accuracy: 0.5430\n",
      "Epoch 52/100\n",
      "2048/2048 [==============================] - 0s 175us/sample - loss: 0.3143 - binary_accuracy: 0.9526 - val_loss: 2.0018 - val_binary_accuracy: 0.5898\n",
      "Epoch 53/100\n",
      "2048/2048 [==============================] - 0s 175us/sample - loss: 0.2856 - binary_accuracy: 0.9570 - val_loss: 1.7459 - val_binary_accuracy: 0.5625\n",
      "Epoch 54/100\n",
      "2048/2048 [==============================] - 0s 176us/sample - loss: 0.3407 - binary_accuracy: 0.9502 - val_loss: 2.0486 - val_binary_accuracy: 0.5938\n",
      "Epoch 55/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.3379 - binary_accuracy: 0.9448 - val_loss: 1.9774 - val_binary_accuracy: 0.5781\n",
      "Epoch 56/100\n",
      "2048/2048 [==============================] - 0s 175us/sample - loss: 0.3283 - binary_accuracy: 0.9482 - val_loss: 2.1596 - val_binary_accuracy: 0.5820\n",
      "Epoch 57/100\n",
      "2048/2048 [==============================] - 0s 176us/sample - loss: 0.3238 - binary_accuracy: 0.9448 - val_loss: 2.2154 - val_binary_accuracy: 0.5898\n",
      "Epoch 58/100\n",
      "2048/2048 [==============================] - 0s 174us/sample - loss: 0.3058 - binary_accuracy: 0.9570 - val_loss: 2.0288 - val_binary_accuracy: 0.5859\n",
      "Epoch 59/100\n",
      "2048/2048 [==============================] - 0s 174us/sample - loss: 0.3087 - binary_accuracy: 0.9536 - val_loss: 2.0142 - val_binary_accuracy: 0.5898\n",
      "Epoch 60/100\n",
      "2048/2048 [==============================] - 0s 178us/sample - loss: 0.2909 - binary_accuracy: 0.9614 - val_loss: 1.8272 - val_binary_accuracy: 0.5820\n",
      "Epoch 61/100\n",
      "2048/2048 [==============================] - 0s 180us/sample - loss: 0.2826 - binary_accuracy: 0.9541 - val_loss: 1.9083 - val_binary_accuracy: 0.5938\n",
      "Epoch 62/100\n",
      "2048/2048 [==============================] - 0s 191us/sample - loss: 0.2956 - binary_accuracy: 0.9546 - val_loss: 2.2068 - val_binary_accuracy: 0.5820\n",
      "Epoch 63/100\n",
      "2048/2048 [==============================] - 0s 196us/sample - loss: 0.3308 - binary_accuracy: 0.9482 - val_loss: 1.6158 - val_binary_accuracy: 0.5977\n",
      "Epoch 64/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3448 - binary_accuracy: 0.9414 - val_loss: 2.0979 - val_binary_accuracy: 0.5898\n",
      "Epoch 65/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.2859 - binary_accuracy: 0.9609 - val_loss: 1.9088 - val_binary_accuracy: 0.5508\n",
      "Epoch 66/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3161 - binary_accuracy: 0.9443 - val_loss: 2.1647 - val_binary_accuracy: 0.5898\n",
      "Epoch 67/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3421 - binary_accuracy: 0.9429 - val_loss: 2.0674 - val_binary_accuracy: 0.5977\n",
      "Epoch 68/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3292 - binary_accuracy: 0.9365 - val_loss: 1.8442 - val_binary_accuracy: 0.5312\n",
      "Epoch 69/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3475 - binary_accuracy: 0.9375 - val_loss: 1.7696 - val_binary_accuracy: 0.5703\n",
      "Epoch 70/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.2825 - binary_accuracy: 0.9546 - val_loss: 1.5620 - val_binary_accuracy: 0.5898\n",
      "Epoch 71/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3343 - binary_accuracy: 0.9526 - val_loss: 2.3351 - val_binary_accuracy: 0.4922\n",
      "Epoch 72/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3326 - binary_accuracy: 0.9502 - val_loss: 2.2587 - val_binary_accuracy: 0.6055\n",
      "Epoch 73/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3058 - binary_accuracy: 0.9507 - val_loss: 2.1187 - val_binary_accuracy: 0.5898\n",
      "Epoch 74/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3231 - binary_accuracy: 0.9546 - val_loss: 1.9212 - val_binary_accuracy: 0.5742\n",
      "Epoch 75/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3267 - binary_accuracy: 0.9443 - val_loss: 1.9888 - val_binary_accuracy: 0.5898\n",
      "Epoch 76/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.2844 - binary_accuracy: 0.9556 - val_loss: 2.0820 - val_binary_accuracy: 0.5977\n",
      "Epoch 77/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.2809 - binary_accuracy: 0.9604 - val_loss: 1.8461 - val_binary_accuracy: 0.5781\n",
      "Epoch 78/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.2980 - binary_accuracy: 0.9561 - val_loss: 1.9047 - val_binary_accuracy: 0.5820\n",
      "Epoch 79/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3312 - binary_accuracy: 0.9487 - val_loss: 2.3640 - val_binary_accuracy: 0.5898\n",
      "Epoch 80/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3310 - binary_accuracy: 0.9521 - val_loss: 2.3201 - val_binary_accuracy: 0.5898\n",
      "Epoch 81/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3099 - binary_accuracy: 0.9536 - val_loss: 2.2124 - val_binary_accuracy: 0.5820\n",
      "Epoch 82/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.4279 - binary_accuracy: 0.9351 - val_loss: 2.0547 - val_binary_accuracy: 0.5938\n",
      "Epoch 83/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3555 - binary_accuracy: 0.9521 - val_loss: 1.9958 - val_binary_accuracy: 0.5938\n",
      "Epoch 84/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.4029 - binary_accuracy: 0.9282 - val_loss: 2.5012 - val_binary_accuracy: 0.5938\n",
      "Epoch 85/100\n",
      "2048/2048 [==============================] - 0s 205us/sample - loss: 0.3359 - binary_accuracy: 0.9531 - val_loss: 1.8814 - val_binary_accuracy: 0.5000\n",
      "Epoch 86/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3492 - binary_accuracy: 0.9409 - val_loss: 2.0628 - val_binary_accuracy: 0.5273\n",
      "Epoch 87/100\n",
      "2048/2048 [==============================] - 0s 209us/sample - loss: 0.3456 - binary_accuracy: 0.9438 - val_loss: 2.2871 - val_binary_accuracy: 0.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3236 - binary_accuracy: 0.9482 - val_loss: 1.7511 - val_binary_accuracy: 0.5508\n",
      "Epoch 89/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3293 - binary_accuracy: 0.9521 - val_loss: 1.9355 - val_binary_accuracy: 0.5977\n",
      "Epoch 90/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.3412 - binary_accuracy: 0.9468 - val_loss: 2.3229 - val_binary_accuracy: 0.5820\n",
      "Epoch 91/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.3169 - binary_accuracy: 0.9478 - val_loss: 1.9576 - val_binary_accuracy: 0.5898\n",
      "Epoch 92/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3259 - binary_accuracy: 0.9443 - val_loss: 2.1103 - val_binary_accuracy: 0.5859\n",
      "Epoch 93/100\n",
      "2048/2048 [==============================] - 0s 202us/sample - loss: 0.3195 - binary_accuracy: 0.9536 - val_loss: 1.7434 - val_binary_accuracy: 0.5977\n",
      "Epoch 94/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3514 - binary_accuracy: 0.9404 - val_loss: 1.7266 - val_binary_accuracy: 0.5664\n",
      "Epoch 95/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.3384 - binary_accuracy: 0.9365 - val_loss: 2.1216 - val_binary_accuracy: 0.5352\n",
      "Epoch 96/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3328 - binary_accuracy: 0.9468 - val_loss: 1.7670 - val_binary_accuracy: 0.5508\n",
      "Epoch 97/100\n",
      "2048/2048 [==============================] - 0s 203us/sample - loss: 0.3517 - binary_accuracy: 0.9414 - val_loss: 2.0553 - val_binary_accuracy: 0.5391\n",
      "Epoch 98/100\n",
      "2048/2048 [==============================] - 0s 201us/sample - loss: 0.3399 - binary_accuracy: 0.9419 - val_loss: 1.8939 - val_binary_accuracy: 0.5469\n",
      "Epoch 99/100\n",
      "2048/2048 [==============================] - 0s 200us/sample - loss: 0.3400 - binary_accuracy: 0.9458 - val_loss: 2.0000 - val_binary_accuracy: 0.5938\n",
      "Epoch 100/100\n",
      "2048/2048 [==============================] - 0s 204us/sample - loss: 0.2694 - binary_accuracy: 0.9595 - val_loss: 2.9129 - val_binary_accuracy: 0.5859\n"
     ]
    }
   ],
   "source": [
    "#Run Experiments\n",
    "#change the num_training_samples and num_validation_samples make sure its multiple of 128 when running on tpu\n",
    "#change to tpu_enabled = 1 when running on tpu \n",
    "#Change the batch_size and epochs too\n",
    "run_experiments(Experiment_Dic=Experiment_Dic,\n",
    "                Experiment_No=0,\n",
    "                embedding_matrix=embedding_matrix,\n",
    "                ndim = 100,\n",
    "                tpu_enabled=0,\n",
    "                num_training_samples=2048,\n",
    "                num_validation_samples = 256,\n",
    "                num_epochs = 100,\n",
    "                batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
     ]
    }
   ],
   "source": [
    "#Run Inference\n",
    "inference_answer_model,\\\n",
    "inference_encoder_model,\\\n",
    "inference_feasibility_model = create_inference_model(Experiment_Dic=Experiment_Dic,\n",
    "                                                     Experiment_No=0,\n",
    "                                                     embedding_matrix=embedding_matrix,\n",
    "                                                     ndim = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: how many members are on the city commission\n",
      "Predicted Answer: 1 000 </s> \n",
      "Actual answer: <s> five </s>\n",
      "question: how does ps4 limit data use\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: what game was released in north america but not europe\n",
      "Predicted Answer: </s> \n",
      "Actual answer: <s> </s>\n",
      "question: how much was public expenditure on the island in 2001 2002\n",
      "Predicted Answer: 1 907 bc </s> \n",
      "Actual answer: <s> £10 million </s>\n",
      "question: what generates output streams from each endpoint\n",
      "Predicted Answer: free </s> \n",
      "Actual answer: <s> the mp </s>\n",
      "question: when was the london jewish forum established\n",
      "Predicted Answer: may new england </s> \n",
      "Actual answer: <s> 2006 </s>\n",
      "question: what kingdom came to power in assam\n",
      "Predicted Answer: kokhir co uk </s> \n",
      "Actual answer: <s> ahom kingdom </s>\n",
      "question: what mathematician advance greek science\n",
      "Predicted Answer: antibodies the greens </s> \n",
      "Actual answer: <s> greek science </s>\n",
      "question: how long did the lenin square protest last\n",
      "Predicted Answer: karim khan </s> \n",
      "Actual answer: <s> 18 days </s>\n",
      "question: what language besides latin did pope pius ix speak\n",
      "Predicted Answer: seven </s> \n",
      "Actual answer: <s> </s>\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(8000,8010):\n",
    "    # Take one sequence (part of the training test)\n",
    "    # for trying out decoding.\n",
    "    context_input_seq = train_context_padded_seq[seq_index: seq_index+ 1]\n",
    "    question_input_seq = train_question_seq_padded[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sentence(context_input_seq,\n",
    "                                       question_input_seq,\n",
    "                                       inference_encoder_model,\n",
    "                                       inference_answer_model)\n",
    "    print(\"question:\",' '.join([id_vocab.get(i) for i in train_question_seq_padded[seq_index].tolist() if i !=0]))\n",
    "    print('Predicted Answer:', decoded_sentence)\n",
    "    act_answer = ' '.join([id_vocab.get(i) for i in train_answer_input_seq_padded[seq_index].tolist() if i !=0])\n",
    "    print('Actual answer:',act_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_cpu_2",
   "language": "python",
   "name": "tensorflow_cpu_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
